File Dump from Project: C:\Users\jacob\Documents\Scripts\useful-helper-scripts\_MicroserviceCOLLECTION
Generated: 2025-12-02 20:54:00

-------------------- FILE: XXX_Example_and_Snippets_XXX\KB_Box_Snippets.json -----------
{
  "KB_Box_Variants_Snippets": {
    "source_apps": ["_ProjectKB", "kb-box.py"],
    "description": "Logic for AST parsing, heuristic summarization, and lightweight search.",
    "snippets": [
      {
        "name": "Heuristic_Summarizer",
        "description": "Generates a file summary using Regex instead of LLMs. Captures headers, function signatures, and docstrings.",
        "code": "import re\nimport os\n\n_SIG_RE = re.compile(r'^\\s*(def|class|function|interface|struct)\\s+([A-Za-z_][A-Za-z0-9_]*)')\n_MD_HDR = re.compile(r'^\\s{0,3}(#{1,3})\\s+(.+)')\n_DOC_RE = re.compile(r'^\\s*(\"{3}|\'{3})(.*)', re.DOTALL)\n\ndef heuristic_summary(path: str, chunk: str) -> str:\n    lines = chunk.splitlines()\n    picks = []\n    for ln in lines[:20]:\n        m = _MD_HDR.match(ln)\n        if m: picks.append(f\"Heading: {m.group(2).strip()}\")\n    for ln in lines[:40]:\n        m = _SIG_RE.match(ln)\n        if m: picks.append(f\"{m.group(1)} {m.group(2)}\")\n    if lines:\n        joined = \"\\n\".join(lines[:80])\n        m = _DOC_RE.match(joined)\n        if m:\n            after = joined.splitlines()[1:3]\n            if after: picks.append(\"Doc: \" + \" \".join(s.strip() for s in after).strip())\n    picks.append(f\"[{os.path.basename(path)}]\")\n    # Deduplicate and join\n    seen = set()\n    uniq = []\n    for p in picks:\n        if p and p not in seen: \n            uniq.append(p); seen.add(p)\n    return \" | \".join(uniq)[:480]"
      },
      {
        "name": "Line_Based_Chunker",
        "description": "Chunks text by line count with a hard character limit guard. simpler than token-based chunking.",
        "code": "def chunk_text_lines(text: str, max_lines: int = 200, max_chars: int = 4000):\n    lines = text.splitlines()\n    chunks = []\n    start = 0\n    while start < len(lines):\n        end = min(start + max_lines, len(lines))\n        chunk = \"\\n\".join(lines[start:end])\n        while len(chunk) > max_chars and end > start + 1:\n            end -= 1\n            chunk = \"\\n\".join(lines[start:end])\n        chunks.append((start+1, end, chunk))\n        start = end\n    return chunks"
      },
      {
        "name": "FastAPI_Factory",
        "description": "Dynamically creates a FastAPI app from a core logic object. Useful for exposing microservices.",
        "code": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nimport uvicorn\n\ndef run_api(core_logic_obj, host=\"0.0.0.0\", port=8099):\n    app = FastAPI(title=\"Microservice API\")\n    app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"])\n\n    @app.get(\"/health\")\n    def health(): return {\"status\": \"ok\"}\n\n    # Example of wrapping a method\n    class IngestReq(BaseModel):\n        root_dir: str\n    @app.post(\"/ingest\")\n    def ingest(req: IngestReq):\n        return core_logic_obj.ingest(req.root_dir)\n\n    uvicorn.run(app, host=host, port=port)"
      },
      {
        "name": "SQLite_FTS5_Triggers",
        "description": "SQL triggers to automatically keep a Full Text Search index in sync with a main content table.",
        "code": "CREATE VIRTUAL TABLE IF NOT EXISTS chunks_fts USING fts5(\n  text,\n  content='chunks',\n  content_rowid='id'\n);\n\n-- Triggers to sync updates\nCREATE TRIGGER IF NOT EXISTS chunks_ai AFTER INSERT ON chunks BEGIN\n  INSERT INTO chunks_fts(rowid, text) VALUES (new.id, new.text);\nEND;\nCREATE TRIGGER IF NOT EXISTS chunks_ad AFTER DELETE ON chunks BEGIN\n  INSERT INTO chunks_fts(chunks_fts, rowid, text) VALUES('delete', old.id, old.text);\nEND;"
      },
      {
        "name": "Get_Text_Slice_With_Context",
        "description": "Retrieves exact lines from a file with surrounding context (padding), useful for code previews.",
        "code": "def get_text_slice(conn, rel_path, start_line, end_line, pad=0):\n    cur = conn.cursor()\n    row = cur.execute(\"SELECT text FROM files WHERE rel_path=?\", (rel_path,)).fetchone()\n    if not row: return \"\"\n    lines = row[0].splitlines()\n    s = max(1, int(start_line) - int(pad))\n    e = min(len(lines), int(end_line) + int(pad))\n    snippet = lines[s-1:e]\n    return \"\\n\".join([f\"{i+1:6d}: {ln}\" for i, ln in enumerate(snippet, start=s-1)])"
      }
    ]
  }
}
--------------------------------------------------------------------------------

-------------------- FILE: XXX_Example_and_Snippets_XXX\NeoCORTEX_Database_snippets.json 
{
  "Cortex_Backend_Snippets": {
    "source_app": "Cortex_v2",
    "description": "High-performance SQLite configurations for RAG.",
    "snippets": [
      {
        "name": "SQLite_RAG_Optimizations",
        "description": "Critical PRAGMA settings to make SQLite performant for vector search and concurrent writes.",
        "code": "def optimize_sqlite(conn, mmap_gb=30):\n    # Enable Write-Ahead Logging for concurrency\n    conn.execute(\"PRAGMA journal_mode = WAL;\")\n    conn.execute(\"PRAGMA synchronous = NORMAL;\")\n    # Enable Memory Mapping for fast vector reads\n    mmap_size = mmap_gb * 1024 * 1024 * 1024\n    conn.execute(f\"PRAGMA mmap_size = {mmap_size};\ foreign_keys = ON;\")"
      },
      {
        "name": "Recursive_Directory_Cleaner",
        "description": "Naive SQL cleanup for re-ingestion scenarios.",
        "code": "def clear_knowledge_tables(cursor):\n    tables = ['knowledge_vectors', 'knowledge_chunks', 'documents_fts', 'nodes', 'edges']\n    for t in tables:\n        try: cursor.execute(f\"DELETE FROM {t}\")\n        except: pass"
      }
    ]
  }
}
--------------------------------------------------------------------------------

-------------------- FILE: XXX_Example_and_Snippets_XXX\ProjectMAPPER_snippets.json ----
{
  "ProjectMAPPER_Common_Snippets": {
    "source_app": "_ProjectMAPPER",
    "description": "Reusable logic blocks for file system traversal, exclusion filtering, and system command execution.",
    "snippets": [
      {
        "name": "Standard_Developer_Exclusions",
        "description": "A robust set of folders and file patterns to ignore when scanning developer projects (node_modules, venv, git, etc.).",
        "code": "DEFAULT_IGNORE_DIRS = {\n    \"node_modules\", \".git\", \"__pycache__\", \".venv\", \"venv\", \"env\",\n    \".mypy_cache\", \".pytest_cache\", \".idea\", \".vscode\", \n    \"dist\", \"build\", \"coverage\", \"target\", \"out\", \"bin\", \"obj\"\n}\n\nDEFAULT_IGNORE_FILES = {\n    \".DS_Store\", \"Thumbs.db\", \"*.pyc\", \"*.pyo\", \"*.log\", \"*.tmp\"\n}"
      },
      {
        "name": "Binary_File_Detection",
        "description": "Determines if a file is binary or text using a two-step process: extension check (fast) and content inspection for null bytes (accurate).",
        "code": "def is_binary(file_path):\n    # 1. Fast Fail on Extension\n    BINARY_EXTENSIONS = {\n        \".png\", \".jpg\", \".jpeg\", \".gif\", \".exe\", \".dll\", \".so\", \".pyc\",\n        \".zip\", \".tar.gz\", \".pdf\", \".db\", \".sqlite\"\n    }\n    if \"\".join(Path(file_path).suffixes).lower() in BINARY_EXTENSIONS:\n        return True\n    \n    # 2. Content Inspection (Read first 1KB)\n    try:\n        with open(file_path, 'rb') as f:\n            chunk = f.read(1024)\n            if b'\\x00' in chunk:\n                return True\n    except (IOError, OSError):\n        return True\n    return False"
      },
      {
        "name": "Human_Readable_Size",
        "description": "Converts raw bytes into a human-readable string (B, KB, MB, GB).",
        "code": "def format_size(size_bytes):\n    if size_bytes < 1024: return f\"{size_bytes} B\"\n    kb = size_bytes / 1024\n    if kb < 1024: return f\"{kb:.1f} KB\"\n    mb = kb / 1024\n    if mb < 1024: return f\"{mb:.1f} MB\"\n    return f\"{mb/1024:.2f} GB\""
      },
      {
        "name": "Safe_Shell_Command_Runner",
        "description": "Executes shell commands safely, capturing stdout/stderr and handling timeouts. Useful for system auditing.",
        "code": "import subprocess\n\ndef run_cmd(cmd, timeout=5):\n    try:\n        result = subprocess.run(\n            cmd, \n            text=True, \n            capture_output=True, \n            check=False, \n            shell=True, \n            timeout=timeout\n        )\n        if result.returncode == 0 and result.stdout:\n            return result.stdout.strip()\n        elif result.stderr:\n            return f\"[Error]: {result.stderr.strip()}\"\n        return \"[No Output]\"\n    except Exception as e:\n        return f\"[Execution Error]: {e}\""
      },
      {
        "name": "Smart_Exclusion_Check",
        "description": "Checks a filename against a set of exclusion patterns, supporting both exact matches and glob wildcards (e.g., *.log).",
        "code": "import fnmatch\n\ndef is_excluded(name, exclusion_set):\n    for pattern in exclusion_set:\n        if name == pattern:\n            return True\n        if fnmatch.fnmatch(name, pattern):\n            return True\n    return False"
      }
    ]
  }
}
--------------------------------------------------------------------------------

-------------------- FILE: XXX_Logs_XXX\MicroservicesLIBRARY__file_dump.txt ------------
File Dump from Project: C:\Users\jacob\Documents\Scripts\useful-helper-scripts\_MicroserviceCOLLECTION
Generated: 2025-12-02 20:48:42

-------------------- FILE: XXX_Example_and_Snippets_XXX\KB_Box_Snippets.json -----------
{
  "KB_Box_Variants_Snippets": {
    "source_apps": ["_ProjectKB", "kb-box.py"],
    "description": "Logic for AST parsing, heuristic summarization, and lightweight search.",
    "snippets": [
      {
        "name": "Heuristic_Summarizer",
        "description": "Generates a file summary using Regex instead of LLMs. Captures headers, function signatures, and docstrings.",
        "code": "import re\nimport os\n\n_SIG_RE = re.compile(r'^\\s*(def|class|function|interface|struct)\\s+([A-Za-z_][A-Za-z0-9_]*)')\n_MD_HDR = re.compile(r'^\\s{0,3}(#{1,3})\\s+(.+)')\n_DOC_RE = re.compile(r'^\\s*(\"{3}|\'{3})(.*)', re.DOTALL)\n\ndef heuristic_summary(path: str, chunk: str) -> str:\n    lines = chunk.splitlines()\n    picks = []\n    for ln in lines[:20]:\n        m = _MD_HDR.match(ln)\n        if m: picks.append(f\"Heading: {m.group(2).strip()}\")\n    for ln in lines[:40]:\n        m = _SIG_RE.match(ln)\n        if m: picks.append(f\"{m.group(1)} {m.group(2)}\")\n    if lines:\n        joined = \"\\n\".join(lines[:80])\n        m = _DOC_RE.match(joined)\n        if m:\n            after = joined.splitlines()[1:3]\n            if after: picks.append(\"Doc: \" + \" \".join(s.strip() for s in after).strip())\n    picks.append(f\"[{os.path.basename(path)}]\")\n    # Deduplicate and join\n    seen = set()\n    uniq = []\n    for p in picks:\n        if p and p not in seen: \n            uniq.append(p); seen.add(p)\n    return \" | \".join(uniq)[:480]"
      },
      {
        "name": "Line_Based_Chunker",
        "description": "Chunks text by line count with a hard character limit guard. simpler than token-based chunking.",
        "code": "def chunk_text_lines(text: str, max_lines: int = 200, max_chars: int = 4000):\n    lines = text.splitlines()\n    chunks = []\n    start = 0\n    while start < len(lines):\n        end = min(start + max_lines, len(lines))\n        chunk = \"\\n\".join(lines[start:end])\n        while len(chunk) > max_chars and end > start + 1:\n            end -= 1\n            chunk = \"\\n\".join(lines[start:end])\n        chunks.append((start+1, end, chunk))\n        start = end\n    return chunks"
      },
      {
        "name": "FastAPI_Factory",
        "description": "Dynamically creates a FastAPI app from a core logic object. Useful for exposing microservices.",
        "code": "from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nimport uvicorn\n\ndef run_api(core_logic_obj, host=\"0.0.0.0\", port=8099):\n    app = FastAPI(title=\"Microservice API\")\n    app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"])\n\n    @app.get(\"/health\")\n    def health(): return {\"status\": \"ok\"}\n\n    # Example of wrapping a method\n    class IngestReq(BaseModel):\n        root_dir: str\n    @app.post(\"/ingest\")\n    def ingest(req: IngestReq):\n        return core_logic_obj.ingest(req.root_dir)\n\n    uvicorn.run(app, host=host, port=port)"
      },
      {
        "name": "SQLite_FTS5_Triggers",
        "description": "SQL triggers to automatically keep a Full Text Search index in sync with a main content table.",
        "code": "CREATE VIRTUAL TABLE IF NOT EXISTS chunks_fts USING fts5(\n  text,\n  content='chunks',\n  content_rowid='id'\n);\n\n-- Triggers to sync updates\nCREATE TRIGGER IF NOT EXISTS chunks_ai AFTER INSERT ON chunks BEGIN\n  INSERT INTO chunks_fts(rowid, text) VALUES (new.id, new.text);\nEND;\nCREATE TRIGGER IF NOT EXISTS chunks_ad AFTER DELETE ON chunks BEGIN\n  INSERT INTO chunks_fts(chunks_fts, rowid, text) VALUES('delete', old.id, old.text);\nEND;"
      },
      {
        "name": "Get_Text_Slice_With_Context",
        "description": "Retrieves exact lines from a file with surrounding context (padding), useful for code previews.",
        "code": "def get_text_slice(conn, rel_path, start_line, end_line, pad=0):\n    cur = conn.cursor()\n    row = cur.execute(\"SELECT text FROM files WHERE rel_path=?\", (rel_path,)).fetchone()\n    if not row: return \"\"\n    lines = row[0].splitlines()\n    s = max(1, int(start_line) - int(pad))\n    e = min(len(lines), int(end_line) + int(pad))\n    snippet = lines[s-1:e]\n    return \"\\n\".join([f\"{i+1:6d}: {ln}\" for i, ln in enumerate(snippet, start=s-1)])"
      }
    ]
  }
}
--------------------------------------------------------------------------------

-------------------- FILE: XXX_Example_and_Snippets_XXX\NeoCORTEX_Database_snippets.json 
{
  "Cortex_Backend_Snippets": {
    "source_app": "Cortex_v2",
    "description": "High-performance SQLite configurations for RAG.",
    "snippets": [
      {
        "name": "SQLite_RAG_Optimizations",
        "description": "Critical PRAGMA settings to make SQLite performant for vector search and concurrent writes.",
        "code": "def optimize_sqlite(conn, mmap_gb=30):\n    # Enable Write-Ahead Logging for concurrency\n    conn.execute(\"PRAGMA journal_mode = WAL;\")\n    conn.execute(\"PRAGMA synchronous = NORMAL;\")\n    # Enable Memory Mapping for fast vector reads\n    mmap_size = mmap_gb * 1024 * 1024 * 1024\n    conn.execute(f\"PRAGMA mmap_size = {mmap_size};\ foreign_keys = ON;\")"
      },
      {
        "name": "Recursive_Directory_Cleaner",
        "description": "Naive SQL cleanup for re-ingestion scenarios.",
        "code": "def clear_knowledge_tables(cursor):\n    tables = ['knowledge_vectors', 'knowledge_chunks', 'documents_fts', 'nodes', 'edges']\n    for t in tables:\n        try: cursor.execute(f\"DELETE FROM {t}\")\n        except: pass"
      }
    ]
  }
}
--------------------------------------------------------------------------------

-------------------- FILE: XXX_Example_and_Snippets_XXX\ProjectMAPPER_snippets.json ----
{
  "ProjectMAPPER_Common_Snippets": {
    "source_app": "_ProjectMAPPER",
    "description": "Reusable logic blocks for file system traversal, exclusion filtering, and system command execution.",
    "snippets": [
      {
        "name": "Standard_Developer_Exclusions",
        "description": "A robust set of folders and file patterns to ignore when scanning developer projects (node_modules, venv, git, etc.).",
        "code": "DEFAULT_IGNORE_DIRS = {\n    \"node_modules\", \".git\", \"__pycache__\", \".venv\", \"venv\", \"env\",\n    \".mypy_cache\", \".pytest_cache\", \".idea\", \".vscode\", \n    \"dist\", \"build\", \"coverage\", \"target\", \"out\", \"bin\", \"obj\"\n}\n\nDEFAULT_IGNORE_FILES = {\n    \".DS_Store\", \"Thumbs.db\", \"*.pyc\", \"*.pyo\", \"*.log\", \"*.tmp\"\n}"
      },
      {
        "name": "Binary_File_Detection",
        "description": "Determines if a file is binary or text using a two-step process: extension check (fast) and content inspection for null bytes (accurate).",
        "code": "def is_binary(file_path):\n    # 1. Fast Fail on Extension\n    BINARY_EXTENSIONS = {\n        \".png\", \".jpg\", \".jpeg\", \".gif\", \".exe\", \".dll\", \".so\", \".pyc\",\n        \".zip\", \".tar.gz\", \".pdf\", \".db\", \".sqlite\"\n    }\n    if \"\".join(Path(file_path).suffixes).lower() in BINARY_EXTENSIONS:\n        return True\n    \n    # 2. Content Inspection (Read first 1KB)\n    try:\n        with open(file_path, 'rb') as f:\n            chunk = f.read(1024)\n            if b'\\x00' in chunk:\n                return True\n    except (IOError, OSError):\n        return True\n    return False"
      },
      {
        "name": "Human_Readable_Size",
        "description": "Converts raw bytes into a human-readable string (B, KB, MB, GB).",
        "code": "def format_size(size_bytes):\n    if size_bytes < 1024: return f\"{size_bytes} B\"\n    kb = size_bytes / 1024\n    if kb < 1024: return f\"{kb:.1f} KB\"\n    mb = kb / 1024\n    if mb < 1024: return f\"{mb:.1f} MB\"\n    return f\"{mb/1024:.2f} GB\""
      },
      {
        "name": "Safe_Shell_Command_Runner",
        "description": "Executes shell commands safely, capturing stdout/stderr and handling timeouts. Useful for system auditing.",
        "code": "import subprocess\n\ndef run_cmd(cmd, timeout=5):\n    try:\n        result = subprocess.run(\n            cmd, \n            text=True, \n            capture_output=True, \n            check=False, \n            shell=True, \n            timeout=timeout\n        )\n        if result.returncode == 0 and result.stdout:\n            return result.stdout.strip()\n        elif result.stderr:\n            return f\"[Error]: {result.stderr.strip()}\"\n        return \"[No Output]\"\n    except Exception as e:\n        return f\"[Execution Error]: {e}\""
      },
      {
        "name": "Smart_Exclusion_Check",
        "description": "Checks a filename against a set of exclusion patterns, supporting both exact matches and glob wildcards (e.g., *.log).",
        "code": "import fnmatch\n\ndef is_excluded(name, exclusion_set):\n    for pattern in exclusion_set:\n        if name == pattern:\n            return True\n        if fnmatch.fnmatch(name, pattern):\n            return True\n    return False"
      }
    ]
  }
}
--------------------------------------------------------------------------------

-------------------- FILE: XXX_Logs_XXX\MicroservicesLIBRARY__file_tree.txt ------------
Project Root: C:\Users\jacob\Documents\Scripts\useful-helper-scripts\_MicroserviceCOLLECTION
Generated: 2025-12-02 11:22:43
Global Default Folder Exclusions: .git, .idea, .mypy_cache, .venv, .vscode, Debug, Release, __pycache__, _logs, bin, build, dist, logs, node_modules, obj, out, target
Predefined Filename Exclusions: *.pyc, *.pyo, *.swo, *.swp, .DS_Store, Thumbs.db, package-lock.json, yarn.lock
Dynamic Filename Exclusions: None

[X] _MicroserviceCOLLECTION/ (Project Root)
  â”œâ”€â”€ [X] _APIGatewayMS/
  â”‚   â”œâ”€â”€ ðŸ“„ api_gateway.py
  â”‚   â””â”€â”€ ðŸ“„ requirements.txt
  â”œâ”€â”€ [X] _ArchiveBotMS/
  â”‚   â””â”€â”€ ðŸ“„ archive_bot.py
  â”œâ”€â”€ [X] _AuthMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ auth.py
  â”œâ”€â”€ [X] _CodeGrapherMS/
  â”‚   â””â”€â”€ ðŸ“„ code_grapher.py
  â”œâ”€â”€ [X] _CognitiveMemoryMS/
  â”‚   â”œâ”€â”€ ðŸ“„ cognitive_memory.py
  â”‚   â””â”€â”€ ðŸ“„ requirements.txt
  â”œâ”€â”€ [X] _ContextAggregatorMS/
  â”‚   â””â”€â”€ ðŸ“„ context_aggregator.py
  â”œâ”€â”€ [X] _DiffEngineMS/
  â”‚   â””â”€â”€ ðŸ“„ diff_engine.py
  â”œâ”€â”€ [X] _ExplorerWidgetMS/
  â”‚   â””â”€â”€ ðŸ“„ explorer_widget.py
  â”œâ”€â”€ [X] _FingerprintScannerMS/
  â”‚   â””â”€â”€ ðŸ“„ fingerprint_scanner.py
  â”œâ”€â”€ [X] _GitPilotMS/
  â”‚   â””â”€â”€ ðŸ“„ git_pilot.py
  â”œâ”€â”€ [X] _GraphEngineMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â”œâ”€â”€ ðŸ“„ graph_engine.py
  â”‚   â””â”€â”€ ðŸ“„ graph_view.py
  â”œâ”€â”€ [X] _HeruisticSumMS/
  â”‚   â””â”€â”€ ðŸ“„ heuristic_summarizer.py
  â”œâ”€â”€ [X] _IngestEngineMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ ingest_engine.py
  â”œâ”€â”€ [X] _IsoProcessMS/
  â”‚   â””â”€â”€ ðŸ“„ iso_process.py
  â”œâ”€â”€ [X] _LexicalSearchMS/
  â”‚   â””â”€â”€ ðŸ“„ lexical_search.py
  â”œâ”€â”€ [X] _LibrarianServiceMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ librarian_service.py
  â”œâ”€â”€ [X] _MonacoHostMS/
  â”‚   â”œâ”€â”€ ðŸ“„ editor.html
  â”‚   â””â”€â”€ ðŸ“„ monaco_host.py
  â”œâ”€â”€ [X] _PolyglotParserMS/
  â”‚   â””â”€â”€ ðŸ“„ polyglot_parser.py
  â”œâ”€â”€ [X] _PromptOptimizerMS/
  â”‚   â””â”€â”€ ðŸ“„ prompt_optimizer.py
  â”œâ”€â”€ [X] _PromptVaultMS/
  â”‚   â””â”€â”€ ðŸ“„ prompt_vault.py
  â”œâ”€â”€ [X] _SandboxManagerMS/
  â”‚   â””â”€â”€ ðŸ“„ sandbox_manager.py
  â”œâ”€â”€ [X] _ScannerMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ scanner.py
  â”œâ”€â”€ [X] _SearchEngineMS/
  â”‚   â””â”€â”€ ðŸ“„ search_engine.py
  â”œâ”€â”€ [X] _ServiceRegistryMS/
  â”‚   â””â”€â”€ ðŸ“„ service_registry.py
  â”œâ”€â”€ [X] _SysInspectorMS/
  â”‚   â””â”€â”€ ðŸ“„ sys_inspector.py
  â”œâ”€â”€ [X] _TasklistVaultMS/
  â”‚   â””â”€â”€ ðŸ“„ task_vault.py
  â”œâ”€â”€ [X] _ThoughtStreamMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ thought_stream.py
  â”œâ”€â”€ [X] _TreeMapperMS/
  â”‚   â””â”€â”€ ðŸ“„ tree_mapper.py
  â”œâ”€â”€ [X] _VectorFactoryMS/
  â”‚   â”œâ”€â”€ ðŸ“„ requirements.txt
  â”‚   â””â”€â”€ ðŸ“„ vector_factory.py
  â”œâ”€â”€ [X] _WebScraperMS/
  â”‚   â”œâ”€â”€ ðŸ“„ requirements.txt
  â”‚   â””â”€â”€ ðŸ“„ web_scraper.py
  â”œâ”€â”€ [X] XXX_Example_and_Snippets_XXX/
  â”‚   â”œâ”€â”€ ðŸ“„ KB_Box_Snippets.json
  â”‚   â””â”€â”€ ðŸ“„ ProjectMAPPER_snippets.json
  â””â”€â”€ [X] XXX_Logs_XXX/
--------------------------------------------------------------------------------

-------------------- FILE: XXX_Logs_XXX\MicroservicesLIBRARY__file_tree_2025-12-02_10-58-03.txt 
Project Root: C:\Users\jacob\Documents\Scripts\useful-helper-scripts\_MicroserviceCOLLECTION
Generated: 2025-12-02 10:58:03
Global Default Folder Exclusions: .git, .idea, .mypy_cache, .venv, .vscode, Debug, Release, __pycache__, _logs, bin, build, dist, logs, node_modules, obj, out, target
Predefined Filename Exclusions: *.pyc, *.pyo, *.swo, *.swp, .DS_Store, Thumbs.db, package-lock.json, yarn.lock
Dynamic Filename Exclusions: None

[X] _MicroserviceCOLLECTION/ (Project Root)
  â”œâ”€â”€ [X] _APIGatewayMS/
  â”‚   â”œâ”€â”€ ðŸ“„ api_gateway.py
  â”‚   â””â”€â”€ ðŸ“„ requirements.txt
  â”œâ”€â”€ [X] _ArchiveBotMS/
  â”‚   â””â”€â”€ ðŸ“„ archive_bot.py
  â”œâ”€â”€ [X] _AuthMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ auth.py
  â”œâ”€â”€ [X] _CodeGrapherMS/
  â”‚   â””â”€â”€ ðŸ“„ code_grapher.py
  â”œâ”€â”€ [X] _CognitiveMemoryMS/
  â”‚   â”œâ”€â”€ ðŸ“„ cognitive_memory.py
  â”‚   â””â”€â”€ ðŸ“„ requirements.txt
  â”œâ”€â”€ [X] _ContextAggregatorMS/
  â”‚   â””â”€â”€ ðŸ“„ context_aggregator.py
  â”œâ”€â”€ [X] _DiffEngineMS/
  â”‚   â””â”€â”€ ðŸ“„ diff_engine.py
  â”œâ”€â”€ [X] _ExplorerWidgetMS/
  â”‚   â””â”€â”€ ðŸ“„ explorer_widget.py
  â”œâ”€â”€ [X] _FingerprintScannerMS/
  â”‚   â””â”€â”€ ðŸ“„ fingerprint_scanner.py
  â”œâ”€â”€ [X] _GitPilotMS/
  â”‚   â””â”€â”€ ðŸ“„ git_pilot.py
  â”œâ”€â”€ [X] _GraphEngineMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â”œâ”€â”€ ðŸ“„ graph_engine.py
  â”‚   â””â”€â”€ ðŸ“„ graph_view.py
  â”œâ”€â”€ [X] _HeruisticSumMS/
  â”‚   â””â”€â”€ ðŸ“„ heuristic_summarizer.py
  â”œâ”€â”€ [X] _IngestEngineMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ ingest_engine.py
  â”œâ”€â”€ [X] _IsoProcessMS/
  â”‚   â””â”€â”€ ðŸ“„ iso_process.py
  â”œâ”€â”€ [X] _LexicalSearchMS/
  â”‚   â””â”€â”€ ðŸ“„ lexical_search.py
  â”œâ”€â”€ [X] _LibrarianServiceMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ librarian_service.py
  â”œâ”€â”€ [X] _MonacoHostMS/
  â”‚   â”œâ”€â”€ ðŸ“„ editor.html
  â”‚   â””â”€â”€ ðŸ“„ monaco_host.py
  â”œâ”€â”€ [X] _PolyglotParserMS/
  â”‚   â””â”€â”€ ðŸ“„ polyglot_parser.py
  â”œâ”€â”€ [X] _PromptOptimizerMS/
  â”‚   â””â”€â”€ ðŸ“„ prompt_optimizer.py
  â”œâ”€â”€ [X] _PromptVaultMS/
  â”‚   â””â”€â”€ ðŸ“„ prompt_vault.py
  â”œâ”€â”€ [X] _SandboxManagerMS/
  â”‚   â””â”€â”€ ðŸ“„ sandbox_manager.py
  â”œâ”€â”€ [X] _ScannerMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ scanner.py
  â”œâ”€â”€ [X] _SearchEngineMS/
  â”‚   â””â”€â”€ ðŸ“„ search_engine.py
  â”œâ”€â”€ [X] _SysInspectorMS/
  â”‚   â””â”€â”€ ðŸ“„ sys_inspector.py
  â”œâ”€â”€ [X] _TasklistVaultMS/
  â”‚   â””â”€â”€ ðŸ“„ task_vault.py
  â”œâ”€â”€ [X] _ThoughtStreamMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ thought_stream.py
  â”œâ”€â”€ [X] _TreeMapperMS/
  â”‚   â””â”€â”€ ðŸ“„ tree_mapper.py
  â”œâ”€â”€ [X] _VectorFactoryMS/
  â”‚   â”œâ”€â”€ ðŸ“„ requirements.txt
  â”‚   â””â”€â”€ ðŸ“„ vector_factory.py
  â”œâ”€â”€ [X] _WebScraperMS/
  â”‚   â”œâ”€â”€ ðŸ“„ requirements.txt
  â”‚   â””â”€â”€ ðŸ“„ web_scraper.py
  â”œâ”€â”€ [X] XXX_Example_and_Snippets_XXX/
  â”‚   â”œâ”€â”€ ðŸ“„ KB_Box_Snippets.json
  â”‚   â””â”€â”€ ðŸ“„ ProjectMAPPER_snippets.json
  â””â”€â”€ [X] XXX_Logs_XXX/
--------------------------------------------------------------------------------

-------------------- FILE: _APIGatewayMS\api_gateway.py --------------------------------
import sys
import threading
from typing import Any, Dict, Optional, Callable
import uvicorn

# ==============================================================================
# CONFIGURATION
# ==============================================================================
API_TITLE = "Microservice Gateway"
API_VERSION = "1.0.0"
DEFAULT_HOST = "0.0.0.0"
DEFAULT_PORT = 8099
# ==============================================================================

class APIGatewayMS:
    """
    The Gateway: A wrapper that exposes a local Python object as a REST API.
    
    Features:
    - Auto-generates Swagger UI at /docs
    - Threaded execution (non-blocking for UI apps)
    - CORS enabled by default (for React/Web frontends)
    """

    def __init__(self, backend_core: Any):
        """
        :param backend_core: The logic object to expose (e.g. an instance of SearchEngineMS)
        """
        self.core = backend_core
        self.server_thread = None
        
        # Lazy import to avoid hard crash if libs are missing
        try:
            from fastapi import FastAPI
            from fastapi.middleware.cors import CORSMiddleware
            from pydantic import BaseModel
            self.FastAPI = FastAPI
            self.CORSMiddleware = CORSMiddleware
            self.BaseModel = BaseModel
            self._available = True
        except ImportError:
            print("CRITICAL: 'fastapi' or 'uvicorn' not installed.")
            print("Run: pip install fastapi uvicorn pydantic")
            self._available = False
            return

        self.app = self.FastAPI(title=API_TITLE, version=API_VERSION)
        
        # Enable CORS
        self.app.add_middleware(
            self.CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"]
        )
        
        # Setup Base Routes
        self._setup_system_routes()

    def _setup_system_routes(self):
        @self.app.get("/")
        def root():
            return {"status": "online", "service": API_TITLE}

        @self.app.get("/health")
        def health():
            return {"status": "healthy", "backend_type": type(self.core).__name__}

    def add_endpoint(self, path: str, method: str, handler: Callable):
        """
        Dynamically adds a route to the API.
        
        :param path: URL path (e.g., "/search")
        :param method: "GET" or "POST"
        :param handler: The function to run
        """
        if method.upper() == "POST":
            self.app.post(path)(handler)
        elif method.upper() == "GET":
            self.app.get(path)(handler)

    def start(self, host: str = DEFAULT_HOST, port: int = DEFAULT_PORT, blocking: bool = True):
        """
        Starts the Uvicorn server.
        """
        if not self._available: return

        def _run():
            print(f"ðŸš€ API Gateway running at http://{host}:{port}")
            print(f"ðŸ“„ Docs available at http://{host}:{port}/docs")
            uvicorn.run(self.app, host=host, port=port, log_level="info")

        if blocking:
            _run()
        else:
            self.server_thread = threading.Thread(target=_run, daemon=True)
            self.server_thread.start()

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Define a Mock Backend (The "Core" Logic)
    class MockBackend:
        def search(self, query):
            return [f"Result for {query} 1", f"Result for {query} 2"]
        
        def echo(self, msg):
            return f"Echo: {msg}"

    backend = MockBackend()
    
    # 2. Init Gateway
    gateway = APIGatewayMS(backend)
    
    if gateway._available:
        # 3. Define Request Models (Pydantic) for strong typing in Swagger
        class SearchReq(gateway.BaseModel):
            query: str
            limit: int = 10

        class EchoReq(gateway.BaseModel):
            message: str

        # 4. Map Backend Methods to API Endpoints
        
        def search_endpoint(req: SearchReq):
            """Searches the mock backend."""
            return {"results": backend.search(req.query), "limit": req.limit}
            
        def echo_endpoint(req: EchoReq):
            """Echoes a message."""
            return {"response": backend.echo(req.message)}

        gateway.add_endpoint("/v1/search", "POST", search_endpoint)
        gateway.add_endpoint("/v1/echo", "POST", echo_endpoint)

        # 5. Run
        # Note: In a real app, you might want blocking=False if running a UI too
        gateway.start(port=8099, blocking=True)
--------------------------------------------------------------------------------

-------------------- FILE: _APIGatewayMS\requirements.txt ------------------------------
pip install fastapi uvicorn pydantic
--------------------------------------------------------------------------------

-------------------- FILE: _ArchiveBotMS\archive_bot.py --------------------------------
import tarfile
import os
import fnmatch
import datetime
from pathlib import Path
from typing import Set, Optional, Tuple

# ==============================================================================
# USER CONFIGURATION: DEFAULT EXCLUSIONS
# ==============================================================================
# Folders to ignore by default (Development artifacts, environments, etc.)
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", 
    "dist", "build", "coverage", "target", "out", "bin", "obj"
}

# Files to ignore by default (System metadata, temporary files, etc.)
DEFAULT_IGNORE_FILES = {
    ".DS_Store", "Thumbs.db", "*.pyc", "*.pyo", "*.log", "*.tmp"
}
# ==============================================================================

class ArchiveBotMS:
    """
    The Archiver: Creates compressed .tar.gz backups of project folders.
    """

    def create_backup(self, 
                      source_path: str, 
                      output_dir: str, 
                      extra_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> Tuple[str, int]:
        """
        Compresses the source_path into a .tar.gz file.
        
        :param use_default_exclusions: If False, ignores the DEFAULT_IGNORE lists at top of script.
        """
        src = Path(source_path).resolve()
        out = Path(output_dir).resolve()
        
        out.mkdir(parents=True, exist_ok=True)

        # Build exclusion set
        exclude_set = set()
        if use_default_exclusions:
            exclude_set.update(DEFAULT_IGNORE_DIRS)
            exclude_set.update(DEFAULT_IGNORE_FILES)
            
        if extra_exclusions:
            exclude_set.update(extra_exclusions)

        # Generate Timestamped Filename
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        archive_name = f"backup_{src.name}_{timestamp}.tar.gz"
        archive_path = out / archive_name

        file_count = 0

        try:
            with tarfile.open(archive_path, "w:gz") as tar:
                for root, dirs, files in os.walk(src):
                    # 1. Filter Directories in-place
                    for i in range(len(dirs) - 1, -1, -1):
                        d_name = dirs[i]
                        if self._is_excluded(d_name, exclude_set):
                            dirs.pop(i)
                    
                    # 2. Add Files
                    for file in files:
                        if self._is_excluded(file, exclude_set):
                            continue
                            
                        full_path = Path(root) / file
                        if full_path == archive_path: continue

                        rel_path = full_path.relative_to(src)
                        tar.add(full_path, arcname=rel_path)
                        file_count += 1
                        
            return str(archive_path), file_count

        except Exception as e:
            if archive_path.exists(): os.remove(archive_path)
            raise e

    def _is_excluded(self, name: str, patterns: Set[str]) -> bool:
        for pattern in patterns:
            if name == pattern: return True
            if fnmatch.fnmatch(name, pattern): return True
        return False

if __name__ == "__main__":
    # Test
    bot = ArchiveBotMS()
    print("ArchiveBot initialized. Check top of file to tweak defaults.")
--------------------------------------------------------------------------------

-------------------- FILE: _AuthMS\auth.py ---------------------------------------------
import hashlib
import time
import json
import base64
from typing import Optional, Dict

class AuthMS:
    """
    The Gatekeeper: Manages user authentication and session tokens.
    """
    def __init__(self, secret_key: str = "super_secret_cortex_key"):
        self.secret_key = secret_key
        # In a real scenario, this might load from a secure config file or DB
        self.users_db = {
            "admin": self._hash_password("admin123")
        }

    def login(self, username, password) -> Optional[str]:
        """
        Attempts to log in. Returns a session token if successful, None otherwise.
        """
        if username not in self.users_db:
            return None
        
        stored_hash = self.users_db[username]
        if self._verify_password(password, stored_hash):
            return self._create_token(username)
        
        return None

    def validate_session(self, token: str) -> bool:
        """
        Checks if a token is valid and not expired.
        """
        payload = self._decode_token(token)
        if payload:
            return True
        return False

    def _hash_password(self, password: str) -> str:
        """
        Securely hashes a password using SHA-256 (Simulated salt).
        """
        salt = "cortex_salt"
        return hashlib.sha256((password + salt).encode()).hexdigest()

    def _verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """
        Verifies a provided password against the stored hash.
        """
        return self._hash_password(plain_password) == hashed_password

    def _create_token(self, user_id: str, expires_in: int = 3600) -> str:
        """
        Generates a signed session token.
        Payload includes 'sub' (subject) and 'exp' (expiration).
        """
        payload = {
            "sub": user_id,
            "exp": int(time.time()) + expires_in,
            "iat": int(time.time()),
            "scope": "admin"
        }
        
        # Create the token parts
        json_payload = json.dumps(payload).encode()
        token_part = base64.b64encode(json_payload).decode()
        
        # Sign it
        signature = hashlib.sha256((token_part + self.secret_key).encode()).hexdigest()
        
        return f"{token_part}.{signature}"

    def _decode_token(self, token: str) -> Optional[Dict]:
        """
        Parses and validates the incoming token.
        Returns the payload if valid, None otherwise.
        """
        try:
            if not token or "." not in token:
                return None

            token_part, signature = token.split('.')
            
            # Re-calculate signature to verify integrity
            recalc_signature = hashlib.sha256((token_part + self.secret_key).encode()).hexdigest()
            
            if signature != recalc_signature:
                return None # Invalid Signature
            
            # Decode payload
            payload_json = base64.b64decode(token_part).decode()
            payload = json.loads(payload_json)
            
            # Check expiration
            if payload['exp'] < time.time():
                return None # Expired
                
            return payload
        except Exception:
            return None

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Auth Service Test ---")
    auth = AuthMS()
    
    # 1. Test Login Success
    print("Attempting login (admin / admin123)...")
    token = auth.login("admin", "admin123")
    
    if token:
        print(f"Login Success! Token: {token[:20]}...")
        
        # 2. Test Validation
        is_valid = auth.validate_session(token)
        print(f"Token Valid? {is_valid}")
    else:
        print("Login Failed.")

    # 3. Test Login Fail
    print("\nAttempting login (admin / wrongpass)...")
    bad_token = auth.login("admin", "wrongpass")
    if not bad_token:
        print("Login Failed (Expected).")
--------------------------------------------------------------------------------

-------------------- FILE: _CodeChunkerMS\code_chunker.py ------------------------------
import os
from typing import List, Dict, Any, Optional
from pathlib import Path

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Map extensions to tree-sitter language names
LANG_MAP = {
    ".py": "python",
    ".js": "javascript", ".jsx": "javascript",
    ".ts": "typescript", ".tsx": "typescript",
    ".go": "go",
    ".rs": "rust",
    ".java": "java",
    ".cpp": "cpp", ".cc": "cpp", ".c": "c", ".h": "c"
}

# The "Atomic Units" we want to keep whole
CHUNK_NODES = {
    "class_definition", "function_definition", "method_definition", # Python
    "class_declaration", "function_declaration", "method_definition", # JS/TS
    "func_literal", "function_declaration", # Go
    "function_item", "impl_item" # Rust
}

class CodeChunkerMS:
    """
    The Surgeon: Uses Tree-Sitter (CST) to structurally parse code.
    Splits files into 'Semantic Chunks' (Classes, Functions) rather than
    arbitrary text slices. Preserves comments and docstrings.
    """
    def __init__(self):
        self._available = False
        try:
            from tree_sitter import Parser
            from tree_sitter_languages import get_language
            self.Parser = Parser
            self.get_language = get_language
            self._available = True
        except ImportError:
            print("CRITICAL: tree-sitter not installed. Chunker will fail.")

    def chunk_file(self, file_path: str, max_chars: int = 1500) -> List[Dict[str, Any]]:
        """
        Reads a file and breaks it into semantic code blocks.
        """
        if not self._available: return []
        
        path = Path(file_path)
        ext = path.suffix.lower()
        if ext not in LANG_MAP:
            return [] # fallback to text chunker for unknown types?

        lang_id = LANG_MAP[ext]
        try:
            code = path.read_text(encoding="utf-8", errors="ignore")
            return self._parse(code, lang_id, max_chars)
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
            return []

    def _parse(self, code: str, lang_id: str, max_chars: int) -> List[Dict]:
        language = self.get_language(lang_id)
        parser = self.Parser()
        parser.set_language(language)
        
        tree = parser.parse(bytes(code, "utf8"))
        chunks = []
        
        # Cursor allows efficient traversal
        cursor = tree.walk()
        
        # Recursive walker to find significant nodes
        def walk(node):
            # If this node is a major block (Function/Class)
            if node.type in CHUNK_NODES:
                # 1. Capture the code including preceding comments
                start_byte = node.start_byte
                end_byte = node.end_byte
                
                # Look backwards for docstrings/comments attached to this node
                # (Simple heuristic: scan previous sibling)
                prev = node.prev_sibling
                if prev and prev.type == "comment":
                    start_byte = prev.start_byte

                chunk_text = code[start_byte:end_byte]
                
                # 2. Check size
                if len(chunk_text) > max_chars:
                    # Too big? Recurse into children (split the function apart)
                    for child in node.children:
                        walk(child)
                else:
                    # Good size? Keep it as a chunk
                    chunks.append({
                        "type": node.type,
                        "text": chunk_text,
                        "start_line": node.start_point[0],
                        "end_line": node.end_point[0]
                    })
                return # Don't recurse if we kept the parent

            # If not a major node, just keep walking down
            for child in node.children:
                walk(child)

        walk(tree.root_node)
        
        # Fallback: If no structural chunks found (e.g. flat script), return whole file
        if not chunks and code.strip():
            chunks.append({
                "type": "file", 
                "text": code, 
                "start_line": 0, 
                "end_line": len(code.splitlines())
            })
            
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
    # Test with a dummy Python file
    chunker = CodeChunkerMS()
    
    if chunker._available:
        py_code = """
        # This is a helper function
        def helper(x):
            return x * 2

        class Processor:
            '''
            Main processing class.
            '''
            def process(self, data):
                # Process the data
                if data:
                    return helper(data)
                return None
        """
        
        # Write temp file
        import tempfile
        with tempfile.NamedTemporaryFile(suffix=".py", mode="w+", delete=False) as tmp:
            tmp.write(py_code)
            tmp_path = tmp.name
            
        print(f"--- Chunking {tmp_path} ---")
        chunks = chunker.chunk_file(tmp_path)
        
        for i, c in enumerate(chunks):
            print(f"\n[Chunk {i}] Type: {c['type']} (Lines {c['start_line']}-{c['end_line']})")
            print(f"{'-'*20}\n{c['text'].strip()}\n{'-'*20}")
            
        os.remove(tmp_path)
    else:
        print("Skipping test: tree-sitter not installed.")
--------------------------------------------------------------------------------

-------------------- FILE: _CodeGrapherMS\code_grapher.py ------------------------------
import ast
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional

class CodeGrapherMS:
    """
    The Cartographer of Logic: Parses Python code to extract high-level 
    symbols (classes, functions) and maps their 'Call' relationships.
    
    Output: A graph structure (Nodes + Edges) suitable for visualization 
    or dependency analysis.
    """

    def __init__(self):
        self.nodes = [] # List of symbols (functions, classes)
        self.edges = [] # List of relationships (source -> target)

    def scan_directory(self, root_path: str) -> Dict[str, Any]:
        """
        Recursively scans a directory for .py files and builds the graph.
        """
        root = Path(root_path).resolve()
        self.nodes = []
        self.edges = []
        
        if not root.exists():
            return {"error": f"Path {root} does not exist"}

        # 1. Parsing Pass (Create Nodes)
        for path in root.rglob("*.py"):
            try:
                # Skip hidden/venv folders
                if any(p.startswith('.') for p in path.parts) or "venv" in path.parts:
                    continue
                    
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    source = f.read()
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_symbols = self._parse_source(source, rel_path)
                self.nodes.extend(file_symbols)
                
            except Exception as e:
                print(f"Failed to parse {path.name}: {e}")

        # 2. Linking Pass (Create Edges)
        self._build_edges()

        return {
            "root": str(root),
            "node_count": len(self.nodes),
            "edge_count": len(self.edges),
            "nodes": self.nodes,
            "edges": self.edges
        }

    def _parse_source(self, source: str, file_path: str) -> List[Dict]:
        """
        Uses Python's AST to extract surgical symbol info.
        """
        try:
            tree = ast.parse(source)
        except SyntaxError:
            return []

        visitor = SurgicalVisitor(file_path)
        visitor.visit(tree)
        return visitor.symbols

    def _build_edges(self):
        """
        Resolves 'calls' strings into explicit graph edges.
        """
        # Create a quick lookup map: "my_function" -> NodeID
        # Note: This is a naive lookup (name collision possible). 
        # A robust version would use "module.class.method" fully qualified names.
        name_map = {n['name']: n['id'] for n in self.nodes}

        for node in self.nodes:
            source_id = node['id']
            calls = node.get('calls', [])
            
            for target_name in calls:
                if target_name in name_map:
                    target_id = name_map[target_name]
                    
                    # Avoid self-loops for cleanliness
                    if source_id != target_id:
                        self.edges.append({
                            "source": source_id,
                            "target": target_id,
                            "type": "calls"
                        })

# --- Helper Class: The AST Walker ---

class SurgicalVisitor(ast.NodeVisitor):
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.symbols = []

    def visit_FunctionDef(self, node):
        self._handle_func(node, "function")

    def visit_AsyncFunctionDef(self, node):
        self._handle_func(node, "async_function")

    def visit_ClassDef(self, node):
        # Record the class
        class_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": class_id,
            "file": self.file_path,
            "name": node.name,
            "type": "class",
            "line": node.lineno,
            "calls": [] # Classes don't 'call' things directly usually, their methods do
        })
        # Visit children (methods)
        self.generic_visit(node)

    def _handle_func(self, node, type_name):
        # Extract outgoing calls from the function body
        calls = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.append(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.append(child.func.attr)
        
        unique_calls = list(set(calls))
        
        node_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": node_id,
            "file": self.file_path,
            "name": node.name,
            "type": type_name,
            "line": node.lineno,
            "calls": unique_calls
        })

# --- Independent Test Block ---
if __name__ == "__main__":
    import sys
    
    # Defaults to current directory
    target_dir = sys.argv[1] if len(sys.argv) > 1 else "."
    
    print(f"Mapping Logic in: {target_dir}")
    grapher = CodeGrapherMS()
    graph_data = grapher.scan_directory(target_dir)
    
    print(f"\n--- Scan Complete ---")
    print(f"Nodes Found: {graph_data['node_count']}")
    print(f"Edges Built: {graph_data['edge_count']}")
    
    # Save to JSON for inspection
    out_file = "code_graph_dump.json"
    with open(out_file, "w") as f:
        json.dump(graph_data, f, indent=2)
    print(f"Graph saved to {out_file}")
--------------------------------------------------------------------------------

-------------------- FILE: _CognitiveMemoryMS\cognitive_memory.py ----------------------
import uuid
import json
import logging
import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from pydantic import BaseModel, Field

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_MEMORY_FILE = Path("working_memory.jsonl")
FLUSH_THRESHOLD = 5  # Number of turns before summarizing to Long Term Memory
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("CognitiveMem")
# ==============================================================================

class MemoryEntry(BaseModel):
    """Atomic unit of memory."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)
    role: str # 'user', 'assistant', 'system', 'tool'
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

class CognitiveMemoryMS:
    """
    The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
    flushing to Long-Term Memory (Vector Store).
    """
    def __init__(
        self, 
        persistence_path: Path = DEFAULT_MEMORY_FILE,
        summarizer_func: Optional[Callable[[str], str]] = None,
        long_term_ingest_func: Optional[Callable[[str, Dict], None]] = None
    ):
        """
        :param persistence_path: Where to save the working memory (JSONL).
        :param summarizer_func: Function to compress text (e.g., using LLM or Heuristics).
        :param long_term_ingest_func: Function to save summaries to Vector DB.
        """
        self.file_path = persistence_path
        self.summarizer = summarizer_func
        self.ingestor = long_term_ingest_func
        
        self.working_memory: List[MemoryEntry] = []
        self._load_working_memory()

    # --- Working Memory Operations ---

    def add_entry(self, role: str, content: str, metadata: Dict = None) -> MemoryEntry:
        """Adds an item to working memory and persists it."""
        entry = MemoryEntry(role=role, content=content, metadata=metadata or {})
        self.working_memory.append(entry)
        self._append_to_file(entry)
        log.info(f"Added memory: [{role}] {content[:30]}...")
        return entry

    def get_context(self, limit: int = 10) -> str:
        """
        Returns the most recent conversation history formatted for an LLM.
        """
        recent = self.working_memory[-limit:]
        return "\n".join([f"{e.role.upper()}: {e.content}" for e in recent])

    def get_full_history(self) -> List[Dict]:
        """Returns the raw list of memory objects."""
        return [e.dict() for e in self.working_memory]

    # --- Consolidation (The "Sleep" Cycle) ---

    def commit_turn(self):
        """
        Signal that a "Turn" (User + AI response) is complete.
        Checks if memory is full and triggers a flush if needed.
        """
        if len(self.working_memory) >= FLUSH_THRESHOLD:
            self._flush_to_long_term()

    def _flush_to_long_term(self):
        """
        Compresses working memory into a summary and moves it to Long-Term storage.
        """
        if not self.summarizer or not self.ingestor:
            log.warning("Flush triggered but Summarizer/Ingestor not configured. Skipping.")
            return

        log.info("ðŸŒ€ Flushing Working Memory to Long-Term Storage...")
        
        # 1. Combine Text
        full_text = "\n".join([f"{e.role}: {e.content}" for e in self.working_memory])
        
        # 2. Summarize
        try:
            summary = self.summarizer(full_text)
            log.info(f"Summary generated: {summary[:50]}...")
        except Exception as e:
            log.error(f"Summarization failed: {e}")
            return

        # 3. Ingest into Vector DB
        try:
            meta = {
                "source": "cognitive_memory_flush", 
                "date": datetime.datetime.utcnow().isoformat(),
                "original_entry_count": len(self.working_memory)
            }
            self.ingestor(summary, meta)
            log.info("âœ… Saved to Long-Term Memory.")
        except Exception as e:
            log.error(f"Ingestion failed: {e}")
            return

        # 4. Clear Working Memory (but keep file history or archive it?)
        # For this pattern, we clear the 'Active' RAM, and maybe rotate the log file.
        self.working_memory.clear()
        self._rotate_log_file()

    # --- Persistence Helpers ---

    def _load_working_memory(self):
        """Rehydrates memory from the JSONL file."""
        if not self.file_path.exists():
            return
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.working_memory.append(MemoryEntry.parse_raw(line))
            log.info(f"Loaded {len(self.working_memory)} items from {self.file_path}")
        except Exception as e:
            log.error(f"Corrupt memory file: {e}")

    def _append_to_file(self, entry: MemoryEntry):
        """Appends a single entry to the JSONL log."""
        with open(self.file_path, 'a', encoding='utf-8') as f:
            f.write(entry.json() + "\n")

    def _rotate_log_file(self):
        """Renames the current log to an archive timestamp."""
        if self.file_path.exists():
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_name = self.file_path.with_name(f"memory_archive_{timestamp}.jsonl")
            self.file_path.rename(archive_name)
            log.info(f"Rotated memory log to {archive_name}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # 1. Setup Mock Dependencies
    def mock_summarizer(text):
        return f"SUMMARY OF {len(text)} CHARS: The user and AI discussed AI architecture."

    def mock_ingest(text, metadata):
        print(f"\n[VectorDB] Indexing: '{text}'\n[VectorDB] Meta: {metadata}")

    # 2. Initialize
    print("--- Initializing Cognitive Memory ---")
    mem = CognitiveMemoryMS(
        summarizer_func=mock_summarizer,
        long_term_ingest_func=mock_ingest
    )

    # 3. Simulate Conversation
    print("\n--- Simulating Conversation ---")
    mem.add_entry("user", "Hello, who are you?")
    mem.add_entry("assistant", "I am a Cognitive Agent.")
    mem.add_entry("user", "What is your memory capacity?")
    mem.add_entry("assistant", "I have a tiered memory system.")
    mem.add_entry("user", "That sounds complex.")
    
    print(f"\nCurrent Context:\n{mem.get_context()}")

    # 4. Trigger Flush (Threshold is 5)
    print("\n--- Triggering Memory Flush ---")
    mem.commit_turn() # Should trigger flush because count is 5
    
    print(f"\nWorking Memory after flush: {len(mem.working_memory)} items")
    
    # Cleanup
    if Path("working_memory.jsonl").exists():
        os.remove("working_memory.jsonl")
    # Clean up archives if any were made
    for p in Path(".").glob("memory_archive_*.jsonl"):
        os.remove(p)
--------------------------------------------------------------------------------

-------------------- FILE: _CognitiveMemoryMS\requirements.txt -------------------------
pip install pydantic
--------------------------------------------------------------------------------

-------------------- FILE: _ContextAggregatorMS\context_aggregator.py ------------------
import os
import fnmatch
import datetime
from pathlib import Path
from typing import Set, Optional

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
# Extensions known to be binary/non-text (Images, Archives, Executables)
DEFAULT_BINARY_EXTENSIONS = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".mp4", ".mkv", ".avi", ".mov", ".webm",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".exe", ".dll", ".so",
    ".db", ".sqlite", ".mdb", ".pyc", ".pyo", ".class", ".jar", ".wasm"
}

# Folders to ignore by default
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", ".env", 
    "dist", "build", "coverage", ".idea", ".vscode"
}
# ==============================================================================

class ContextAggregatorMS:
    """
    The Context Builder: Flattens a project folder into a single readable text file.
    """

    def __init__(self, max_file_size_mb: int = 1):
        self.max_file_size_bytes = max_file_size_mb * 1024 * 1024

    def aggregate(self, 
                  root_path: str, 
                  output_file: str, 
                  extra_exclusions: Optional[Set[str]] = None,
                  use_default_exclusions: bool = True) -> int:
        
        project_root = Path(root_path).resolve()
        out_path = Path(output_file).resolve()
        
        # Build Exclusions
        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_IGNORE_DIRS)
        if extra_exclusions:
            exclusions.update(extra_exclusions)

        # Build Binary List
        binary_exts = DEFAULT_BINARY_EXTENSIONS.copy() # Always keep binaries as base unless manually cleared
        
        file_count = 0
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        try:
            with open(out_path, "w", encoding="utf-8") as out_f:
                out_f.write(f"File Dump from Project: {project_root.name}\nGenerated: {timestamp}\n{'='*60}\n\n")

                for root, dirs, files in os.walk(project_root):
                    dirs[:] = [d for d in dirs if d not in exclusions]
                    
                    for filename in files:
                        if self._should_exclude(filename, exclusions): continue

                        file_path = Path(root) / filename
                        if file_path.resolve() == out_path: continue

                        if self._is_safe_to_dump(file_path, binary_exts):
                            self._write_file_content(out_f, file_path, project_root)
                            file_count += 1
                            
        except IOError as e: print(f"Error writing dump: {e}")
        return file_count

    def _should_exclude(self, filename: str, exclusions: Set[str]) -> bool:
        return any(fnmatch.fnmatch(filename, pattern) for pattern in exclusions)

    def _is_safe_to_dump(self, file_path: Path, binary_exts: Set[str]) -> bool:
        if "".join(file_path.suffixes).lower() in binary_exts: return False
        try:
            if file_path.stat().st_size > self.max_file_size_bytes: return False
            with open(file_path, 'rb') as f:
                if b'\0' in f.read(1024): return False
        except (IOError, OSError): return False
        return True

    def _write_file_content(self, out_f, file_path: Path, project_root: Path):
        relative_path = file_path.relative_to(project_root)
        header = f"\n{'-'*20} FILE: {relative_path} {'-'*20}\n"
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as in_f:
                out_f.write(header + in_f.read() + f"\n{'-'*60}\n")
        except Exception as e:
            out_f.write(f"\n[Error reading file: {e}]\n")

if __name__ == "__main__":
    print("ContextAggregator initialized. Check top of file to tweak defaults.")
--------------------------------------------------------------------------------

-------------------- FILE: _DiffEngineMS\diff_engine.py --------------------------------
import sqlite3
import difflib
import datetime
import uuid
import logging
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Any

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("diff_engine.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("DiffEngine")
# ==============================================================================

class DiffEngineMS:
    """
    The Timekeeper: Implements a 'Hybrid' versioning architecture.
    1. HEAD: Stores full current content for fast read access (UI/RAG).
    2. HISTORY: Stores diff deltas using difflib for audit trails.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. The Head (Fast Access Cache)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id TEXT PRIMARY KEY,
                    path TEXT UNIQUE NOT NULL,
                    content TEXT,
                    last_updated TIMESTAMP
                )
            """)
            
            # 2. The Rising Edge (Diff History)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS diff_log (
                    id TEXT PRIMARY KEY,
                    file_id TEXT NOT NULL,
                    timestamp TIMESTAMP,
                    change_type TEXT,  -- 'CREATE', 'EDIT', 'DELETE'
                    diff_blob TEXT,    -- The text output of difflib
                    author TEXT,
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)

    # --- Core Workflow ---

    def update_file(self, path: str, new_content: str, author: str = "agent") -> Dict[str, Any]:
        """
        The Atomic Update Operation:
        1. Checks current state.
        2. Calculates Diff.
        3. Writes Diff to History.
        4. Updates Head to New Content.
        
        Returns: Dict with status (changed/unchanged), file_id, and diff_summary.
        """
        path = str(Path(path).as_posix()) # Normalize path
        now = datetime.datetime.utcnow()
        
        with self._get_conn() as conn:
            # 1. Fetch Head
            row = conn.execute("SELECT id, content FROM files WHERE path = ?", (path,)).fetchone()
            
            if not row:
                # --- CASE: NEW FILE ---
                file_id = str(uuid.uuid4())
                conn.execute(
                    "INSERT INTO files (id, path, content, last_updated) VALUES (?, ?, ?, ?)",
                    (file_id, path, new_content, now)
                )
                self._log_diff(conn, file_id, "CREATE", "[New File Created]", author, now)
                log.info(f"Created new file: {path}")
                return {"status": "created", "file_id": file_id}

            # --- CASE: EXISTING FILE ---
            file_id = row['id']
            old_content = row['content'] or ""
            
            # 2. Calculate Diff
            # difflib needs lists of lines
            old_lines = old_content.splitlines(keepends=True)
            new_lines = new_content.splitlines(keepends=True)
            
            # Standard unified diff
            diff_gen = difflib.unified_diff(
                old_lines, new_lines, 
                fromfile=f"a/{path}", tofile=f"b/{path}",
                lineterm=''
            )
            diff_text = "".join(diff_gen)

            if not diff_text:
                return {"status": "unchanged", "file_id": file_id}

            # 3. Write History
            self._log_diff(conn, file_id, "EDIT", diff_text, author, now)

            # 4. Update Head
            conn.execute(
                "UPDATE files SET content = ?, last_updated = ? WHERE id = ?",
                (new_content, now, file_id)
            )
            log.info(f"Updated file: {path}")
            return {"status": "updated", "file_id": file_id, "diff_size": len(diff_text)}

    def _log_diff(self, conn, file_id, change_type, diff_text, author, timestamp):
        diff_id = str(uuid.uuid4())
        conn.execute(
            "INSERT INTO diff_log (id, file_id, timestamp, change_type, diff_blob, author) VALUES (?, ?, ?, ?, ?, ?)",
            (diff_id, file_id, timestamp, change_type, diff_text, author)
        )

    # --- Retrieval ---

    def get_head(self, path: str) -> Optional[str]:
        """Fast retrieval of current content."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT content FROM files WHERE path = ?", (path,)).fetchone()
            return row['content'] if row else None

    def get_history(self, path: str) -> List[Dict]:
        """Retrieves the full evolution history of a file."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT id FROM files WHERE path = ?", (path,)).fetchone()
            if not row: return []
            
            rows = conn.execute(
                "SELECT timestamp, change_type, diff_blob, author FROM diff_log WHERE file_id = ? ORDER BY timestamp DESC",
                (row['id'],)
            ).fetchall()
            
            return [dict(r) for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    engine = DiffEngineMS()
    
    print("--- 1. Creating File ---")
    engine.update_file("notes.txt", "Todo List:\n1. Buy Milk\n")
    
    print("\n--- 2. Updating File (The Rising Edge) ---")
    # Change: Add 'Buy Eggs', Remove 'Buy Milk' (Simulating a replacement)
    new_text = "Todo List:\n1. Buy Eggs\n2. Code Python\n"
    res = engine.update_file("notes.txt", new_text, author="Jacob")
    
    print(f"Update Result: {res['status']}")
    
    print("\n--- 3. Inspecting History ---")
    history = engine.get_history("notes.txt")
    for event in history:
        print(f"\n[{event['timestamp']}] {event['change_type']} by {event['author']}")
        print(f"Diff Preview:\n{event['diff_blob'].strip()}")

    print("\n--- 4. Inspecting Head (Cache) ---")
    print(engine.get_head("notes.txt"))
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)
--------------------------------------------------------------------------------

-------------------- FILE: _ExplorerWidgetMS\explorer_widget.py ------------------------
import tkinter as tk
from tkinter import ttk
import os
import threading
import queue
import fnmatch
from pathlib import Path
from typing import Dict, Set

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
DEFAULT_EXCLUDED_FOLDERS = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "target", "out",
    "bin", "obj", "Debug", "Release", "logs"
}
# ==============================================================================

class ExplorerWidget(ttk.Frame):
    """
    A standalone file system tree viewer.
    """
    
    GLYPH_CHECKED = "[X]"
    GLYPH_UNCHECKED = "[ ]"

    def __init__(self, parent, root_path: str = ".", use_default_exclusions: bool = True, *args, **kwargs):
        super().__init__(parent, *args, **kwargs)
        
        self.root_path = Path(root_path).resolve()
        self.use_defaults = use_default_exclusions
        self.gui_queue = queue.Queue()
        self.folder_item_states: Dict[str, str] = {} 
        self.state_lock = threading.RLock()
        
        self._setup_styles()
        self._build_ui()
        self.process_gui_queue()
        self.refresh_tree()

    def _setup_styles(self):
        style = ttk.Style()
        if "clam" in style.theme_names(): style.theme_use("clam")
        style.configure("Explorer.Treeview", background="#252526", foreground="lightgray", fieldbackground="#252526", borderwidth=0, font=("Consolas", 10))
        style.map("Explorer.Treeview", background=[('selected', '#007ACC')], foreground=[('selected', 'white')])

    def _build_ui(self):
        self.columnconfigure(0, weight=1); self.rowconfigure(0, weight=1)
        self.tree = ttk.Treeview(self, show="tree", columns=("size",), selectmode="none", style="Explorer.Treeview")
        self.tree.column("size", width=80, anchor="e")
        ysb = ttk.Scrollbar(self, orient="vertical", command=self.tree.yview)
        xsb = ttk.Scrollbar(self, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=ysb.set, xscrollcommand=xsb.set)
        self.tree.grid(row=0, column=0, sticky="nsew")
        ysb.grid(row=0, column=1, sticky="ns"); xsb.grid(row=1, column=0, sticky="ew")
        self.tree.bind("<ButtonRelease-1>", self._on_click)

    def refresh_tree(self):
        for i in self.tree.get_children(): self.tree.delete(i)
        with self.state_lock:
            self.folder_item_states.clear()
            self.folder_item_states[str(self.root_path)] = "checked"
        
        tree_data = [{'parent': '', 'iid': str(self.root_path), 'text': f" {self.root_path.name} (Root)", 'open': True}]
        self._scan_recursive(self.root_path, str(self.root_path), tree_data)
        
        for item in tree_data:
            self.tree.insert(item['parent'], "end", iid=item['iid'], text=item['text'], open=item.get('open', False))
            self.tree.set(item['iid'], "size", "...")

        self._refresh_visuals(str(self.root_path))
        threading.Thread(target=self._calc_sizes_thread, args=(str(self.root_path),), daemon=True).start()

    def _scan_recursive(self, current_path: Path, parent_id: str, data_list: list):
        try:
            items = sorted(current_path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))
            for item in items:
                if not item.is_dir(): continue
                path_str = str(item.resolve())
                
                state = "checked"
                if self.use_defaults and item.name in DEFAULT_EXCLUDED_FOLDERS:
                    state = "unchecked"
                
                with self.state_lock: self.folder_item_states[path_str] = state
                data_list.append({'parent': parent_id, 'iid': path_str, 'text': f" {item.name}"})
                self._scan_recursive(item, path_str, data_list)
        except (PermissionError, OSError): pass

    def _on_click(self, event):
        item_id = self.tree.identify_row(event.y)
        if not item_id: return
        with self.state_lock:
            curr = self.folder_item_states.get(item_id, "unchecked")
            self.folder_item_states[item_id] = "checked" if curr == "unchecked" else "unchecked"
        self._refresh_visuals(str(self.root_path))

    def _refresh_visuals(self, start_node):
        def _update(node_id):
            if not self.tree.exists(node_id): return
            with self.state_lock: state = self.folder_item_states.get(node_id, "unchecked")
            glyph = self.GLYPH_CHECKED if state == "checked" else self.GLYPH_UNCHECKED
            name = Path(node_id).name
            if node_id == str(self.root_path): name += " (Root)"
            self.tree.item(node_id, text=f"{glyph} {name}")
            for child in self.tree.get_children(node_id): _update(child)
        _update(start_node)

    def _calc_sizes_thread(self, root_id):
        # (Simplified for brevity, same logic as before but uses queue)
        pass 
        # Note: In production you'd include the full calc logic here as in the previous version

    def get_selected_paths(self) -> list[str]:
        selected = []
        with self.state_lock:
            for path, state in self.folder_item_states.items():
                if state == "checked": selected.append(path)
        return selected

    def process_gui_queue(self):
        while not self.gui_queue.empty():
            try: self.gui_queue.get_nowait()()
            except queue.Empty: pass
        self.after(100, self.process_gui_queue)

if __name__ == "__main__":
    print("ExplorerWidget initialized. Check top of file to tweak defaults.")
--------------------------------------------------------------------------------

-------------------- FILE: _FingerprintScannerMS\fingerprint_scanner.py ----------------
import hashlib
import os
import logging
from pathlib import Path
from typing import Dict, Set, Optional, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Folders to ignore during the scan (Standard developer noise)
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", 
    "dist", "build", "coverage", "target", "out", "bin", "obj",
    "_project_library", "_sandbox", "_logs"
}

# Files to ignore
DEFAULT_IGNORE_FILES = {
    ".DS_Store", "Thumbs.db", "*.log", "*.tmp", "*.lock"
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("Fingerprint")
# ==============================================================================

class FingerprintScannerMS:
    """
    The Detective: Scans a directory tree and generates a deterministic
    'Fingerprint' (SHA-256 Merkle Root) representing its exact state.
    """
    
    def __init__(self):
        pass

    def scan_project(self, root_path: str) -> Dict[str, Any]:
        """
        Scans the project and returns a comprehensive state object.
        output = {
            "root": str,
            "project_fingerprint": str (The global hash),
            "file_hashes": {rel_path: sha256},
            "file_count": int
        }
        """
        root = Path(root_path).resolve()
        if not root.exists():
            raise FileNotFoundError(f"Path not found: {root}")

        file_map = {}
        
        # 1. Walk and Hash
        # Use sorted() to ensure iteration order doesn't affect the final hash
        for path in sorted(root.rglob("*")):
            if path.is_file():
                if self._should_ignore(path, root):
                    continue
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_hash = self._hash_file(path)
                
                if file_hash:
                    file_map[rel_path] = file_hash

        # 2. Calculate Merkle Root (Global Fingerprint)
        # We sort by relative path to ensure deterministic ordering
        sorted_hashes = [file_map[p] for p in sorted(file_map.keys())]
        combined_data = "".join(sorted_hashes).encode('utf-8')
        project_fingerprint = hashlib.sha256(combined_data).hexdigest()

        log.info(f"Scanned {len(file_map)} files. Fingerprint: {project_fingerprint[:8]}...")

        return {
            "root": str(root),
            "project_fingerprint": project_fingerprint,
            "file_hashes": file_map,
            "file_count": len(file_map)
        }

    def _should_ignore(self, path: Path, root: Path) -> bool:
        """Checks path against exclusion lists."""
        try:
            rel_parts = path.relative_to(root).parts
            
            # Check directories
            # If any parent directory is in the ignore list, skip
            for part in rel_parts[:-1]: 
                if part in DEFAULT_IGNORE_DIRS:
                    return True
            
            # Check filename
            import fnmatch
            name = path.name
            if name in DEFAULT_IGNORE_FILES:
                return True
            if any(fnmatch.fnmatch(name, pat) for pat in DEFAULT_IGNORE_FILES):
                return True
                
            return False
        except ValueError:
            return True

    def _hash_file(self, path: Path) -> Optional[str]:
        """Reads file bytes and returns SHA256 hash."""
        try:
            # Read binary to avoid encoding issues and to hash exact content
            content = path.read_bytes()
            return hashlib.sha256(content).hexdigest()
        except (PermissionError, OSError):
            log.warning(f"Could not read/hash: {path}")
            return None

# --- Independent Test Block ---
if __name__ == "__main__":
    import time
    
    # 1. Create a dummy project
    test_dir = Path("test_fingerprint_proj")
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
    test_dir.mkdir()
    
    (test_dir / "main.py").write_text("print('hello')")
    (test_dir / "utils.py").write_text("def add(a,b): return a+b")
    
    scanner = FingerprintScannerMS()
    
    # 2. Initial Scan
    print("--- Scan 1 (Initial) ---")
    state_1 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 1: {state_1['project_fingerprint']}")
    
    # 3. Modify a file
    print("\n--- Modifying 'main.py' ---")
    time.sleep(0.1) # Ensure filesystem timestamp tick (though we hash content)
    (test_dir / "main.py").write_text("print('hello world')")
    
    # 4. Scan again
    print("--- Scan 2 (After Modification) ---")
    state_2 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 2: {state_2['project_fingerprint']}")
    
    # 5. Compare
    if state_1['project_fingerprint'] != state_2['project_fingerprint']:
        print("\nâœ… SUCCESS: Fingerprint changed as expected.")
        # Find the diff
        for path, h in state_2['file_hashes'].items():
            if state_1['file_hashes'].get(path) != h:
                print(f"   Changed File: {path}")
    else:
        print("\nâŒ FAILURE: Fingerprint did not change.")

    # Cleanup
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
--------------------------------------------------------------------------------

-------------------- FILE: _GitPilotMS\git_pilot.py ------------------------------------
import os
import subprocess
import threading
import queue
import time
import tkinter as tk
from tkinter import ttk, messagebox, simpledialog
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple, Any, Callable

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Detect if GitHub CLI is available
def which(cmd: str) -> Optional[str]:
    for p in os.environ.get("PATH", "").split(os.pathsep):
        f = Path(p) / cmd
        if os.name == 'nt':
            for ext in (".exe", ".cmd", ".bat"): 
                if (f.with_suffix(ext)).exists(): return str(f.with_suffix(ext))
        if f.exists() and os.access(f, os.X_OK): return str(f)
    return None

USE_GH = which("gh") is not None
# ==============================================================================

@dataclass
class GitStatusEntry:
    path: str
    index: str
    workdir: str

@dataclass
class GitStatus:
    repo_path: str
    branch: Optional[str]
    ahead: int
    behind: int
    entries: List[GitStatusEntry]

# --- Backend: The Git Wrapper ---
class GitCLI:
    """
    A robust wrapper around the git command line executable.
    """
    def __init__(self, repo_path: Path):
        self.root = self._resolve_repo_root(repo_path)

    def _run(self, args: List[str], *, cwd: Optional[Path] = None) -> Tuple[str, str]:
        cmd = ["git", *args]
        # Prevent console window popping up on Windows
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

        proc = subprocess.run(
            cmd,
            cwd=str(cwd or self.root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
            startupinfo=startupinfo
        )
        if proc.returncode != 0:
            raise RuntimeError(proc.stderr.strip() or f"git {' '.join(args)} failed")
        return proc.stdout, proc.stderr

    @staticmethod
    def _resolve_repo_root(path: Path) -> Path:
        path = path.resolve()
        if (path / ".git").exists(): return path
        p = path
        while True:
            if (p / ".git").exists(): return p
            if p.parent == p: break
            p = p.parent
        return path

    def init(self) -> None:
        self._run(["init"])

    def status(self) -> GitStatus:
        try:
            out, _ = self._run(["rev-parse", "--abbrev-ref", "HEAD"])
            branch = out.strip()
        except Exception: branch = None
        
        ahead = behind = 0
        try:
            out, _ = self._run(["rev-list", "--left-right", "--count", "@{upstream}...HEAD"])
            left, right = out.strip().split()
            behind, ahead = int(left), int(right)
        except Exception: pass
        
        out, _ = self._run(["status", "--porcelain=v1"])
        entries = []
        for line in out.splitlines():
            if not line.strip(): continue
            xy = line[:2]
            path = line[3:]
            index, work = xy[0], xy[1]
            entries.append(GitStatusEntry(path=path, index=index, workdir=work))
        return GitStatus(str(self.root), branch, ahead, behind, entries)

    def stage(self, paths: List[str]) -> None:
        if paths: self._run(["add", "--"] + paths)

    def unstage(self, paths: List[str]) -> None:
        if paths: self._run(["reset", "HEAD", "--"] + paths)

    def diff(self, file: Optional[str] = None) -> str:
        args = ["diff"]
        if file: args += ["--", file]
        out, _ = self._run(args)
        return out

    def commit(self, message: str, author_name: str, author_email: str) -> str:
        env = os.environ.copy()
        if author_name: 
            env["GIT_AUTHOR_NAME"] = author_name
            env["GIT_COMMITTER_NAME"] = author_name
        if author_email:
            env["GIT_AUTHOR_EMAIL"] = author_email
            env["GIT_COMMITTER_EMAIL"] = author_email
            
        proc = subprocess.run(
            ["git", "commit", "-m", message], 
            cwd=str(self.root), 
            capture_output=True, 
            text=True, 
            env=env
        )
        if proc.returncode != 0: raise RuntimeError(proc.stderr.strip() or proc.stdout.strip())
        out, _ = self._run(["rev-parse", "HEAD"])
        return out.strip()

    def log(self, limit: int = 100) -> List[Tuple[str, str, str, int]]:
        fmt = "%H%x1f%s%x1f%an%x1f%at"
        try:
            out, _ = self._run(["log", f"-n{limit}", f"--pretty=format:{fmt}"])
            items = []
            for line in out.splitlines():
                commit, summary, author, at = line.split("\x1f")
                items.append((commit, summary, author, int(at)))
            return items
        except Exception: return []

    def branches(self) -> List[Tuple[str, bool]]:
        try:
            out, _ = self._run(["branch"])
            res = []
            for line in out.splitlines():
                is_head = line.strip().startswith("*")
                name = line.replace("*", "", 1).strip()
                res.append((name, is_head))
            return res
        except Exception: return []

    def checkout(self, name: str, create: bool = False) -> None:
        if create: self._run(["checkout", "-B", name])
        else: self._run(["checkout", name])

    def push(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        args = ["push", remote]
        if branch: args.append(branch)
        out, _ = self._run(args)
        return out

    def pull(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        if branch: out, _ = self._run(["pull", remote, branch])
        else: out, _ = self._run(["pull", remote])
        return out

# --- Threading Helper ---
class Worker:
    def __init__(self, ui_callback):
        self.q = queue.Queue()
        self.ui_callback = ui_callback
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()

    def submit(self, op: str, func, *args, **kwargs):
        self.q.put((op, func, args, kwargs))

    def _loop(self):
        while True:
            op, func, args, kwargs = self.q.get()
            try:
                result = op, True, func(*args, **kwargs)
            except Exception as e:
                result = op, False, e
            finally:
                self.ui_callback(result)

# --- Frontend: The GUI Panel ---
class GitPilotPanel(ttk.Frame):
    def __init__(self, master, initial_path: Optional[Path] = None, **kwargs):
        super().__init__(master, **kwargs)
        self.repo_path = None
        self.git = None
        self.worker = Worker(self._on_worker_done)

        self._build_ui()
        if initial_path:
            self.set_repo(initial_path)

    def set_repo(self, path: Path):
        try:
            self.git = GitCLI(path)
            self.repo_path = self.git.root
            self.path_var.set(f"Repo: {self.repo_path}")
            self._refresh()
        except Exception as e:
            self.path_var.set(f"Error: {e}")

    def _build_ui(self):
        self.columnconfigure(0, weight=1)
        self.rowconfigure(1, weight=1)

        # Status Bar
        bar = ttk.Frame(self)
        bar.grid(row=0, column=0, sticky="ew")
        self.path_var = tk.StringVar(value="No Repo Selected")
        self.busy_var = tk.StringVar()
        ttk.Label(bar, textvariable=self.path_var).pack(side="left", padx=5)
        ttk.Label(bar, textvariable=self.busy_var, foreground="blue").pack(side="right", padx=5)

        # Tabs
        self.nb = ttk.Notebook(self)
        self.nb.grid(row=1, column=0, sticky="nsew")
        
        self.tab_changes = self._build_changes_tab(self.nb)
        self.tab_log = self._build_log_tab(self.nb)
        
        self.nb.add(self.tab_changes, text="Changes")
        self.nb.add(self.tab_log, text="History")

    def _build_changes_tab(self, parent):
        frame = ttk.Frame(parent)
        paned = ttk.PanedWindow(frame, orient=tk.VERTICAL)
        paned.pack(fill="both", expand=True)

        # File List
        top = ttk.Frame(paned)
        top.rowconfigure(1, weight=1)
        top.columnconfigure(0, weight=1)
        
        # Toolbar
        tb = ttk.Frame(top)
        tb.grid(row=0, column=0, sticky="ew")
        ttk.Button(tb, text="Refresh", command=self._refresh).pack(side="left")
        ttk.Button(tb, text="Stage", command=self._stage).pack(side="left")
        ttk.Button(tb, text="Unstage", command=self._unstage).pack(side="left")
        ttk.Button(tb, text="Diff", command=self._show_diff).pack(side="left")
        ttk.Button(tb, text="Push", command=self._push).pack(side="left", padx=10)
        ttk.Button(tb, text="Pull", command=self._pull).pack(side="left")

        # Treeview
        self.tree = ttk.Treeview(top, columns=("path", "idx", "wd"), show="headings", selectmode="extended")
        self.tree.heading("path", text="Path")
        self.tree.heading("idx", text="Index")
        self.tree.heading("wd", text="Workdir")
        self.tree.column("path", width=400)
        self.tree.column("idx", width=50, anchor="center")
        self.tree.column("wd", width=50, anchor="center")
        self.tree.grid(row=1, column=0, sticky="nsew")
        
        paned.add(top, weight=3)

        # Commit Area
        bot = ttk.Frame(paned)
        bot.columnconfigure(1, weight=1)
        ttk.Label(bot, text="Message:").grid(row=0, column=0, sticky="nw")
        self.msg_text = tk.Text(bot, height=4)
        self.msg_text.grid(row=0, column=1, sticky="nsew")
        ttk.Button(bot, text="Commit", command=self._commit).grid(row=1, column=1, sticky="e", pady=5)
        
        paned.add(bot, weight=1)
        return frame

    def _build_log_tab(self, parent):
        frame = ttk.Frame(parent)
        self.log_tree = ttk.Treeview(frame, columns=("sha", "msg", "auth", "time"), show="headings")
        self.log_tree.heading("sha", text="SHA")
        self.log_tree.heading("msg", text="Message")
        self.log_tree.heading("auth", text="Author")
        self.log_tree.heading("time", text="Time")
        self.log_tree.column("sha", width=80)
        self.log_tree.column("msg", width=400)
        self.log_tree.pack(fill="both", expand=True)
        return frame

    # --- Actions ---

    def _submit(self, label, func, *args):
        self.busy_var.set(f"{label}...")
        self.worker.submit(label, func, *args)

    def _on_worker_done(self, result):
        self.after(0, self._handle_result, result)

    def _handle_result(self, result):
        label, ok, data = result
        self.busy_var.set("")
        if not ok:
            messagebox.showerror("Error", str(data))
            return
        
        if label == "refresh":
            status, logs = data
            self.tree.delete(*self.tree.get_children())
            for e in status.entries:
                self.tree.insert("", "end", values=(e.path, e.index, e.workdir))
            
            self.log_tree.delete(*self.log_tree.get_children())
            for sha, msg, auth, ts in logs:
                t_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(ts))
                self.log_tree.insert("", "end", values=(sha[:7], msg, auth, t_str))
        
        if label == "diff":
            top = tk.Toplevel(self)
            top.title("Diff")
            txt = tk.Text(top, font=("Consolas", 10))
            txt.pack(fill="both", expand=True)
            txt.insert("1.0", data)

        if label in ["stage", "unstage", "commit", "push", "pull"]:
            self._refresh()

    def _refresh(self):
        if not self.git: return
        self._submit("refresh", lambda: (self.git.status(), self.git.log()))

    def _get_selection(self):
        return [self.tree.item(i)['values'][0] for i in self.tree.selection()]

    def _stage(self):
        paths = self._get_selection()
        if paths: self._submit("stage", self.git.stage, paths)

    def _unstage(self):
        paths = self._get_selection()
        if paths: self._submit("unstage", self.git.unstage, paths)

    def _commit(self):
        msg = self.msg_text.get("1.0", "end").strip()
        if not msg: return
        self._submit("commit", self.git.commit, msg, "GitPilot", "pilot@local")
        self.msg_text.delete("1.0", "end")

    def _push(self):
        self._submit("push", self.git.push)

    def _pull(self):
        self._submit("pull", self.git.pull)

    def _show_diff(self):
        sel = self._get_selection()
        file = sel[0] if sel else None
        self._submit("diff", self.git.diff, file)

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Git Pilot Test")
    root.geometry("800x600")
    
    # Use current directory
    cwd = Path(os.getcwd())
    
    panel = GitPilotPanel(root, initial_path=cwd)
    panel.pack(fill="both", expand=True)
    
    root.mainloop()
--------------------------------------------------------------------------------

-------------------- FILE: _GraphEngineMS\graph_engine.py ------------------------------
import pygame
import math
import random

# Initialize font module globally once
pygame.font.init()

class GraphRenderer:
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        self.width = width
        self.height = height
        self.bg_color = bg_color
        
        # Surface for drawing
        self.surface = pygame.Surface((width, height))
        
        # Camera State
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction State
        self.dragged_node_idx = None
        self.hovered_node_idx = None

    def resize(self, width, height):
        """Re-initialize surface on window resize"""
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    def set_data(self, nodes, links):
        """
        Expects nodes to have: id, type ('file'|'concept'), label
        Expects links to be tuples of indices: (source_idx, target_idx)
        """
        self.nodes = nodes
        self.links = links
        
        # Initialize physics state for new nodes
        for n in self.nodes:
            if 'x' not in n:
                n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
                n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Cache visual properties based on type
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # #007ACC (Blue)
                n['_radius'] = 6
            else:
                n['_color'] = (160, 32, 240) # #A020F0 (Purple)
                n['_radius'] = 8

    # --- INPUT HANDLING (Coordinate Transforms) ---
    
    def screen_to_world(self, sx, sy):
        """Convert Tkinter screen coordinates to Physics world coordinates"""
        # (Screen - Center) / Zoom + Center - Camera
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        # Find clicked node
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2: # Generous hit box
                self.dragged_node_idx = i
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            # Move the node directly
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
        else:
            # Hover check
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            
            return prev_hover != self.hovered_node_idx # Return True if redraw needed

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        # Zoom towards mouse pointer logic could go here
        # For now, simple center zoom
        old_zoom = self.zoom
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    # --- PHYSICS ---

    def step_physics(self):
        if not self.nodes: return

        # Constants matching D3 feel
        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.9
        
        cx, cy = self.width / 2, self.height / 2

        # 1. Repulsion (Nodes push apart)
        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue # Don't move dragged node
            
            fx, fy = 0, 0
            
            # Center Gravity (Pull lightly to middle so they don't drift away)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # Node-Node Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Force = k / dist^2
                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 2. Attraction (Links pull together)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            
            # Spring force
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 3. Apply Velocity
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']

    # --- RENDERING ---

    def get_image_bytes(self):
        """ Renders the scene and returns raw RGB bytes + size """
        self.surface.fill(self.bg_color)
        
        # Pre-calculate center offset
        cx, cy = self.width / 2, self.height / 2
        
        # Helper for transforms
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # 1. Draw Links
        for u, v in self.links:
            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # 2. Draw Nodes
        for i, n in enumerate(self.nodes):
            sx, sy = to_screen(n['x'], n['y'])
            
            # Culling: Don't draw if off screen
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20:
                continue
                
            rad = int(n['_radius'] * self.zoom)
            col = n['_color']
            
            # Highlight hovered
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)
            
            pygame.draw.circle(self.surface, col, (sx, sy), rad)
            
            # Draw Labels (only if zoomed in enough or hovered)
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

        return pygame.image.tostring(self.surface, 'RGB')
--------------------------------------------------------------------------------

-------------------- FILE: _GraphEngineMS\graph_view.py --------------------------------
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import os

# Use relative import for sibling module
from .graph_engine import GraphRenderer

class GraphView(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        self.pack(fill="both", expand=True)
        
        # Widget logic
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)
        
        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None # Keep reference to avoid GC
        
        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)
        # Zoom bindings
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1)) # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9)) # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)
        
        # Logic State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False
        
        # Start Loop
        self.animate()

    def load_from_db(self, db_path):
        """
        Fetches Nodes and Edges from the SQLite DB and formats them 
        for the Pygame Physics Engine.
        """
        if not os.path.exists(db_path):
            print(f"GraphView Error: DB not found at {db_path}")
            return

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # 1. Fetch Nodes
        # Schema: id, type, label, data_json
        try:
            db_nodes = cursor.execute("SELECT id, type, label FROM graph_nodes").fetchall()
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
        except sqlite3.OperationalError:
            print("Graph tables missing. Run Ingest first.")
            conn.close()
            return

        conn.close()

        # 2. Map String IDs -> Integer Indices
        # The physics engine uses list indices (0, 1, 2) for speed.
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label = row
            id_to_index[node_id] = idx
            formatted_nodes.append({
                'id': node_id,
                'type': n_type,
                'label': label
            })

        # 3. Translate Edges
        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                u = id_to_index[src]
                v = id_to_index[tgt]
                formatted_links.append((u, v))

        # 4. Push to Engine
        print(f"Graph Loaded: {len(formatted_nodes)} Nodes, {len(formatted_links)} Edges")
        self.engine.set_data(formatted_nodes, formatted_links)

    # --- EVENT HANDLERS ---
    
    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        self.is_dragging_node = hit

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        else:
            # Pan Camera
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        # Just update hover state for aesthetics
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)

    def on_windows_scroll(self, event):
        # Windows typically gives 120 or -120
        if event.delta > 0:
            self.on_zoom(1.1)
        else:
            self.on_zoom(0.9)

    # --- RENDER LOOP ---

    def animate(self):
        # 1. Step Physics
        self.engine.step_
--------------------------------------------------------------------------------

-------------------- FILE: _HeruisticSumMS\heuristic_summarizer.py ---------------------
import re
import os
from typing import List, Optional

# ==============================================================================
# CONFIGURATION: REGEX PATTERNS
# ==============================================================================
# Captures: def my_func, class MyClass, function myFunc, interface MyInterface
SIG_RE = re.compile(r'^\s*(def|class|function|interface|struct|impl|func)\s+([A-Za-z_][A-Za-z0-9_]*)')

# Captures: # Heading, ## Subheading
MD_HDR_RE = re.compile(r'^\s{0,3}(#{1,3})\s+(.+)')

# Captures: """ Docstring """ or ''' Docstring ''' (Start of block)
DOC_RE = re.compile(r'^\s*("{3}|\'{3})(.*)', re.DOTALL)
# ==============================================================================

class HeuristicSumMS:
    """
    The Skimmer: Generates quick summaries of code/text files without AI.
    Scans for high-value lines (headers, signatures, docstrings) and concatenates them.
    """

    def summarize(self, text: str, filename: str = "", max_chars: int = 480) -> str:
        """
        Generates a summary string from the provided text.
        """
        lines = text.splitlines()
        picks = []

        # 1. Scan top 20 lines for Markdown Headers
        for ln in lines[:20]:
            m = MD_HDR_RE.match(ln)
            if m:
                picks.append(f"Heading: {m.group(2).strip()}")

        # 2. Scan top 40 lines for Code Signatures (Functions/Classes)
        for ln in lines[:40]:
            m = SIG_RE.match(ln)
            if m:
                picks.append(f"{m.group(1)} {m.group(2)}")

        # 3. Check for Docstrings / Preamble
        if lines:
            # Join first 80 lines to check for multi-line docstrings
            joined = "\n".join(lines[:80])
            m = DOC_RE.match(joined)
            if m:
                # Grab the first few lines of the docstring content
                after = joined.splitlines()[1:3]
                if after:
                    clean_doc = " ".join(s.strip() for s in after).strip()
                    picks.append(f"Doc: {clean_doc}")

        # 4. Fallback: First non-empty line if nothing else found
        if not picks:
            head = " ".join(l.strip() for l in lines[:2] if l.strip())
            if head:
                picks.append(head)

        # 5. Add Filename Context
        if filename:
            picks.append(f"[{os.path.basename(filename)}]")

        # 6. Deduplicate and Format
        seen = set()
        uniq = []
        for p in picks:
            if p and p not in seen:
                uniq.append(p)
                seen.add(p)

        summary = " | ".join(uniq)
        
        # 7. Truncate
        if len(summary) > max_chars:
            summary = summary[:max_chars-3] + "..."
            
        return summary.strip() if summary else "[No summary available]"

# --- Independent Test Block ---
if __name__ == "__main__":
    skimmer = HeuristicSumMS()
    
    # Test 1: Python Code
    py_code = """
    class DataProcessor:
        '''
        Handles the transformation of raw input data into structured formats.
        '''
        def process(self, data):
            pass
    """
    print(f"Python Summary: {skimmer.summarize(py_code, 'processor.py')}")

    # Test 2: Markdown
    md_text = """
    # Project Roadmap
    ## Phase 1
    We begin with ingestion.
    """
    print(f"Markdown Summary: {skimmer.summarize(md_text, 'README.md')}")
--------------------------------------------------------------------------------

-------------------- FILE: _IngestEngineMS\ingest_engine.py ----------------------------
import os
import time
import re
import sqlite3
import requests
import json
from typing import List, Generator, Dict, Any, Optional, Set
from dataclasses import dataclass

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

@dataclass
class IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

class SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    Used to generate the 'DEPENDS_ON' edges in the Knowledge Graph.
    """
    def __init__(self):
        # Python: "from x import y", "import x"
        self.py_pattern = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')
        # JS/TS: "import ... from 'x'", "require('x')"
        self.js_pattern = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        ext = os.path.splitext(file_path)[1].lower()
        
        lines = content.split('\n')
        for line in lines:
            match = None
            if ext == '.py':
                match = self.py_pattern.match(line)
            elif ext in ['.js', '.ts', '.tsx', '.jsx']:
                match = self.js_pattern.search(line)
            
            if match:
                # Clean up the module name (e.g., "backend.database" -> "database")
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        
        return dependencies

class IngestEngine:
    """
    The Heavy Lifter: Reads files, chunks text, fetches embeddings,
    populates the Graph Nodes, and weaves Graph Edges.
    """
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.stop_signal = False
        self.weaver = SynapseWeaver()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags")
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    def process_files(self, file_paths: List[str], model_name: str = "none") -> Generator[IngestStatus, None, None]:
        total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Optimization settings
        cursor.execute("PRAGMA synchronous = OFF")
        cursor.execute("PRAGMA journal_mode = MEMORY")

        # Memory for graph weaving (Node Name -> Node ID)
        node_registry = {}
        file_contents = {} # Cache content for the weaving pass

        # --- PHASE 1: INGESTION (Files, Chunks, Nodes) ---
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, "Ingestion Aborted.")
                break

            filename = os.path.basename(file_path)

            # 1. Read
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                file_contents[filename] = content # Cache for Phase 2
            except Exception as e:
                yield IngestStatus(file_path, (idx/total)*100, idx, total, f"Error: {e}")
                continue

            # 2. Track File
            try:
                cursor.execute("INSERT OR REPLACE INTO files (path, last_updated) VALUES (?, ?)", 
                              (file_path, time.time()))
                file_id = cursor.lastrowid
            except sqlite3.Error:
                continue

            # 3. Create Graph Node (for Visualization)
            # We use the filename as the unique ID for the graph to make linking easier
            cursor.execute("""
                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                VALUES (?, ?, ?, ?)
            """, (filename, 'file', filename, json.dumps({"path": file_path})))
            
            node_registry[filename] = filename

            # 4. Chunking & Embedding
            chunks = self._chunk_text(content)
            
            for i, chunk_text in enumerate(chunks):
                if self.stop_signal: break
                
                embedding = None
                if model_name != "none":
                    embedding = self._get_embedding(model_name, chunk_text)
                
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                
                cursor.execute("""
                    INSERT INTO chunks (file_id, chunk_index, content, embedding)
                    VALUES (?, ?, ?, ?)
                """, (file_id, i, chunk_text, emb_blob))

                # Visual Feedback
                thought_frame = {
                    "id": f"{file_id}_{i}",
                    "file": filename,
                    "chunk_index": i,
                    "content": chunk_text,
                    "vector_preview": embedding[:20] if embedding else [],
                    "concept_color": "#007ACC"
                }
                
                yield IngestStatus(
                    current_file=filename,
                    progress_percent=((idx + (i/len(chunks))) / total) * 100,
                    processed_files=idx,
                    total_files=total,
                    log_message=f"Processing {filename}...",
                    thought_frame=thought_frame
                )

            # Checkpoint per file
            conn.commit()

        # --- PHASE 2: WEAVING (Edges) ---
        yield IngestStatus("Graph", 100, total, total, "Weaving Knowledge Graph...")
        
        edge_count = 0
        for filename, content in file_contents.items():
            if self.stop_signal: break
            
            # Find imports
            deps = self.weaver.extract_dependencies(content, filename)
            
            for dep in deps:
                # Naive matching: if 'database' is imported, look for 'database.py' or 'database.ts'
                # in our registry.
                target_id = None
                for potential_match in node_registry.keys():
                    if potential_match.startswith(dep + '.') or potential_match == dep:
                        target_id = potential_match
                        break
                
                if target_id and target_id != filename:
                    try:
                        cursor.execute("""
                            INSERT OR IGNORE INTO graph_edges (source, target, weight)
                            VALUES (?, ?, 1.0)
                        """, (filename, target_id))
                        edge_count += 1
                    except:
                        pass

        conn.commit()
        conn.close()

        yield IngestStatus(
            current_file="Complete",
            progress_percent=100,
            processed_files=total,
            total_files=total,
            log_message=f"Ingestion Complete. Created {edge_count} dependency edges."
        )

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        if len(text) < chunk_size: return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += (chunk_size - overlap)
        return chunks

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": model, "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
        return None

# --- Independent Test Block ---
if __name__ == "__main__":
    TEST_DB = "test_ingest_v2.db"
    
    # Init DB Schema manually for test
    conn = sqlite3.connect(TEST_DB)
    conn.execute("CREATE TABLE IF NOT EXISTS files (id INTEGER PRIMARY KEY, path TEXT, last_updated REAL)")
    conn.execute("CREATE TABLE IF NOT EXISTS chunks (id INTEGER PRIMARY KEY, file_id INT, chunk_index INT, content TEXT, embedding BLOB)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, weight REAL)")
    conn.close()

    engine = IngestEngine(TEST_DB)
    # Self-ingest to test dependency parsing
    files = ["_IngestEngineMS.py"] 
    
    print("Running Ingest V2...")
    for status in engine.process_files(files, "none"):
        print(f"[{status.progress_percent:.0f}%] {status.log_message}")
    
    # Verify Edges
    conn = sqlite3.connect(TEST_DB)
    edges = conn.execute("SELECT * FROM graph_edges").fetchall()
    nodes = conn.execute("SELECT * FROM graph_nodes").fetchall()
    print(f"\nResult: {len(nodes)} Nodes, {len(edges)} Edges.")
    conn.close()
    
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)
--------------------------------------------------------------------------------

-------------------- FILE: _IsoProcessMS\iso_process.py --------------------------------
import multiprocessing as mp
import logging
import logging.handlers
import time
import queue
from typing import Any, Dict, Optional

# ==============================================================================
# WORKER LOGIC (Runs in Child Process)
# ==============================================================================
def _isolated_worker(result_queue: mp.Queue, log_queue: mp.Queue, payload: Any, config: Dict[str, Any]):
    """
    Entry point for the child process.
    Configures a logging handler to send records back to the parent.
    """
    # 1. Setup Logging Bridge
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    # Clear default handlers to avoid duplicate prints in child
    for h in root.handlers[:]:
        root.removeHandler(h)
    
    # Send all logs to the parent via the queue
    qh = logging.handlers.QueueHandler(log_queue)
    root.addHandler(qh)
    
    log = logging.getLogger("IsoWorker")

    try:
        log.info(f"Worker PID {mp.current_process().pid} started.")
        
        # --- 2. Heavy Imports (Simulated) ---
        log.info("Loading heavy libraries (Torch/Transformers)...")
        # from transformers import pipeline
        time.sleep(0.2) # Simulate import time

        # --- 3. The Logic ---
        model_name = config.get("model_name", "default-model")
        log.info(f"Initializing model '{model_name}'...")
        
        # Simulate processing steps with progress reporting
        for i in range(1, 4):
            time.sleep(0.3)
            log.info(f"Processing chunk {i}/3...")
        
        processed_data = f"Processed({payload}) via {model_name}"
        
        # --- 4. Return Result ---
        log.info("Work complete. Returning result.")
        result_queue.put({"success": True, "data": processed_data})

    except Exception as e:
        log.exception("Critical failure in worker process.")
        result_queue.put({"success": False, "error": str(e)})

# ==============================================================================
# PARENT CONTROLLER (Runs in Main Process)
# ==============================================================================
class IsoProcessMS:
    """
    The Safety Valve: Spawns isolated processes with real-time logging feedback.
    """
    def __init__(self, timeout_seconds: int = 60):
        self.timeout = timeout_seconds
        
        # Setup main logger
        self.log = logging.getLogger("IsoParent")
        if not self.log.handlers:
            logging.basicConfig(
                level=logging.INFO, 
                format='%(asctime)s [%(name)s] %(message)s',
                datefmt='%H:%M:%S'
            )

    def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
        config = config or {}
        
        # 1. Setup Queues
        ctx = mp.get_context("spawn")
        result_queue = ctx.Queue()
        log_queue = ctx.Queue()

        # 2. Setup Log Listener (The "Ear" of the parent)
        # This thread pulls logs from the queue and handles them in the main process
        listener = logging.handlers.QueueListener(log_queue, *logging.getLogger().handlers)
        listener.start()

        # 3. Launch Process
        process = ctx.Process(
            target=_isolated_worker,
            args=(result_queue, log_queue, payload, config)
        )
        
        self.log.info("ðŸš€ Spawning isolated process...")
        process.start()
        
        try:
            # 4. Wait for Result
            result_packet = result_queue.get(timeout=self.timeout)
            process.join()

            if result_packet["success"]:
                return result_packet["data"]
            else:
                raise RuntimeError(f"Worker Error: {result_packet['error']}")

        except queue.Empty:
            self.log.error("â³ Worker timed out! Terminating...")
            process.terminate()
            process.join()
            raise TimeoutError(f"Task exceeded {self.timeout}s limit.")
            
        finally:
            # Clean up the log listener so it doesn't hang
            listener.stop()

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing IsoProcessMS with Live Logging ---")
    iso = IsoProcessMS(timeout_seconds=5)
    
    try:
        result = iso.execute("Sensitive Data", {"model_name": "DeepSeek-V3"})
        print(f"\n[Parent] Final Result: {result}")
    except Exception as e:
        print(f"\n[Parent] Failed: {e}")
--------------------------------------------------------------------------------

-------------------- FILE: _LexicalSearchMS\lexical_search.py --------------------------
import sqlite3
import json
import os
from typing import List, Dict, Any, Optional

class LexicalSearchMS:
    """
    The Librarian's Index: A lightweight, AI-free search engine.
    
    Uses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).
    Ideal for environments where installing PyTorch/Transformers is impossible
    or overkill.
    """

    def __init__(self, db_path: str = "lexical_index.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """
        Sets up the schema. 
        Uses Triggers to automatically keep the FTS index in sync with the main table.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # 1. Main Content Table (Stores the actual data)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                metadata TEXT  -- JSON blob for extra info (path, author, etc)
            );
        """)
        
        # 2. Virtual FTS Table (The Search Index)
        # content='documents' means it references the table above (saves space)
        cur.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                content,
                content='documents',
                content_rowid='rowid'  -- Internal SQLite mapping
            );
        """)

        # 3. Triggers (The "Magic" - Auto-sync index on Insert/Delete/Update)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ai AFTER INSERT ON documents BEGIN
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ad AFTER DELETE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_au AFTER UPDATE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        
        conn.commit()
        conn.close()

    def add_document(self, doc_id: str, text: str, metadata: Dict[str, Any] = None):
        """
        Adds or updates a document in the index.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        meta_json = json.dumps(metadata or {})
        
        # Upsert logic (Replace if ID exists)
        cur.execute("""
            INSERT OR REPLACE INTO documents (id, content, metadata)
            VALUES (?, ?, ?)
        """, (doc_id, text, meta_json))
        
        conn.commit()
        conn.close()

    def search(self, query: str, top_k: int = 20) -> List[Dict]:
        """
        Performs a BM25 Ranked Search.
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row # Allows dict-like access
        cur = conn.cursor()
        
        try:
            # The SQL Magic: 'bm25(documents_fts)' calculates relevance score
            sql = """
                SELECT 
                    d.id, 
                    d.content, 
                    d.metadata,
                    snippet(documents_fts, 0, '<b>', '</b>', '...', 15) as preview,
                    bm25(documents_fts) as score
                FROM documents_fts 
                JOIN documents d ON d.rowid = documents_fts.rowid
                WHERE documents_fts MATCH ? 
                ORDER BY score ASC
                LIMIT ?
            """
            # FTS5 query syntax: quotes typically help with special chars
            safe_query = f'"{query}"'
            rows = cur.execute(sql, (safe_query, top_k)).fetchall()
            
            results = []
            for r in rows:
                results.append({
                    "id": r['id'],
                    "score": round(r['score'], 4),
                    "preview": r['preview'], # FTS5 auto-generates snippets!
                    "metadata": json.loads(r['metadata']),
                    "full_content": r['content']
                })
            
            return results
            
        except sqlite3.OperationalError as e:
            # Usually happens if query syntax is bad (e.g. unmatched quotes)
            print(f"Search syntax error: {e}")
            return []
        finally:
            conn.close()

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    db_name = "test_lexical.db"
    
    # 1. Init
    engine = LexicalSearchMS(db_name)
    
    # 2. Ingest Data
    print("Ingesting test data...")
    engine.add_document("doc1", "Python is a great language for data science.", {"category": "coding"})
    engine.add_document("doc2", "The snake python is a reptile found in jungles.", {"category": "biology"})
    engine.add_document("doc3", "Data science involves python, pandas, and SQL.", {"category": "coding"})
    
    # 3. Search
    query = "python data"
    print(f"\nSearching for: '{query}'")
    hits = engine.search(query)
    
    for hit in hits:
        print(f"[{hit['score']:.4f}] {hit['id']} ({hit['metadata']['category']})")
        print(f"   Preview: {hit['preview']}")
        
    # Cleanup
    if os.path.exists(db_name):
        os.remove(db_name)
--------------------------------------------------------------------------------

-------------------- FILE: _LibrarianServiceMS\librarian_service.py --------------------
import os
import shutil
import sqlite3
import time
from pathlib import Path
from typing import List, Dict, Optional

class LibrarianMS:
    """
    The Librarian: Manages the physical creation, deletion, and listing
    of Knowledge Base (KB) files.
    """
    
    def __init__(self, storage_dir: str = "./cortex_dbs"):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)

    def list_kbs(self) -> List[str]:
        """
        Scans the storage directory for .db files.
        Equivalent to api.listKBs() in Sidebar.tsx.
        """
        if not self.storage_dir.exists():
            return []
        
        # Return simple filenames sorted by modification time (newest first)
        files = list(self.storage_dir.glob("*.db"))
        files.sort(key=os.path.getmtime, reverse=True)
        return [f.name for f in files]

    def create_kb(self, name: str) -> Dict[str, str]:
        """
        Creates a new SQLite database and initializes the Cortex Schema.
        """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            raise FileExistsError(f"Knowledge Base '{safe_name}' already exists.")

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # --- THE CORTEX SCHEMA ---
            # 1. System Config: Stores version and global metadata
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS config (
                    key TEXT PRIMARY KEY,
                    value TEXT
                )
            """)
            
            # 2. Files: Tracks scanned files to avoid re-ingesting unchanged ones
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    path TEXT UNIQUE NOT NULL,
                    checksum TEXT,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'indexed'
                )
            """)
            
            # 3. Chunks: The actual atomic units of knowledge
            # Note: 'embedding' is stored as a BLOB (bytes) for raw vector data
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS chunks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_id INTEGER,
                    chunk_index INTEGER,
                    content TEXT,
                    embedding BLOB, 
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)
            
            # 4. Graph Nodes: For the GraphView visualization
            # Distinguishes between 'file' nodes and 'concept' nodes
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_nodes (
                    id TEXT PRIMARY KEY,
                    type TEXT,  -- 'file' or 'concept'
                    label TEXT,
                    data_json TEXT -- Flexible JSON for positions/colors
                )
            """)
            
            # 5. Graph Edges: The connections
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_edges (
                    source TEXT,
                    target TEXT,
                    weight REAL DEFAULT 1.0,
                    FOREIGN KEY(source) REFERENCES graph_nodes(id),
                    FOREIGN KEY(target) REFERENCES graph_nodes(id)
                )
            """)
            
            # Timestamp creation
            cursor.execute("INSERT INTO config (key, value) VALUES (?, ?)", 
                           ("created_at", str(time.time())))
            
            conn.commit()
            conn.close()
            return {"status": "success", "path": str(db_path), "name": safe_name}
            
        except Exception as e:
            # Cleanup on failure
            if db_path.exists():
                os.remove(db_path)
            raise e

    def delete_kb(self, name: str) -> bool:
        """
        Physically removes the database file.
        """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            os.remove(db_path)
            return True
        return False

    def duplicate_kb(self, source_name: str) -> Dict[str, str]:
        """
        Creates a copy of an existing KB.
        """
        safe_source = self._sanitize_name(source_name)
        source_path = self.storage_dir / safe_source
        
        if not source_path.exists():
            raise FileNotFoundError(f"Source KB '{safe_source}' not found.")
            
        # Generate new name
        base = safe_source.replace('.db', '')
        new_name = f"{base}_copy.db"
        dest_path = self.storage_dir / new_name
        
        # Handle collision if copy already exists
        counter = 1
        while dest_path.exists():
            new_name = f"{base}_copy_{counter}.db"
            dest_path = self.storage_dir / new_name
            counter += 1
            
        shutil.copy2(source_path, dest_path)
        return {"status": "success", "name": new_name}

    def _sanitize_name(self, name: str) -> str:
        """Ensures the filename ends in .db and has no illegal chars."""
        clean = "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).strip()
        clean = clean.replace(' ', '_')
        if not clean.endswith('.db'):
            clean += '.db'
        return clean

# --- Independent Test Block ---
if __name__ == "__main__":
    print("Initializing Librarian Service...")
    lib = LibrarianMS("./test_brains")
    
    # 1. Create
    print("Creating 'Project_Alpha'...")
    try:
        lib.create_kb("Project Alpha")
    except FileExistsError:
        print("Project Alpha already exists.")
        
    # 2. List
    kbs = lib.list_kbs()
    print(f"Available Brains: {kbs}")
    
    # 3. Duplicate
    if "Project_Alpha.db" in kbs:
        print("Duplicating Alpha...")
        lib.duplicate_kb("Project_Alpha.db")
        
    # 4. Final List
    print(f"Final Brains: {lib.list_kbs()}")
--------------------------------------------------------------------------------

-------------------- FILE: _MonacoHostMS\editor.html -----------------------------------
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Monaco Host</title>
    <style>
        html, body { margin: 0; padding: 0; width: 100%; height: 100%; overflow: hidden; background-color: #1e1e1e; font-family: sans-serif; }
        #container { display: flex; flex-direction: column; height: 100%; }
        #tabs { background: #252526; display: flex; overflow-x: auto; height: 35px; }
        .tab { 
            padding: 8px 15px; color: #969696; background: #2d2d2d; cursor: pointer; border-right: 1px solid #1e1e1e; font-size: 13px;
            display: flex; align-items: center;
        }
        .tab.active { background: #1e1e1e; color: #fff; }
        .tab:hover { background: #2d2d2d; color: #fff; }
        #editor { flex-grow: 1; }
    </style>
</head>
<body>
    <div id="container">
        <div id="tabs"></div>
        <div id="editor"></div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs/loader.js"></script>
    <script>
        require.config({ paths: { 'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs' }});

        let editor;
        let models = {}; // filepath -> model
        let currentPath = null;

        require(['vs/editor/editor.main'], function() {
            editor = monaco.editor.create(document.getElementById('editor'), {
                value: "# Ready.\n",
                language: 'python',
                theme: 'vs-dark',
                automaticLayout: true
            });

            // Signal Python that we are ready
            if (window.pywebview) window.pywebview.api.signal_editor_ready();

            // Keybindings
            editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.KeyS, function() {
                if (currentPath) {
                    const content = editor.getValue();
                    window.pywebview.api.save_file(currentPath, content);
                }
            });
        });

        // --- API exposed to Python ---
        window.pywebview = window.pywebview || {};
        window.pywebview.api = window.pywebview.api || {};

        window.pywebview.api.open_in_tab = function(filepath, content) {
            // Determine language
            let lang = 'plaintext';
            if (filepath.endsWith('.py')) lang = 'python';
            if (filepath.endsWith('.js')) lang = 'javascript';
            if (filepath.endsWith('.html')) lang = 'html';
            if (filepath.endsWith('.json')) lang = 'json';

            // Create or switch model
            if (!models[filepath]) {
                const uri = monaco.Uri.file(filepath);
                models[filepath] = monaco.editor.createModel(content, lang, uri);
                addTab(filepath);
            }
            
            switchTo(filepath);
        };

        window.pywebview.api.reveal_range = function(filepath, startLine, endLine) {
            if (filepath !== currentPath) switchTo(filepath);
            editor.revealLineInCenter(startLine);
            editor.setSelection({
                startLineNumber: startLine,
                startColumn: 1,
                endLineNumber: endLine,
                endColumn: 1000
            });
        };

        // --- Internal Helpers ---
        function addTab(filepath) {
            const tabs = document.getElementById('tabs');
            const tab = document.createElement('div');
            tab.className = 'tab';
            tab.innerText = filepath;
            tab.onclick = () => switchTo(filepath);
            tab.dataset.path = filepath;
            tabs.appendChild(tab);
        }

        function switchTo(filepath) {
            if (!models[filepath]) return;
            editor.setModel(models[filepath]);
            currentPath = filepath;

            // UI update
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            const activeTab = document.querySelector(`.tab[data-path="${filepath}"]`);
            if (activeTab) activeTab.classList.add('active');
        }
    </script>
</body>
</html>
--------------------------------------------------------------------------------

-------------------- FILE: _MonacoHostMS\monaco_host.py --------------------------------
import webview
import threading
import json
import os
from pathlib import Path
from typing import Optional, Callable

class MonacoBridge:
    """
    The Bridge: Handles bidirectional communication between Python and the Monaco Editor.
    """
    def __init__(self):
        self._window = None
        self._ready_event = threading.Event()
        self.on_save_callback: Optional[Callable[[str, str], None]] = None

    def set_window(self, window):
        self._window = window

    # --- JS -> Python (Called from Editor) ---
    
    def signal_editor_ready(self):
        """Called by JS when Monaco is fully loaded."""
        self._ready_event.set()
        print("Monaco Editor is ready.")

    def save_file(self, filepath: str, content: str):
        """Called by JS when Ctrl+S is pressed."""
        if self.on_save_callback:
            self.on_save_callback(filepath, content)
        else:
            print(f"Saved {filepath} (No callback registered)")

    def log(self, message: str):
        """Called by JS to print to Python console."""
        print(f"[Monaco JS]: {message}")

    # --- Python -> JS (Called from App) ---

    def open_file(self, filepath: str, content: str):
        """Opens a file in a new tab in the editor."""
        self._ready_event.wait(timeout=5)
        if not self._window: return
        
        safe_path = filepath.replace('\\', '\\\\').replace("'", "\\'")
        safe_content = json.dumps(content)
        
        js = f"window.pywebview.api.open_in_tab('{safe_path}', {safe_content})"
        self._window.evaluate_js(js)

    def highlight_range(self, filepath: str, start_line: int, end_line: int):
        """Scrolls to and highlights a specific line range."""
        self._ready_event.wait(timeout=5)
        if not self._window: return
        
        safe_path = filepath.replace('\\', '\\\\')
        js = f"window.pywebview.api.reveal_range('{safe_path}', {start_line}, {end_line})"
        self._window.evaluate_js(js)

class MonacoHostMS:
    """
    The Host: Manages the PyWebView window lifecycle.
    """
    def __init__(self, html_path: str = "editor.html"):
        self.api = MonacoBridge()
        self.html_path = Path(html_path).resolve()
        self.window = None

    def launch(self, title="Monaco Editor", width=800, height=600, func=None):
        """
        Starts the editor window.
        :param func: Optional function to run in a background thread after launch.
        """
        self.window = webview.create_window(
            title, 
            str(self.html_path), 
            js_api=self.api,
            width=width, 
            height=height
        )
        self.api.set_window(self.window)
        
        if func:
            webview.start(func, debug=True)
        else:
            webview.start(debug=True)

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Setup
    host = MonacoHostMS()
    
    # 2. Define a background task to simulate "Opening a file" after 2 seconds
    def background_actions():
        import time
        print("Waiting for editor...")
        host.api._ready_event.wait()
        
        time.sleep(1)
        print("Opening test file...")
        
        dummy_code = """def hello_world():
    print("Hello from Python!")
    return True
"""
        host.api.open_file("test_script.py", dummy_code)
        
        # Define what happens on save
        host.api.on_save_callback = lambda path, content: print(f"SAVED TO DISK: {path}\nContent Size: {len(content)}")

    # 3. Launch
    print("Launching Editor...")
    host.launch(func=background_actions)
--------------------------------------------------------------------------------

-------------------- FILE: _NetworkLayoutMS\network_layout.py --------------------------
import networkx as nx
import logging
from typing import List, Dict, Any, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("NetLayout")
# ==============================================================================

class NetworkLayoutMS:
    """
    The Topologist: Calculates visual coordinates for graph nodes using
    server-side algorithms (NetworkX). 
    Useful for generating static map snapshots or pre-calculating positions 
    to offload client-side rendering.
    """
    def __init__(self):
        pass

    def calculate_layout(self, nodes: List[str], edges: List[Tuple[str, str]], 
                         algorithm: str = "spring", **kwargs) -> Dict[str, Tuple[float, float]]:
        """
        Computes (x, y) coordinates for the given graph.
        
        :param nodes: List of node IDs.
        :param edges: List of (source, target) tuples.
        :param algorithm: 'spring' (Force-directed) or 'circular'.
        :return: Dictionary {node_id: (x, y)}
        """
        G = nx.DiGraph()
        G.add_nodes_from(nodes)
        G.add_edges_from(edges)
        
        log.info(f"Computing layout for {len(nodes)} nodes, {len(edges)} edges...")
        
        try:
            if algorithm == "circular":
                pos = nx.circular_layout(G)
            else:
                # Spring layout (Fruchterman-Reingold) is standard for knowledge graphs
                k_val = kwargs.get('k', 0.15) # Optimal distance between nodes
                iter_val = kwargs.get('iterations', 50)
                pos = nx.spring_layout(G, k=k_val, iterations=iter_val, seed=42)
                
            # Convert numpy arrays to simple lists/tuples for JSON serialization
            return {n: (float(p[0]), float(p[1])) for n, p in pos.items()}
            
        except Exception as e:
            log.error(f"Layout calculation failed: {e}")
            return {}

# --- Independent Test Block ---
if __name__ == "__main__":
    layout = NetworkLayoutMS()
    
    # 1. Define a simple graph
    test_nodes = ["Main", "Utils", "Config", "DB", "Auth"]
    test_edges = [
        ("Main", "Utils"),
        ("Main", "Config"),
        ("Main", "DB"),
        ("Main", "Auth"),
        ("DB", "Config"),
        ("Auth", "DB")
    ]
    
    # 2. Compute Layout
    positions = layout.calculate_layout(test_nodes, test_edges, k=0.5)
    
    print("--- Calculated Positions ---")
    for node, (x, y) in positions.items():
        print(f"{node:<10}: ({x: .4f}, {y: .4f})")
--------------------------------------------------------------------------------

-------------------- FILE: _NetworkLayoutMS\requirements.txt ---------------------------
pip install networkx
--------------------------------------------------------------------------------

-------------------- FILE: _PromptOptimizerMS\prompt_optimizer.py ----------------------
import json
import logging
from typing import List, Dict, Any, Callable, Optional

# ==============================================================================
# CONFIGURATION: META-PROMPTS
# ==============================================================================
# The system prompt used to turn the LLM into a Prompt Engineer
REFINE_SYSTEM_PROMPT = (
    "You are a world-class prompt engineer. "
    "Given an original prompt and specific feedback, "
    "provide an improved, refined version of the prompt that incorporates the feedback. "
    "Return ONLY the refined prompt text, no preamble."
)

# The system prompt used to generate A/B test variations
VARIATION_SYSTEM_PROMPT = (
    "You are a creative AI assistant. "
    "Generate {num} innovative and diverse variations of the following prompt. "
    "Return the result as a valid JSON array of strings. "
    "Example: [\"variation 1\", \"variation 2\"]"
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptOpt")
# ==============================================================================

class PromptOptimizerMS:
    """
    The Tuner: Uses an LLM to refine prompts or generate variations.
    """
    def __init__(self, inference_func: Callable[[str], str]):
        """
        :param inference_func: A function that takes a string (prompt) and returns a string (completion).
                               Compatible with your ModelController.infer or OpenAI calls.
        """
        self.infer = inference_func

    def refine_prompt(self, draft_prompt: str, feedback: str) -> str:
        """
        Rewrites a prompt based on feedback.
        """
        full_prompt = (
            f"{REFINE_SYSTEM_PROMPT}\n\n"
            f"[Original Prompt]:\n{draft_prompt}\n\n"
            f"[Feedback]:\n{feedback}\n\n"
            f"[Refined Prompt]:"
        )
        
        log.info("Refining prompt...")
        try:
            result = self.infer(full_prompt)
            return result.strip()
        except Exception as e:
            log.error(f"Refinement failed: {e}")
            return draft_prompt # Fallback to original

    def generate_variations(self, draft_prompt: str, num_variations: int = 3, context_data: Optional[Dict] = None) -> List[str]:
        """
        Generates multiple versions of a prompt for testing.
        """
        meta_prompt = VARIATION_SYSTEM_PROMPT.format(num=num_variations)
        
        prompt_content = draft_prompt
        if context_data:
            prompt_content += f"\n\n--- Context ---\n{json.dumps(context_data, indent=2)}"

        full_prompt = (
            f"{meta_prompt}\n\n"
            f"[Original Prompt]:\n{prompt_content}\n\n"
            f"[JSON Array of Variations]:"
        )

        log.info(f"Generating {num_variations} variations...")
        try:
            # We explicitly ask for JSON, but LLMs are chatty, so we might need cleaning logic here
            raw_response = self.infer(full_prompt)
            
            # Simple cleanup to find the JSON array if the LLM added text around it
            start = raw_response.find('[')
            end = raw_response.rfind(']') + 1
            if start == -1 or end == 0:
                raise ValueError("No JSON array found in response")
                
            clean_json = raw_response[start:end]
            variations = json.loads(clean_json)
            
            if isinstance(variations, list):
                return [str(v) for v in variations]
            return []
            
        except Exception as e:
            log.error(f"Variation generation failed: {e}")
            return []

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Mock Inference Engine (Simulating an LLM)
    def mock_llm(prompt: str) -> str:
        if "[Refined Prompt]" in prompt:
            return "You are a helpful assistant who speaks like a pirate. How may I help ye?"
        if "[JSON Array]" in prompt:
            return '["Variation A: Pirate Mode", "Variation B: Formal Mode", "Variation C: Concise Mode"]'
        return "Error"

    optimizer = PromptOptimizerMS(inference_func=mock_llm)

    # 2. Test Refine
    print("--- Test: Refine ---")
    draft = "Help me."
    feedback = "Make it sound like a pirate."
    refined = optimizer.refine_prompt(draft, feedback)
    print(f"Original: {draft}")
    print(f"Refined:  {refined}")

    # 3. Test Variations
    print("\n--- Test: Variations ---")
    vars = optimizer.generate_variations(draft, num_variations=3)
    for i, v in enumerate(vars):
        print(f" {i+1}. {v}")
--------------------------------------------------------------------------------

-------------------- FILE: _PromptVaultMS\prompt_vault.py ------------------------------
import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, Callable
from pydantic import BaseModel, Field, ValidationError
from jinja2 import Environment, BaseLoader

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("prompt_vault.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptVault")
# ==============================================================================

# --- Data Models ---

class PromptVersion(BaseModel):
    """A specific historical version of a prompt."""
    version_num: int
    content: str
    author: str
    timestamp: datetime.datetime
    embedding: Optional[List[float]] = None

class PromptTemplate(BaseModel):
    """The master record for a prompt."""
    id: str
    slug: str
    title: str
    description: Optional[str] = ""
    tags: List[str] = []
    latest_version_num: int
    versions: List[PromptVersion] = []
    
    @property
    def latest(self) -> PromptVersion:
        """Helper to get the most recent content."""
        if not self.versions:
            raise ValueError("No versions found.")
        # versions are stored sorted by DB insertion usually, but let's be safe
        return sorted(self.versions, key=lambda v: v.version_num)[-1]

# --- Database Management ---

class PromptVaultMS:
    """
    The Vault: A persistent SQLite store for managing, versioning, 
    and rendering AI prompts.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()
        self.jinja_env = Environment(loader=BaseLoader())

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        """Bootstraps the schema."""
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS templates (
                    id TEXT PRIMARY KEY,
                    slug TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    tags_json TEXT,
                    latest_version INTEGER DEFAULT 1,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS versions (
                    id TEXT PRIMARY KEY,
                    template_id TEXT,
                    version_num INTEGER,
                    content TEXT,
                    author TEXT,
                    timestamp TIMESTAMP,
                    embedding_json TEXT,
                    FOREIGN KEY(template_id) REFERENCES templates(id)
                )
            """)

    # --- CRUD Operations ---

    def create_template(self, slug: str, title: str, content: str, author: str = "system", tags: List[str] = None) -> PromptTemplate:
        """Creates a new prompt template with an initial version 1."""
        tags = tags or []
        now = datetime.datetime.utcnow()
        t_id = str(uuid.uuid4())
        v_id = str(uuid.uuid4())

        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO templates (id, slug, title, description, tags_json, latest_version, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                    (t_id, slug, title, "", json.dumps(tags), 1, now, now)
                )
                conn.execute(
                    "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                    (v_id, t_id, 1, content, author, now)
                )
            log.info(f"Created template: {slug}")
            return self.get_template(slug)
        except sqlite3.IntegrityError:
            raise ValueError(f"Template '{slug}' already exists.")

    def add_version(self, slug: str, content: str, author: str = "user") -> PromptTemplate:
        """Adds a new version to an existing template."""
        current = self.get_template(slug)
        if not current:
            raise ValueError(f"Template '{slug}' not found.")

        new_ver = current.latest_version_num + 1
        now = datetime.datetime.utcnow()
        v_id = str(uuid.uuid4())

        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                (v_id, current.id, new_ver, content, author, now)
            )
            conn.execute(
                "UPDATE templates SET latest_version = ?, updated_at = ? WHERE id = ?",
                (new_ver, now, current.id)
            )
        log.info(f"Updated {slug} to v{new_ver}")
        return self.get_template(slug)

    def get_template(self, slug: str) -> Optional[PromptTemplate]:
        """Retrieves a full template with all history."""
        with self._get_conn() as conn:
            # 1. Fetch Template
            row = conn.execute("SELECT * FROM templates WHERE slug = ?", (slug,)).fetchone()
            if not row: return None

            # 2. Fetch Versions
            v_rows = conn.execute("SELECT * FROM versions WHERE template_id = ? ORDER BY version_num ASC", (row['id'],)).fetchall()
            
            versions = []
            for v in v_rows:
                versions.append(PromptVersion(
                    version_num=v['version_num'],
                    content=v['content'],
                    author=v['author'],
                    timestamp=v['timestamp']
                    # embedding logic skipped for brevity
                ))

            return PromptTemplate(
                id=row['id'],
                slug=row['slug'],
                title=row['title'],
                description=row['description'],
                tags=json.loads(row['tags_json']),
                latest_version_num=row['latest_version'],
                versions=versions
            )

    def render(self, slug: str, context: Dict[str, Any] = None) -> str:
        """Fetches the latest version and renders it with Jinja2."""
        template = self.get_template(slug)
        if not template:
            raise ValueError(f"Template '{slug}' not found.")
        
        raw_text = template.latest.content
        jinja_template = self.jinja_env.from_string(raw_text)
        return jinja_template.render(**(context or {}))

    def list_slugs(self) -> List[str]:
        with self._get_conn() as conn:
            rows = conn.execute("SELECT slug FROM templates").fetchall()
            return [r[0] for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # 1. Setup
    if DB_PATH.exists(): os.remove(DB_PATH)
    vault = PromptVaultMS()
    
    # 2. Create
    print("--- Creating Prompt ---")
    vault.create_template(
        slug="greet_user",
        title="Greeting Protocol",
        content="Hello {{ name }}, welcome to the {{ system_name }}!",
        tags=["ui", "onboarding"]
    )
    
    # 3. Versioning
    print("--- Updating Prompt ---")
    vault.add_version("greet_user", "Greetings, {{ name }}. System {{ system_name }} is online.")
    
    # 4. Retrieval & Rendering
    print("--- Rendering ---")
    final_text = vault.render("greet_user", {"name": "Alice", "system_name": "Nexus"})
    print(f"Rendered Output: {final_text}")
    
    # 5. Inspection
    tpl = vault.get_template("greet_user")
    print(f"Current Version: v{tpl.latest_version_num}")
    print(f"History: {[v.content for v in tpl.versions]}")
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)
--------------------------------------------------------------------------------

-------------------- FILE: _RegexWeaverMS\regex_weaver.py ------------------------------
import re
import logging
from typing import List, Set

# ==============================================================================
# CONFIGURATION: PATTERNS
# ==============================================================================
# Python: "import x", "from x import y"
PY_IMPORT = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')

# JS/TS: "import ... from 'x'", "require('x')"
JS_IMPORT = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RegexWeaver")
# ==============================================================================

class RegexWeaverMS:
    """
    The Weaver: A fault-tolerant dependency extractor.
    Uses Regex to find imports, making it faster and more permissive
    than AST parsers (works on broken code).
    """
    def __init__(self):
        pass

    def extract_dependencies(self, content: str, language: str) -> List[str]:
        """
        Scans code content for import statements.
        :param language: 'python' or 'javascript' (includes ts/jsx).
        """
        dependencies: Set[str] = set()
        lines = content.splitlines()
        
        pattern = PY_IMPORT if language == 'python' else JS_IMPORT
        
        for line in lines:
            # Skip comments roughly
            if line.strip().startswith(('#', '//')):
                continue
                
            if language == 'python':
                match = pattern.match(line)
            else:
                match = pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                # Clean up: "backend.database" -> "database"
                # We usually want the leaf name for simple linking
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                dependencies.add(clean_dep)
                
        return sorted(list(dependencies))

# --- Independent Test Block ---
if __name__ == "__main__":
    weaver = RegexWeaverMS()
    
    # 1. Python Test
    py_code = """
    import os
    from backend.utils import helper
    # from commented.out import ignore_me
    import pandas as pd
    """
    print(f"Python Deps: {weaver.extract_dependencies(py_code, 'python')}")
    
    # 2. JS Test
    js_code = """
    import React from 'react';
    const utils = require('./lib/utils');
    // import hidden from 'hidden';
    """
    print(f"JS Deps:     {weaver.extract_dependencies(js_code, 'javascript')}")
--------------------------------------------------------------------------------

-------------------- FILE: _SandboxManagerMS\sandbox_manager.py ------------------------
import shutil
import hashlib
import os
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Default folders to ignore when syncing or diffing
DEFAULT_EXCLUDES = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "_sandbox", "_project_library"
}
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("SandboxMgr")
# ==============================================================================

class SandboxManagerMS:
    """
    The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.
    Allows for safe experimentation, diffing, and atomic promotion of changes.
    """
    def __init__(self, live_path: str, sandbox_path: str):
        self.live_root = Path(live_path).resolve()
        self.sandbox_root = Path(sandbox_path).resolve()

    def init_sandbox(self, force: bool = False):
        """
        Creates or resets the sandbox by mirroring the live project.
        """
        if self.sandbox_root.exists():
            if not force:
                raise FileExistsError(f"Sandbox already exists at {self.sandbox_root}")
            log.info("Wiping existing sandbox...")
            shutil.rmtree(self.sandbox_root)
        
        log.info(f"Cloning {self.live_root} -> {self.sandbox_root}...")
        self._mirror_tree(self.live_root, self.sandbox_root)
        log.info("Sandbox initialized.")

    def reset_sandbox(self):
        """
        Discards all sandbox changes and re-syncs from live.
        """
        self.init_sandbox(force=True)

    def get_diff(self) -> Dict[str, List[str]]:
        """
        Compares Sandbox vs Live. Returns added, modified, and deleted files.
        """
        sandbox_files = self._scan_files(self.sandbox_root)
        live_files = self._scan_files(self.live_root)
        
        sandbox_paths = set(sandbox_files.keys())
        live_paths = set(live_files.keys())

        # 1. Added: In sandbox but not in live
        added = sorted(list(sandbox_paths - live_paths))
        
        # 2. Deleted: In live but not in sandbox
        deleted = sorted(list(live_paths - sandbox_paths))
        
        # 3. Modified: In both, but hashes differ
        common = sandbox_paths.intersection(live_paths)
        modified = []
        for rel_path in common:
            if sandbox_files[rel_path] != live_files[rel_path]:
                modified.append(rel_path)
        modified.sort()

        return {
            "added": added,
            "modified": modified,
            "deleted": deleted
        }

    def promote_changes(self) -> Tuple[int, int, int]:
        """
        Applies changes from Sandbox to Live.
        Returns (added_count, modified_count, deleted_count).
        """
        diff = self.get_diff()
        
        # 1. Additions & Modifications (Copy file -> file)
        for rel_path in diff['added'] + diff['modified']:
            src = self.sandbox_root / rel_path
            dst = self.live_root / rel_path
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)
            
        # 2. Deletions (Remove file)
        for rel_path in diff['deleted']:
            target = self.live_root / rel_path
            if target.exists():
                os.remove(target)
                
        log.info(f"Promoted: {len(diff['added'])} added, {len(diff['modified'])} modified, {len(diff['deleted'])} deleted.")
        return len(diff['added']), len(diff['modified']), len(diff['deleted'])

    # --- Internal Helpers ---

    def _mirror_tree(self, src_root: Path, dst_root: Path):
        """Recursive copy that respects the exclusion list."""
        if not dst_root.exists():
            dst_root.mkdir(parents=True, exist_ok=True)

        for item in src_root.iterdir():
            if item.name in DEFAULT_EXCLUDES:
                continue
                
            dst_path = dst_root / item.name
            
            if item.is_dir():
                self._mirror_tree(item, dst_path)
            else:
                shutil.copy2(item, dst_path)

    def _scan_files(self, root: Path) -> Dict[str, str]:
        """
        Scans directory and returns {relative_path: sha256_hash}.
        """
        file_map = {}
        if not root.exists():
            return {}
            
        for path in root.rglob("*"):
            if path.is_file() and not self._is_excluded(path, root):
                rel = str(path.relative_to(root)).replace("\\", "/")
                file_map[rel] = self._get_hash(path)
        return file_map

    def _is_excluded(self, path: Path, root: Path) -> bool:
        """Checks if any part of the path is in the exclusion list."""
        try:
            rel_parts = path.relative_to(root).parts
            return any(p in DEFAULT_EXCLUDES for p in rel_parts)
        except ValueError:
            return False

    def _get_hash(self, path: Path) -> str:
        """Fast SHA-256 for file content."""
        try:
            # Skip binary files if needed, or hash them too (hashing is safe)
            return hashlib.sha256(path.read_bytes()).hexdigest()
        except Exception:
            return "read_error"

# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup test environment
    base = Path("test_env")
    live = base / "live_project"
    box = base / "sandbox"
    
    if base.exists(): shutil.rmtree(base)
    live.mkdir(parents=True)
    
    # 1. Create Mock Live Project
    (live / "main.py").write_text("print('v1')")
    (live / "utils.py").write_text("def help(): pass")
    (live / "node_modules").mkdir() # Should be ignored
    (live / "node_modules" / "junk.js").write_text("junk")
    
    print("--- Initializing Sandbox ---")
    mgr = SandboxManagerMS(str(live), str(box))
    mgr.init_sandbox()
    
    # 2. Make Changes in Sandbox
    print("\n--- Modifying Sandbox ---")
    (box / "main.py").write_text("print('v2')") # Modify
    (box / "new_feature.py").write_text("print('new')") # Add
    os.remove(box / "utils.py") # Delete
    
    # 3. Check Diff
    diff = mgr.get_diff()
    print(f"Diff Analysis:\n Added: {diff['added']}\n Modified: {diff['modified']}\n Deleted: {diff['deleted']}")
    
    # 4. Promote
    print("\n--- Promoting Changes ---")
    mgr.promote_changes()
    
    # Verify Live
    print(f"Live 'main.py' content: {(live / 'main.py').read_text()}")
    print(f"Live 'utils.py' exists? {(live / 'utils.py').exists()}")
    
    # Cleanup
    if base.exists(): shutil.rmtree(base)
--------------------------------------------------------------------------------

-------------------- FILE: _ScannerMS\scanner.py ---------------------------------------
import os
import time
from typing import Dict, List, Any, Optional

class ScannerMS:
    """
    The Scanner: Walks the file system, filters junk, and detects binary files.
    Generates the tree structure used by the UI.
    """
    
    def __init__(self):
        # Folders to completely ignore (Standard developer noise)
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage'
        }
        
        # Extensions that are explicitly binary/junk
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', '.svg', 
            '.zip', '.tar', '.gz', '.pdf', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }

    def is_binary(self, file_path: str) -> bool:
        """
        Determines if a file is binary using two heuristics:
        1. Extension check (Fast)
        2. Content check for null bytes (Accurate)
        """
        # 1. Fast Fail on Extension
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS:
            return True
            
        # 2. Content Inspection (Read first 1KB)
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                # Text files shouldn't contain null bytes
                if b'\x00' in chunk:
                    return True
        except (IOError, OSError):
            # If we can't read it, treat as binary/unsafe
            return True
            
        return False

    def scan_directory(self, root_path: str) -> Optional[Dict[str, Any]]:
        """
        Recursively scans a directory and returns a JSON-compatible tree.
        Returns None if path is invalid.
        """
        target = os.path.abspath(root_path)
        
        if not os.path.exists(target):
            return None
            
        if not os.path.isdir(target):
            # Handle single file case
            return self._create_node(target, is_dir=False)

        return self._scan_recursive(target)

    def _scan_recursive(self, current_path: str) -> Dict[str, Any]:
        """
        Internal recursive worker.
        """
        node = self._create_node(current_path, is_dir=True)
        node['children'] = []
        
        try:
            # os.scandir is faster than os.listdir as it returns file attributes
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                
                for entry in entries:
                    # Skip ignored directories
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS:
                        continue
                        
                    # Skip hidden files (dotfiles)
                    if entry.name.startswith('.'):
                        continue

                    if entry.is_dir():
                        child_node = self._scan_recursive(entry.path)
                        if child_node: # Only add if valid
                            node['children'].append(child_node)
                    else:
                        child_node = self._create_node(entry.path, is_dir=False)
                        node['children'].append(child_node)
                        
        except PermissionError:
            node['error'] = "Access Denied"
            
        return node

    def _create_node(self, path: str, is_dir: bool) -> Dict[str, Any]:
        """
        Standardizes the node structure for the UI.
        """
        name = os.path.basename(path)
        node = {
            'text': name,
            'path': path,
            'type': 'folder' if is_dir else 'file',
            'checked': False, # UI State
        }
        
        if not is_dir:
            if self.is_binary(path):
                node['type'] = 'binary'
                
        return node

    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        """
        Helper to extract all valid file paths from a tree node 
        (e.g., when the user clicks 'Start Ingest').
        """
        files = []
        if tree_node['type'] == 'file':
            files.append(tree_node['path'])
        elif tree_node['type'] == 'folder' and 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files

# --- Independent Test Block ---
if __name__ == "__main__":
    scanner = ScannerMS()
    
    # Scan the current directory
    cwd = os.getcwd()
    print(f"Scanning: {cwd} ...")
    
    start_time = time.time()
    tree = scanner.scan_directory(cwd)
    duration = time.time() - start_time
    
    if tree:
        file_count = len(scanner.flatten_tree(tree))
        print(f"Scan complete in {duration:.4f}s")
        print(f"Found {file_count} files.")
        
        # Print top level children to verify
        print("Top Level Structure:")
        for child in tree.get('children', [])[:5]:
            print(f" - [{child['type'].upper()}] {child['text']}")
    else:
        print("Scan failed or path invalid.")
--------------------------------------------------------------------------------

-------------------- FILE: _SearchEngineMS\search_engine.py ----------------------------
import sqlite3
import json
import struct
import requests
import os
from typing import List, Dict, Any, Optional

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

class SearchEngineMS:
    """
    The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
    
    Architecture:
    1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.
    2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.
    3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).
    """

    def __init__(self, model_name: str = "phi3:mini-128k"):
        self.model = model_name

    def search(self, db_path: str, query: str, limit: int = 10) -> List[Dict]:
        """
        Main entry point. Returns a list of results sorted by relevance.
        """
        if not os.path.exists(db_path):
            return []

        conn = sqlite3.connect(db_path)
        # Enable sqlite-vec extension if needed, though standard connect might miss it 
        # depending on system install. For now, we assume the DB is pre-populated 
        # and standard SQL queries work if the extension is loaded globally or unnecessary 
        # for simple selects (standard SQLite can read vec0 tables usually, just not query them efficiently without ext).
        # Note: If sqlite-vec is not loaded, the vec0 MATCH queries below will fail.
        # We try to load it here just in case.
        conn.enable_load_extension(True)
        try:
            import sqlite_vec
            sqlite_vec.load(conn)
        except:
            print("Warning: sqlite_vec not loaded in Search Engine. Vector search may fail.")

        cursor = conn.cursor()

        # 1. Vectorize the User Query
        query_vec = self._get_query_embedding(query)
        if not query_vec:
            # Fallback to keyword only if embedding fails
            return self._keyword_search_only(cursor, query, limit)

        # Pack vector for sqlite-vec (Float32 Little Endian)
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)

        # 2. HYBRID QUERY (The "Magic" SQL)
        # We use CTEs to get top 50 from Vector and top 50 from Keyword, then merge.
        sql = """
        WITH 
        vec_matches AS (
            SELECT rowid, distance,
            row_number() OVER (ORDER BY distance) as rank
            FROM knowledge_vectors
            WHERE embedding MATCH ? 
            AND k = 50
        ),
        fts_matches AS (
            SELECT rowid, rank as fts_score,
            row_number() OVER (ORDER BY rank) as rank
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT 50
        )
        SELECT 
            kc.file_path,
            kc.content,
            (
                -- RRF Formula: 1 / (k + rank)
                COALESCE(1.0 / (60 + v.rank), 0.0) +
                COALESCE(1.0 / (60 + f.rank), 0.0)
            ) as rrf_score
        FROM knowledge_chunks kc
        LEFT JOIN vec_matches v ON kc.id = v.rowid
        LEFT JOIN fts_matches f ON kc.id = f.rowid
        WHERE v.rowid IS NOT NULL OR f.rowid IS NOT NULL
        ORDER BY rrf_score DESC
        LIMIT ?;
        """

        try:
            # Escape quotes for FTS
            fts_query = f'"{query}"' 
            rows = cursor.execute(sql, (vec_bytes, fts_query, limit)).fetchall()
        except sqlite3.OperationalError as e:
            print(f"Search Error (likely missing sqlite-vec): {e}")
            return []

        results = []
        for r in rows:
            path, content, score = r
            snippet = self._extract_snippet(content, query)
            results.append({
                "path": path,
                "score": round(score, 4),
                "snippet": snippet,
                "full_content": content # Keeping this for "Reconstruct" later
            })

        conn.close()
        return results

    def _keyword_search_only(self, cursor, query: str, limit: int) -> List[Dict]:
        """Fallback if embeddings are offline."""
        sql = """
            SELECT file_path, content
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        rows = cursor.execute(sql, (f'"{query}"', limit)).fetchall()
        return [{
            "path": r[0], 
            "score": 0.0, 
            "snippet": self._extract_snippet(r[1], query),
            "full_content": r[1]
        } for r in rows]

    def _get_query_embedding(self, text: str) -> Optional[List[float]]:
        """Call Ollama to get the vector for the search query."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.model, "prompt": text},
                timeout=5
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
        return None

    def _extract_snippet(self, content: str, query: str) -> str:
        """Finds the best window of text around the keyword."""
        lower_content = content.lower()
        lower_query = query.lower().split()[0] # Take first word for simple centering
        
        idx = lower_content.find(lower_query)
        if idx == -1:
            return content[:200].replace('\n', ' ') + "..."
            
        start = max(0, idx - 60)
        end = min(len(content), idx + 140)
        snippet = content[start:end].replace('\n', ' ')
        return f"...{snippet}..."

# --- Independent Test Block ---
if __name__ == "__main__":
    # Note: Requires a real DB path to work
    print("Initializing Search Engine...")
    engine = SearchEngineMS()
    # Test would go here
--------------------------------------------------------------------------------

-------------------- FILE: _ServiceRegistryMS\service_registry.py ----------------------
import ast
import json
import os
import uuid
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OUTPUT_FILE = "registry.json"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("ServiceRegistry")
# ==============================================================================

class ServiceRegistryMS:
    """
    The Tokenizer: Scans a library of Python microservices and generates
    standardized JSON 'Service Tokens' describing their capabilities.
    """
    def __init__(self, root_path: str = "."):
        self.root = Path(root_path).resolve()
        self.registry = []

    def scan(self, save_to: str = OUTPUT_FILE) -> List[Dict]:
        """
        Scans the root directory for microservices and saves the registry.
        """
        log.info(f"Scanning for microservices in: {self.root}")
        
        # 1. Walk directories looking for likely microservices
        # Convention: Folders starting with '_' and ending with 'MS'
        for item in self.root.iterdir():
            if item.is_dir() and item.name.startswith("_") and item.name.endswith("MS"):
                self._process_microservice_folder(item)
        
        # 2. Save Registry
        try:
            with open(save_to, "w", encoding="utf-8") as f:
                json.dump(self.registry, f, indent=2)
            log.info(f"âœ… Registry built. Found {len(self.registry)} services. Saved to {save_to}")
        except Exception as e:
            log.error(f"Failed to save registry: {e}")
            
        return self.registry

    def _process_microservice_folder(self, folder: Path):
        """Finds and parses the primary Python file in a microservice folder."""
        # Heuristic: Find .py files that aren't __init__ or setup
        candidates = list(folder.glob("*.py"))
        
        for file in candidates:
            if file.name.startswith("__"): continue
            
            try:
                token = self._tokenize_file(file)
                if token:
                    self.registry.append(token)
                    log.info(f"  + Tokenized: {token['name']} ({file.name})")
                    # Assume one main service class per folder for now
                    break 
            except Exception as e:
                log.warning(f"  - Failed to parse {file.name}: {e}")

    def _tokenize_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """Parses a Python file to extract class metadata."""
        with open(file_path, "r", encoding="utf-8") as f:
            source = f.read()
        
        try:
            tree = ast.parse(source)
        except SyntaxError:
            return None
        
        # 1. Find the Main Class (Ends in 'MS' or matches filename concept)
        target_class = None
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                if node.name.endswith("MS"):
                    target_class = node
                    break
        
        if not target_class:
            return None

        # 2. Extract Metadata
        class_name = target_class.name
        docstring = ast.get_docstring(target_class) or "No description provided."
        
        # Generate a deterministic ID based on the class name
        # We use UUID5 with a custom namespace to ensure the ID is always the same for the same class name
        namespace = uuid.uuid5(uuid.NAMESPACE_DNS, "microservice.library")
        token_id = f"MS_{uuid.uuid5(namespace, class_name).hex[:8].upper()}"
        
        # 3. Analyze Dependencies (Imports)
        dependencies = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names: dependencies.add(n.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module: dependencies.add(node.module.split('.')[0])
        
        # 4. Analyze Inputs/Outputs (Public Methods)
        methods = {}
        
        for node in target_class.body:
            if isinstance(node, ast.FunctionDef):
                # Skip private methods and __init__
                if node.name.startswith("_"): continue
                
                method_name = node.name
                method_doc = ast.get_docstring(node) or ""
                
                # Get Arguments
                args = []
                for arg in node.args.args:
                    if arg.arg == "self": continue
                    annotation = self._get_annotation(arg.annotation)
                    args.append(f"{arg.arg}: {annotation}")
                
                # Get Return Type
                ret_anno = self._get_annotation(node.returns)
                
                methods[method_name] = {
                    "args": args,
                    "returns": ret_anno,
                    "doc": method_doc.strip()
                }

        # 5. Construct Token
        rel_path = str(file_path.relative_to(self.root)).replace("\\", "/")
        
        return {
            "token_id": token_id,
            "name": class_name,
            "path": rel_path,
            "description": docstring.strip(),
            "methods": methods,
            "dependencies": sorted(list(dependencies))
        }

    def _get_annotation(self, node) -> str:
        """Recursively resolves AST type annotations to strings."""
        if node is None: return "Any"
        if isinstance(node, ast.Name): return node.id
        if isinstance(node, ast.Constant): return str(node.value)
        if isinstance(node, ast.Subscript):
            # Handle generics like List[str]
            val = self._get_annotation(node.value)
            slice_val = self._get_annotation(node.slice)
            return f"{val}[{slice_val}]"
        if isinstance(node, ast.Attribute):
            return f"{self._get_annotation(node.value)}.{node.attr}"
        return "Complex"

# --- Independent Test Block ---
if __name__ == "__main__":
    # Point this at the directory containing your microservices
    # For self-testing, we scan the current directory
    current_dir = "."
    print(f"--- Running Service Registry on '{current_dir}' ---")
    
    registry = ServiceRegistryMS(current_dir)
    tokens = registry.scan()
    
    if tokens:
        print(f"\n[Sample Token: {tokens[0]['name']}]")
        print(json.dumps(tokens[0], indent=2))
    else:
        print("\nNo microservices found (Did you place this script in the root of your collection?)")
--------------------------------------------------------------------------------

-------------------- FILE: _SmartChunkerMS\smart_chunker.py ----------------------------
import re
from typing import List, Optional

class SmartChunkerMS:
    """
    The Editor: A 'Recursive' text splitter. 
    It respects the natural structure of text (Paragraphs -> Sentences -> Words)
    rather than just hacking it apart by character count.
    """
    
    def __init__(self):
        # Separators in order of preference.
        # 1. Double newline (Paragraph break)
        # 2. Single newline (Line break / List item)
        # 3. Sentence endings (Period, Question, Exclamation + Space)
        # 4. Space (Word break)
        # 5. Empty string (Hard character cut)
        self.separators = ["\n\n", "\n", "(?<=[.?!])\s+", " ", ""]

    def chunk(self, text: str, max_size: int = 1000, overlap: int = 100) -> List[str]:
        """
        Recursively chunks text while trying to keep related content together.
        """
        return self._recursive_split(text, self.separators, max_size, overlap)

    def _recursive_split(self, text: str, separators: List[str], max_size: int, overlap: int) -> List[str]:
        final_chunks = []
        
        # 1. Base Case: If the text fits, return it
        if len(text) <= max_size:
            return [text]
        
        # 2. Edge Case: No more separators, forced hard split
        if not separators:
            return self._hard_split(text, max_size, overlap)

        # 3. Recursive Step: Try to split by the current separator
        current_sep = separators[0]
        next_separators = separators[1:]
        
        # Regex split to keep delimiters if possible (logic varies by regex complexity)
        # For simple string splits like \n\n, we just split.
        if len(current_sep) > 1 and "(" in current_sep: 
            # It's a regex lookbehind (sentence splitter), use re.split
            splits = re.split(current_sep, text)
        else:
            splits = text.split(current_sep)

        # Now we have a list of smaller pieces. We need to merge them back together
        # until they fill the 'max_size' bucket, then start a new bucket.
        current_doc = []
        current_length = 0
        
        for split in splits:
            if not split: continue
            
            # If a single split is STILL too big, recurse deeper on it
            if len(split) > max_size:
                # If we have stuff in the buffer, flush it first
                if current_doc:
                    final_chunks.append(current_sep.join(current_doc))
                    current_doc = []
                    current_length = 0
                
                # Recurse on the big chunk using the NEXT separator
                sub_chunks = self._recursive_split(split, next_separators, max_size, overlap)
                final_chunks.extend(sub_chunks)
                continue

            # Check if adding this split would overflow
            if current_length + len(split) + len(current_sep) > max_size:
                # Flush the current buffer
                doc_text = current_sep.join(current_doc)
                final_chunks.append(doc_text)
                
                # Start new buffer with overlap logic?
                # For simplicity in recursion, we often just start fresh or carry over 
                # a small tail if we implemented a rolling window here.
                # To keep this "Pure logic" simple, we start fresh with the current split.
                current_doc = [split]
                current_length = len(split)
            else:
                # Add to buffer
                current_doc.append(split)
                current_length += len(split) + len(current_sep)

        # Flush remaining
        if current_doc:
            final_chunks.append(current_sep.join(current_doc))

        return final_chunks

    def _hard_split(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Last resort: naive character sliding window."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
    chunker = SmartChunkerMS()
    
    # Example: A technical document with structure
    doc = """
    # Intro to AI
    Artificial Intelligence is great. It helps us code.
    
    ## How it works
    1. Ingestion: Reading data.
    2. Processing: Thinking about data.
    
    This is a very long paragraph that effectively serves as a stress test for the sentence splitter. It should hopefully not break in the middle of a thought! We want to keep sentences whole.
    """
    
    print("--- Testing Smart Chunking (Max 60 chars) ---")
    # We set max_size very small to force it to use the sentence/word splitters
    chunks = chunker.chunk(doc, max_size=60, overlap=0)
    
    for i, c in enumerate(chunks):
        print(f"[{i}] {repr(c)}")
--------------------------------------------------------------------------------

-------------------- FILE: _SysInspectorMS\sys_inspector.py ----------------------------
import platform
import subprocess
import sys
import datetime
from typing import Dict, Optional

class SysInspectorMS:
    """
    The Auditor: Gathers hardware and environment statistics.
    Supports: Windows (WMIC), Linux (lscpu/lspci), and macOS (sysctl/system_profiler).
    """

    def generate_report(self) -> str:
        """
        Runs the full audit and returns a formatted string report.
        """
        system_os = platform.system()
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = [
            f"System Audit Report",
            f"Generated: {timestamp}",
            f"OS: {system_os} {platform.release()} ({platform.machine()})",
            "-" * 40,
            ""
        ]

        # 1. Hardware Section
        report.append("--- Hardware Information ---")
        if system_os == "Windows":
            report.extend(self._audit_windows())
        elif system_os == "Linux":
            report.extend(self._audit_linux())
        elif system_os == "Darwin":
            report.extend(self._audit_mac())
        else:
            report.append("Unsupported Operating System for detailed hardware audit.")

        # 2. Software Section
        report.append("\n--- Software Environment ---")
        report.append(f"Python Version: {platform.python_version()}")
        report.append(f"Python Executable: {sys.executable}")
        
        return "\n".join(report)

    def _run_cmd(self, cmd: str) -> str:
        """Helper to run shell commands safely."""
        try:
            # shell=True is often required for piped commands, specifically on Windows/Linux
            result = subprocess.run(
                cmd, 
                text=True, 
                capture_output=True, 
                check=False, 
                shell=True, 
                timeout=5
            )
            if result.returncode == 0 and result.stdout:
                return result.stdout.strip()
            elif result.stderr:
                return f"[Cmd Error]: {result.stderr.strip()}"
            return "[No Output]"
        except Exception as e:
            return f"[Execution Error]: {e}"

    # --- OS Specific Implementations ---

    def _audit_windows(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("wmic cpu get name"))
        # GPU
        data.append("GPU: " + self._run_cmd("wmic path win32_videocontroller get name"))
        # RAM
        try:
            mem_str = self._run_cmd("wmic computersystem get totalphysicalmemory").splitlines()[-1]
            mem_bytes = int(mem_str)
            data.append(f"Memory: {mem_bytes / (1024**3):.2f} GB")
        except:
            data.append("Memory: Could not retrieve total physical memory.")
        # Disk
        data.append("\nDisks:")
        data.append(self._run_cmd("wmic diskdrive get model,size"))
        return data

    def _audit_linux(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("lscpu | grep 'Model name'"))
        # GPU (Requires lspci, usually in pciutils)
        data.append("GPU: " + self._run_cmd("lspci | grep -i vga"))
        # RAM
        data.append("Memory:\n" + self._run_cmd("free -h"))
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("lsblk -o NAME,SIZE,MODEL"))
        return data

    def _audit_mac(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("sysctl -n machdep.cpu.brand_string"))
        # GPU
        data.append("GPU:\n" + self._run_cmd("system_profiler SPDisplaysDataType | grep -E 'Chipset Model|VRAM'"))
        # RAM
        data.append("Memory Details:\n" + self._run_cmd("system_profiler SPMemoryDataType | grep -E 'Size|Type|Speed'"))
        # RAM Total
        try:
            mem_bytes = int(self._run_cmd('sysctl -n hw.memsize'))
            data.append(f"Total Memory: {mem_bytes / (1024**3):.2f} GB")
        except: 
            pass
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("diskutil list physical"))
        return data

# --- Independent Test Block ---
if __name__ == "__main__":
    inspector = SysInspectorMS()
    print("Running System Inspector...")
    print("\n" + inspector.generate_report())
--------------------------------------------------------------------------------

-------------------- FILE: _TasklistVaultMS\task_vault.py ------------------------------
import sqlite3
import uuid
import logging
import datetime
import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Literal

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("task_vault.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("TaskVault")

TaskStatus = Literal["Pending", "Running", "Complete", "Error", "Awaiting-Approval"]
# ==============================================================================

class TaskVaultMS:
    """
    The Taskmaster: A persistent SQLite engine for hierarchical task management.
    Supports infinite nesting of sub-tasks and status tracking.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. Task Lists (The containers)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_lists (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP
                )
            """)
            # 2. Tasks (The items, supporting hierarchy via parent_id)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tasks (
                    id TEXT PRIMARY KEY,
                    list_id TEXT NOT NULL,
                    parent_id TEXT,
                    content TEXT NOT NULL,
                    status TEXT DEFAULT 'Pending',
                    result TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    FOREIGN KEY(list_id) REFERENCES task_lists(id) ON DELETE CASCADE,
                    FOREIGN KEY(parent_id) REFERENCES tasks(id) ON DELETE CASCADE
                )
            """)

    # --- List Management ---

    def create_list(self, name: str) -> str:
        """Creates a new task list and returns its ID."""
        list_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO task_lists (id, name, created_at) VALUES (?, ?, ?)",
                (list_id, name, now)
            )
        log.info(f"Created Task List: '{name}' ({list_id})")
        return list_id

    def get_lists(self) -> List[Dict]:
        """Returns metadata for all task lists."""
        with self._get_conn() as conn:
            rows = conn.execute("SELECT * FROM task_lists ORDER BY created_at DESC").fetchall()
            return [dict(r) for r in rows]

    # --- Task Management ---

    def add_task(self, list_id: str, content: str, parent_id: Optional[str] = None) -> str:
        """Adds a task (or sub-task) to a list."""
        task_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                """INSERT INTO tasks (id, list_id, parent_id, content, status, created_at, updated_at) 
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (task_id, list_id, parent_id, content, "Pending", now, now)
            )
        return task_id

    def update_task(self, task_id: str, content: str = None, status: TaskStatus = None, result: str = None):
        """Updates a task's details."""
        updates = []
        params = []
        
        if content:
            updates.append("content = ?")
            params.append(content)
        if status:
            updates.append("status = ?")
            params.append(status)
        if result:
            updates.append("result = ?")
            params.append(result)
            
        if not updates: return

        updates.append("updated_at = ?")
        params.append(datetime.datetime.utcnow())
        params.append(task_id)

        sql = f"UPDATE tasks SET {', '.join(updates)} WHERE id = ?"
        
        with self._get_conn() as conn:
            conn.execute(sql, params)
        log.info(f"Updated task {task_id}")

    # --- Tree Reconstruction ---

    def get_full_tree(self, list_id: str) -> Dict[str, Any]:
        """
        Fetches a list and reconstructs the full hierarchy of tasks.
        """
        with self._get_conn() as conn:
            # 1. Get List Info
            list_row = conn.execute("SELECT * FROM task_lists WHERE id = ?", (list_id,)).fetchone()
            if not list_row: return None
            
            # 2. Get All Tasks
            task_rows = conn.execute("SELECT * FROM tasks WHERE list_id = ?", (list_id,)).fetchall()
            
        # 3. Build Adjacency Map
        tasks_by_id = {}
        for r in task_rows:
            t = dict(r)
            t['sub_tasks'] = [] # Prepare children container
            tasks_by_id[t['id']] = t

        # 4. Link Parents and Children
        root_tasks = []
        for t_id, task in tasks_by_id.items():
            parent_id = task['parent_id']
            if parent_id and parent_id in tasks_by_id:
                tasks_by_id[parent_id]['sub_tasks'].append(task)
            else:
                root_tasks.append(task)

        return {
            "id": list_row['id'],
            "name": list_row['name'],
            "tasks": root_tasks
        }

    def delete_list(self, list_id: str):
        with self._get_conn() as conn:
            conn.execute("DELETE FROM task_lists WHERE id = ?", (list_id,))
        log.info(f"Deleted list {list_id}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    vault = TaskVaultMS()
    
    # 1. Create a Plan
    plan_id = vault.create_list("System Upgrade Plan")
    
    # 2. Add Root Tasks
    t1 = vault.add_task(plan_id, "Backup Database")
    t2 = vault.add_task(plan_id, "Update Server")
    
    # 3. Add Sub-Tasks
    t2_1 = vault.add_task(plan_id, "Stop Services", parent_id=t2)
    t2_2 = vault.add_task(plan_id, "Run Installer", parent_id=t2)
    
    # 4. Update Status
    vault.update_task(t1, status="Complete", result="Backup saved to /tmp/bk.tar")
    vault.update_task(t2_1, status="Running")
    
    # 5. Render Tree
    tree = vault.get_full_tree(plan_id)
    print(f"\n--- {tree['name']} ---")
    
    def print_node(node, indent=0):
        status_icon = "âœ“" if node['status'] == 'Complete' else "â—‹"
        print(f"{'  '*indent}{status_icon} {node['content']} [{node['status']}]")
        for child in node['sub_tasks']:
            print_node(child, indent + 1)

    for task in tree['tasks']:
        print_node(task)
        
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)
--------------------------------------------------------------------------------

-------------------- FILE: _TextChunkerMS\text_chunker.py ------------------------------
from typing import List, Tuple, Dict, Any

class TextChunkerMS:
    """
    The Butcher: A unified service for splitting text into digestible chunks
    for RAG (Retrieval Augmented Generation).
    """
    
    @staticmethod
    def chunk_by_chars(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
        """
        Standard Sliding Window. Best for prose/documentation.
        Splits purely by character count.
        """
        if chunk_size <= 0: raise ValueError("chunk_size must be positive")
        
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            # Advance start, backing up by overlap
            start += chunk_size - chunk_overlap
            
        return chunks

    @staticmethod
    def chunk_by_lines(text: str, max_lines: int = 200, max_chars: int = 4000) -> List[Dict[str, Any]]:
        """
        Line-Preserving Chunker. Best for Code.
        Respects line boundaries and returns metadata about line numbers.
        """
        lines = text.splitlines()
        chunks = []
        start = 0
        
        while start < len(lines):
            end = min(start + max_lines, len(lines))
            chunk_str = "\n".join(lines[start:end])
            
            # If too big, shrink window (back off)
            while len(chunk_str) > max_chars and end > start + 1:
                end -= 1
                chunk_str = "\n".join(lines[start:end])
            
            chunks.append({
                "text": chunk_str,
                "start_line": start + 1,
                "end_line": end
            })
            start = end
            
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
    chunker = TextChunkerMS()
    
    # 1. Prose Test
    print("--- Prose Chunking ---")
    lorem = "A" * 100 # 100 chars
    result = chunker.chunk_by_chars(lorem, chunk_size=40, chunk_overlap=10)
    for i, c in enumerate(result):
        print(f"Chunk {i}: len={len(c)}")

    # 2. Code Test
    print("\n--- Code Chunking ---")
    code = "\n".join([f"print('Line {i}')" for i in range(1, 10)])
    # Force splits small for testing
    result_code = chunker.chunk_by_lines(code, max_lines=3, max_chars=100)
    for i, c in enumerate(result_code):
        print(f"Chunk {i}: Lines {c['start_line']}-{c['end_line']}")
--------------------------------------------------------------------------------

-------------------- FILE: _ThoughtStreamMS\thought_stream.py --------------------------
import tkinter as tk
from tkinter import ttk
import datetime

class ThoughtStream(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        
        # Header
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        # The Stream Area (Canvas allows for custom drawing like sparklines)
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340) # Fixed width like React
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    def add_thought_bubble(self, filename, chunk_id, content, vector_preview, color):
        """
        Mimics the 'InspectorFrame' from your React code.
        """
        # Bubble Container
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        # Header: File + Timestamp
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        header_lbl = tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", 
                              fg="#007ACC", bg="#1a1a25", font=("Consolas", 8))
        header_lbl.pack(anchor="w", padx=5, pady=2)
        
        # Content Snippet
        snippet = content[:400] + "..." if len(content) > 400 else content
        content_lbl = tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", 
                               font=("Consolas", 8), justify="left", wraplength=300)
        content_lbl.pack(fill="x", padx=5, pady=2)
        
        # Vector Sparkline (The Custom Draw)
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector, color):
        """
        Recreates the 'vector_preview' visual from React using a micro-canvas.
        """
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        
        bar_w = w / len(vector) if len(vector) > 0 else 0
        
        for i, val in enumerate(vector):
            # Normalize -1..1 to 0..1 for height
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            
            # Draw bar
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")

# --- Usage Example ---
if __name__ == "__main__":
    root = tk.Tk()
    root.geometry("400x600")
    
    stream = ThoughtStream(root)
    stream.pack(fill="both", expand=True)
    
    # Simulate an incoming "Microservice" event
    import random
    fake_vector = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("ExplorerView.tsx", 1, "import React from 'react'...", fake_vector, "#FF00FF")
    
    root.mainloop()
--------------------------------------------------------------------------------

-------------------- FILE: _TreeMapperMS\tree_mapper.py --------------------------------
import os
from pathlib import Path
from typing import List, Set, Optional
import datetime

# ==============================================================================
# USER CONFIGURATION: DEFAULT EXCLUSIONS
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    '.venv', 'env', 'venv', 'dist', 'build', '.DS_Store'
}
# ==============================================================================

class TreeMapperMS:
    """
    The Cartographer: Generates ASCII-art style directory maps.
    """
    
    def generate_tree(self, 
                      root_path: str, 
                      additional_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> str:
        
        start_path = Path(root_path).resolve()
        if not start_path.exists(): return f"Error: Path '{root_path}' does not exist."

        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_EXCLUDES)
        if additional_exclusions:
            exclusions.update(additional_exclusions)

        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lines = [
            f"Project Map: {start_path.name}",
            f"Generated: {timestamp}",
            "-" * 40,
            f"ðŸ“ {start_path.name}/"
        ]

        self._walk(start_path, "", lines, exclusions)
        return "\n".join(lines)

    def _walk(self, directory: Path, prefix: str, lines: List[str], exclusions: Set[str]):
        try:
            children = sorted(
                [p for p in directory.iterdir() if p.name not in exclusions],
                key=lambda x: (x.is_file(), x.name.lower())
            )
        except PermissionError:
            lines.append(f"{prefix}â””â”€â”€ ðŸš« [Permission Denied]")
            return

        count = len(children)
        for index, path in enumerate(children):
            is_last = (index == count - 1)
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            
            if path.is_dir():
                lines.append(f"{prefix}{connector}ðŸ“ {path.name}/")
                extension = "    " if is_last else "â”‚   "
                self._walk(path, prefix + extension, lines, exclusions)
            else:
                lines.append(f"{prefix}{connector}ðŸ“„ {path.name}")

if __name__ == "__main__":
    print("TreeMapper initialized. Check top of file to tweak defaults.")
--------------------------------------------------------------------------------

-------------------- FILE: _VectorFactoryMS\requirements.txt ---------------------------
pip install chromadb faiss-cpu numpy
--------------------------------------------------------------------------------

-------------------- FILE: _VectorFactoryMS\vector_factory.py --------------------------
import os
import uuid
import logging
import shutil
from typing import List, Dict, Any, Optional, Protocol, Union
from pathlib import Path

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("VectorFactory")
# ==============================================================================

# --- Interface Definition ---

class VectorStore(Protocol):
    """The contract that all vector backends must fulfill."""
    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]) -> None:
        ...
    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        ...
    def count(self) -> int:
        ...
    def clear(self) -> None:
        ...

# --- Implementation 1: FAISS (Local, Fast, RAM-heavy) ---

class FaissVectorStore:
    def __init__(self, index_path: str, dimension: int):
        import numpy as np
        import faiss # Lazy import
        self.np = np
        self.faiss = faiss
        
        self.index_path = index_path
        self.dim = dimension
        self.metadata_store = []
        
        # Load or Create
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
            # Load metadata (simple JSON sidecar for this implementation)
            meta_path = index_path + ".meta.json"
            if os.path.exists(meta_path):
                import json
                with open(meta_path, 'r') as f:
                    self.metadata_store = json.load(f)
        else:
            self.index = faiss.IndexFlatL2(dimension)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        
        vecs = self.np.array(embeddings).astype("float32")
        self.index.add(vecs)
        self.metadata_store.extend(metadatas)
        self._save()

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        if self.index.ntotal == 0: return []
        
        q_vec = self.np.array([query_vector]).astype("float32")
        distances, indices = self.index.search(q_vec, k)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx != -1 and idx < len(self.metadata_store):
                entry = self.metadata_store[idx].copy()
                entry['score'] = float(dist) # FAISS returns L2 distance (lower is better)
                results.append(entry)
        return results

    def count(self) -> int:
        return self.index.ntotal

    def clear(self):
        self.index.reset()
        self.metadata_store = []
        self._save()

    def _save(self):
        self.faiss.write_index(self.index, self.index_path)
        import json
        with open(self.index_path + ".meta.json", 'w') as f:
            json.dump(self.metadata_store, f)

# --- Implementation 2: ChromaDB (Persistent, Feature-rich) ---

class ChromaVectorStore:
    def __init__(self, persist_dir: str, collection_name: str):
        import chromadb # Lazy import
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(collection_name)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        # Chroma requires unique IDs
        ids = [str(uuid.uuid4()) for _ in embeddings]
        
        # Ensure metadata is flat (Chroma limitation on nested dicts)
        clean_metas = [{k: str(v) if isinstance(v, (list, dict)) else v for k, v in m.items()} for m in metadatas]
        
        # Chroma expects 'documents' usually, but we handle logic upstream. 
        # We pass empty strings for 'documents' if purely vector-based, 
        # or map content from metadata if available.
        docs = [m.get("content", "") for m in metadatas]

        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            metadatas=clean_metas,
            documents=docs
        )

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        results = self.collection.query(
            query_embeddings=[query_vector],
            n_results=k
        )
        
        output = []
        if not results['ids']: return []

        # Unpack Chroma's columnar response format
        for i in range(len(results['ids'][0])):
            entry = results['metadatas'][0][i].copy()
            entry['score'] = results['distances'][0][i]
            entry['id'] = results['ids'][0][i]
            output.append(entry)
        return output

    def count(self) -> int:
        return self.collection.count()

    def clear(self):
        # Chroma doesn't have a truncate command, so we delete the collection
        name = self.collection.name
        self.client.delete_collection(name)
        self.collection = self.client.get_or_create_collection(name)

# --- The Factory ---

class VectorFactoryMS:
    """
    The Switchboard: Returns the appropriate VectorStore implementation
    based on configuration.
    """
    
    @staticmethod
    def create(backend: str, config: Dict[str, Any]) -> VectorStore:
        """
        :param backend: 'faiss' or 'chroma'
        :param config: Dict containing 'path', 'dim' (for FAISS), or 'collection' (for Chroma)
        """
        log.info(f"Initializing Vector Store: {backend.upper()}")
        
        if backend == "faiss":
            path = config.get("path", "vector_index.bin")
            dim = config.get("dim", 384)
            return FaissVectorStore(path, dim)
            
        elif backend == "chroma":
            path = config.get("path", "./chroma_db")
            name = config.get("collection", "default_collection")
            return ChromaVectorStore(path, name)
            
        else:
            raise ValueError(f"Unknown backend: {backend}")

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing VectorFactoryMS ---")
    
    # 1. Mock Data (dim=4 for simplicity)
    mock_vec = [0.1, 0.2, 0.3, 0.4]
    mock_meta = {"text": "Hello World", "source": "test"}
    
    # 2. Test FAISS
    print("\n[Testing FAISS]")
    try:
        faiss_store = VectorFactoryMS.create("faiss", {"path": "test_faiss.index", "dim": 4})
        faiss_store.add([mock_vec], [mock_meta])
        print(f"Count: {faiss_store.count()}")
        res = faiss_store.search(mock_vec, 1)
        print(f"Search Result: {res[0]['text']}")
        # Cleanup
        if os.path.exists("test_faiss.index"): os.remove("test_faiss.index")
        if os.path.exists("test_faiss.index.meta.json"): os.remove("test_faiss.index.meta.json")
    except ImportError:
        print("Skipping FAISS test (library not installed)")

    # 3. Test Chroma
    print("\n[Testing Chroma]")
    try:
        chroma_store = VectorFactoryMS.create("chroma", {"path": "./test_chroma_db", "collection": "test_col"})
        chroma_store.add([mock_vec], [mock_meta])
        print(f"Count: {chroma_store.count()}")
        res = chroma_store.search(mock_vec, 1)
        print(f"Search Result: {res[0]['text']}")
        # Cleanup
        if os.path.exists("./test_chroma_db"): shutil.rmtree("./test_chroma_db")
    except ImportError:
        print("Skipping Chroma test (library not installed)")
    except Exception as e:
        print(f"Chroma Error: {e}")
--------------------------------------------------------------------------------

-------------------- FILE: _WebScraperMS\requirements.txt ------------------------------
pip install httpx readability-lxml
--------------------------------------------------------------------------------

-------------------- FILE: _WebScraperMS\web_scraper.py --------------------------------
import httpx
import logging
import asyncio
from typing import Optional, Dict, Any
from readability import Document

# ==============================================================================
# CONFIGURATION
# ==============================================================================
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
TIMEOUT_SECONDS = 15.0

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("WebScraper")
# ==============================================================================

class WebScraperMS:
    """
    The Reader: Fetches URLs and extracts the main content using Readability.
    Strips ads, navbars, and boilerplate to return clean text for LLMs.
    """
    def __init__(self):
        self.headers = {"User-Agent": USER_AGENT}

    def scrape(self, url: str) -> Dict[str, Any]:
        """
        Synchronous wrapper for fetching and cleaning a URL.
        Returns: {
            "url": str,
            "title": str,
            "content": str (The main body text),
            "html": str (The raw HTML of the main content area)
        }
        """
        return asyncio.run(self._scrape_async(url))

    async def _scrape_async(self, url: str) -> Dict[str, Any]:
        log.info(f"Fetching: {url}")
        
        async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=TIMEOUT_SECONDS) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                log.error(f"HTTP Error {e.response.status_code}: {e}")
                raise
            except httpx.RequestError as e:
                log.error(f"Request failed: {e}")
                raise

        # Parse with Readability
        try:
            doc = Document(response.text)
            title = doc.title()
            # Summary() returns the HTML of the main content area
            clean_html = doc.summary() 
            
            # Convert HTML content to plain text for the LLM
            # (Simple strip tags implementation, for better results use BeautifulSoup)
            clean_text = self._strip_tags(clean_html)
            
            log.info(f"Successfully scraped '{title}' ({len(clean_text)} chars)")
            
            return {
                "url": url,
                "title": title,
                "content": clean_text,
                "html": clean_html
            }
        except Exception as e:
            log.error(f"Parsing failed: {e}")
            raise

    def _strip_tags(self, html: str) -> str:
        """
        Removes HTML tags to leave only the readable text.
        Note: A robust production version should use BeautifulSoup4.
        """
        import re
        # Remove scripts and styles
        html = re.sub(r'<(script|style).*?>.*?</\1>', '', html, flags=re.DOTALL)
        # Remove tags
        text = re.sub(r'<[^>]+>', ' ', html)
        # Collapse whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# --- Independent Test Block ---
if __name__ == "__main__":
    scraper = WebScraperMS()
    
    # Test URL (Example: Python's PEP 8)
    target_url = "https://peps.python.org/pep-0008/"
    
    print(f"--- Scraping {target_url} ---")
    try:
        data = scraper.scrape(target_url)
        print(f"\nTitle: {data['title']}")
        print(f"Content Preview:\n{data['content'][:500]}...")
        print(f"\nTotal Length: {len(data['content'])} characters")
    except Exception as e:
        print(f"Scrape failed: {e}")
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------

-------------------- FILE: XXX_Logs_XXX\MicroservicesLIBRARY__folder_tree.txt ----------
Project Root: C:\Users\jacob\Documents\Scripts\useful-helper-scripts\_MicroserviceCOLLECTION
Generated: 2025-12-02 20:48:42
Global Default Folder Exclusions: .git, .idea, .mypy_cache, .venv, .vscode, Debug, Release, __pycache__, _logs, bin, build, dist, logs, node_modules, obj, out, target
Predefined Filename Exclusions: *.pyc, *.pyo, *.swo, *.swp, .DS_Store, Thumbs.db, package-lock.json, yarn.lock
Dynamic Filename Exclusions: None

[X] _MicroserviceCOLLECTION/ (Project Root)
  â”œâ”€â”€ [X] _APIGatewayMS/
  â”‚   â”œâ”€â”€ ðŸ“„ api_gateway.py
  â”‚   â””â”€â”€ ðŸ“„ requirements.txt
  â”œâ”€â”€ [X] _ArchiveBotMS/
  â”‚   â””â”€â”€ ðŸ“„ archive_bot.py
  â”œâ”€â”€ [X] _AuthMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ auth.py
  â”œâ”€â”€ [X] _CodeChunkerMS/
  â”‚   â””â”€â”€ ðŸ“„ code_chunker.py
  â”œâ”€â”€ [X] _CodeGrapherMS/
  â”‚   â””â”€â”€ ðŸ“„ code_grapher.py
  â”œâ”€â”€ [X] _CognitiveMemoryMS/
  â”‚   â”œâ”€â”€ ðŸ“„ cognitive_memory.py
  â”‚   â””â”€â”€ ðŸ“„ requirements.txt
  â”œâ”€â”€ [X] _ContextAggregatorMS/
  â”‚   â””â”€â”€ ðŸ“„ context_aggregator.py
  â”œâ”€â”€ [X] _DiffEngineMS/
  â”‚   â””â”€â”€ ðŸ“„ diff_engine.py
  â”œâ”€â”€ [X] _ExplorerWidgetMS/
  â”‚   â””â”€â”€ ðŸ“„ explorer_widget.py
  â”œâ”€â”€ [X] _FingerprintScannerMS/
  â”‚   â””â”€â”€ ðŸ“„ fingerprint_scanner.py
  â”œâ”€â”€ [X] _GitPilotMS/
  â”‚   â””â”€â”€ ðŸ“„ git_pilot.py
  â”œâ”€â”€ [X] _GraphEngineMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â”œâ”€â”€ ðŸ“„ graph_engine.py
  â”‚   â””â”€â”€ ðŸ“„ graph_view.py
  â”œâ”€â”€ [X] _HeruisticSumMS/
  â”‚   â””â”€â”€ ðŸ“„ heuristic_summarizer.py
  â”œâ”€â”€ [X] _IngestEngineMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ ingest_engine.py
  â”œâ”€â”€ [X] _IsoProcessMS/
  â”‚   â””â”€â”€ ðŸ“„ iso_process.py
  â”œâ”€â”€ [X] _LexicalSearchMS/
  â”‚   â””â”€â”€ ðŸ“„ lexical_search.py
  â”œâ”€â”€ [X] _LibrarianServiceMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ librarian_service.py
  â”œâ”€â”€ [X] _MonacoHostMS/
  â”‚   â”œâ”€â”€ ðŸ“„ editor.html
  â”‚   â””â”€â”€ ðŸ“„ monaco_host.py
  â”œâ”€â”€ [X] _NetworkLayoutMS/
  â”‚   â”œâ”€â”€ ðŸ“„ network_layout.py
  â”‚   â””â”€â”€ ðŸ“„ requirements.txt
  â”œâ”€â”€ [X] _PromptOptimizerMS/
  â”‚   â””â”€â”€ ðŸ“„ prompt_optimizer.py
  â”œâ”€â”€ [X] _PromptVaultMS/
  â”‚   â””â”€â”€ ðŸ“„ prompt_vault.py
  â”œâ”€â”€ [X] _RegexWeaverMS/
  â”‚   â””â”€â”€ ðŸ“„ regex_weaver.py
  â”œâ”€â”€ [X] _SandboxManagerMS/
  â”‚   â””â”€â”€ ðŸ“„ sandbox_manager.py
  â”œâ”€â”€ [X] _ScannerMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ scanner.py
  â”œâ”€â”€ [X] _SearchEngineMS/
  â”‚   â””â”€â”€ ðŸ“„ search_engine.py
  â”œâ”€â”€ [X] _ServiceRegistryMS/
  â”‚   â””â”€â”€ ðŸ“„ service_registry.py
  â”œâ”€â”€ [X] _SmartChunkerMS/
  â”‚   â””â”€â”€ ðŸ“„ smart_chunker.py
  â”œâ”€â”€ [X] _SysInspectorMS/
  â”‚   â””â”€â”€ ðŸ“„ sys_inspector.py
  â”œâ”€â”€ [X] _TasklistVaultMS/
  â”‚   â””â”€â”€ ðŸ“„ task_vault.py
  â”œâ”€â”€ [X] _TextChunkerMS/
  â”‚   â””â”€â”€ ðŸ“„ text_chunker.py
  â”œâ”€â”€ [X] _ThoughtStreamMS/
  â”‚   â”œâ”€â”€ [ ] __pycache__/
  â”‚   â””â”€â”€ ðŸ“„ thought_stream.py
  â”œâ”€â”€ [X] _TreeMapperMS/
  â”‚   â””â”€â”€ ðŸ“„ tree_mapper.py
  â”œâ”€â”€ [X] _VectorFactoryMS/
  â”‚   â”œâ”€â”€ ðŸ“„ requirements.txt
  â”‚   â””â”€â”€ ðŸ“„ vector_factory.py
  â”œâ”€â”€ [X] _WebScraperMS/
  â”‚   â”œâ”€â”€ ðŸ“„ requirements.txt
  â”‚   â””â”€â”€ ðŸ“„ web_scraper.py
  â”œâ”€â”€ [X] XXX_Example_and_Snippets_XXX/
  â”‚   â”œâ”€â”€ ðŸ“„ KB_Box_Snippets.json
  â”‚   â”œâ”€â”€ ðŸ“„ NeoCORTEX_Database_snippets.json
  â”‚   â””â”€â”€ ðŸ“„ ProjectMAPPER_snippets.json
  â””â”€â”€ [X] XXX_Logs_XXX/
      â”œâ”€â”€ ðŸ“„ MicroservicesLIBRARY__file_tree.txt
      â””â”€â”€ ðŸ“„ MicroservicesLIBRARY__file_tree_2025-12-02_10-58-03.txt
--------------------------------------------------------------------------------

-------------------- FILE: _APIGatewayMS\api_gateway.py --------------------------------
import sys
import threading
from typing import Any, Dict, Optional, Callable
import uvicorn

# ==============================================================================
# CONFIGURATION
# ==============================================================================
API_TITLE = "Microservice Gateway"
API_VERSION = "1.0.0"
DEFAULT_HOST = "0.0.0.0"
DEFAULT_PORT = 8099
# ==============================================================================

class APIGatewayMS:
    """
    The Gateway: A wrapper that exposes a local Python object as a REST API.
    
    Features:
    - Auto-generates Swagger UI at /docs
    - Threaded execution (non-blocking for UI apps)
    - CORS enabled by default (for React/Web frontends)
    """

    def __init__(self, backend_core: Any):
        """
        :param backend_core: The logic object to expose (e.g. an instance of SearchEngineMS)
        """
        self.core = backend_core
        self.server_thread = None
        
        # Lazy import to avoid hard crash if libs are missing
        try:
            from fastapi import FastAPI
            from fastapi.middleware.cors import CORSMiddleware
            from pydantic import BaseModel
            self.FastAPI = FastAPI
            self.CORSMiddleware = CORSMiddleware
            self.BaseModel = BaseModel
            self._available = True
        except ImportError:
            print("CRITICAL: 'fastapi' or 'uvicorn' not installed.")
            print("Run: pip install fastapi uvicorn pydantic")
            self._available = False
            return

        self.app = self.FastAPI(title=API_TITLE, version=API_VERSION)
        
        # Enable CORS
        self.app.add_middleware(
            self.CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"]
        )
        
        # Setup Base Routes
        self._setup_system_routes()

    def _setup_system_routes(self):
        @self.app.get("/")
        def root():
            return {"status": "online", "service": API_TITLE}

        @self.app.get("/health")
        def health():
            return {"status": "healthy", "backend_type": type(self.core).__name__}

    def add_endpoint(self, path: str, method: str, handler: Callable):
        """
        Dynamically adds a route to the API.
        
        :param path: URL path (e.g., "/search")
        :param method: "GET" or "POST"
        :param handler: The function to run
        """
        if method.upper() == "POST":
            self.app.post(path)(handler)
        elif method.upper() == "GET":
            self.app.get(path)(handler)

    def start(self, host: str = DEFAULT_HOST, port: int = DEFAULT_PORT, blocking: bool = True):
        """
        Starts the Uvicorn server.
        """
        if not self._available: return

        def _run():
            print(f"ðŸš€ API Gateway running at http://{host}:{port}")
            print(f"ðŸ“„ Docs available at http://{host}:{port}/docs")
            uvicorn.run(self.app, host=host, port=port, log_level="info")

        if blocking:
            _run()
        else:
            self.server_thread = threading.Thread(target=_run, daemon=True)
            self.server_thread.start()

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Define a Mock Backend (The "Core" Logic)
    class MockBackend:
        def search(self, query):
            return [f"Result for {query} 1", f"Result for {query} 2"]
        
        def echo(self, msg):
            return f"Echo: {msg}"

    backend = MockBackend()
    
    # 2. Init Gateway
    gateway = APIGatewayMS(backend)
    
    if gateway._available:
        # 3. Define Request Models (Pydantic) for strong typing in Swagger
        class SearchReq(gateway.BaseModel):
            query: str
            limit: int = 10

        class EchoReq(gateway.BaseModel):
            message: str

        # 4. Map Backend Methods to API Endpoints
        
        def search_endpoint(req: SearchReq):
            """Searches the mock backend."""
            return {"results": backend.search(req.query), "limit": req.limit}
            
        def echo_endpoint(req: EchoReq):
            """Echoes a message."""
            return {"response": backend.echo(req.message)}

        gateway.add_endpoint("/v1/search", "POST", search_endpoint)
        gateway.add_endpoint("/v1/echo", "POST", echo_endpoint)

        # 5. Run
        # Note: In a real app, you might want blocking=False if running a UI too
        gateway.start(port=8099, blocking=True)
--------------------------------------------------------------------------------

-------------------- FILE: _APIGatewayMS\requirements.txt ------------------------------
pip install fastapi uvicorn pydantic
--------------------------------------------------------------------------------

-------------------- FILE: _ArchiveBotMS\archive_bot.py --------------------------------
import tarfile
import os
import fnmatch
import datetime
from pathlib import Path
from typing import Set, Optional, Tuple

# ==============================================================================
# USER CONFIGURATION: DEFAULT EXCLUSIONS
# ==============================================================================
# Folders to ignore by default (Development artifacts, environments, etc.)
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", 
    "dist", "build", "coverage", "target", "out", "bin", "obj"
}

# Files to ignore by default (System metadata, temporary files, etc.)
DEFAULT_IGNORE_FILES = {
    ".DS_Store", "Thumbs.db", "*.pyc", "*.pyo", "*.log", "*.tmp"
}
# ==============================================================================

class ArchiveBotMS:
    """
    The Archiver: Creates compressed .tar.gz backups of project folders.
    """

    def create_backup(self, 
                      source_path: str, 
                      output_dir: str, 
                      extra_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> Tuple[str, int]:
        """
        Compresses the source_path into a .tar.gz file.
        
        :param use_default_exclusions: If False, ignores the DEFAULT_IGNORE lists at top of script.
        """
        src = Path(source_path).resolve()
        out = Path(output_dir).resolve()
        
        out.mkdir(parents=True, exist_ok=True)

        # Build exclusion set
        exclude_set = set()
        if use_default_exclusions:
            exclude_set.update(DEFAULT_IGNORE_DIRS)
            exclude_set.update(DEFAULT_IGNORE_FILES)
            
        if extra_exclusions:
            exclude_set.update(extra_exclusions)

        # Generate Timestamped Filename
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        archive_name = f"backup_{src.name}_{timestamp}.tar.gz"
        archive_path = out / archive_name

        file_count = 0

        try:
            with tarfile.open(archive_path, "w:gz") as tar:
                for root, dirs, files in os.walk(src):
                    # 1. Filter Directories in-place
                    for i in range(len(dirs) - 1, -1, -1):
                        d_name = dirs[i]
                        if self._is_excluded(d_name, exclude_set):
                            dirs.pop(i)
                    
                    # 2. Add Files
                    for file in files:
                        if self._is_excluded(file, exclude_set):
                            continue
                            
                        full_path = Path(root) / file
                        if full_path == archive_path: continue

                        rel_path = full_path.relative_to(src)
                        tar.add(full_path, arcname=rel_path)
                        file_count += 1
                        
            return str(archive_path), file_count

        except Exception as e:
            if archive_path.exists(): os.remove(archive_path)
            raise e

    def _is_excluded(self, name: str, patterns: Set[str]) -> bool:
        for pattern in patterns:
            if name == pattern: return True
            if fnmatch.fnmatch(name, pattern): return True
        return False

if __name__ == "__main__":
    # Test
    bot = ArchiveBotMS()
    print("ArchiveBot initialized. Check top of file to tweak defaults.")
--------------------------------------------------------------------------------

-------------------- FILE: _AuthMS\auth.py ---------------------------------------------
import hashlib
import time
import json
import base64
from typing import Optional, Dict

class AuthMS:
    """
    The Gatekeeper: Manages user authentication and session tokens.
    """
    def __init__(self, secret_key: str = "super_secret_cortex_key"):
        self.secret_key = secret_key
        # In a real scenario, this might load from a secure config file or DB
        self.users_db = {
            "admin": self._hash_password("admin123")
        }

    def login(self, username, password) -> Optional[str]:
        """
        Attempts to log in. Returns a session token if successful, None otherwise.
        """
        if username not in self.users_db:
            return None
        
        stored_hash = self.users_db[username]
        if self._verify_password(password, stored_hash):
            return self._create_token(username)
        
        return None

    def validate_session(self, token: str) -> bool:
        """
        Checks if a token is valid and not expired.
        """
        payload = self._decode_token(token)
        if payload:
            return True
        return False

    def _hash_password(self, password: str) -> str:
        """
        Securely hashes a password using SHA-256 (Simulated salt).
        """
        salt = "cortex_salt"
        return hashlib.sha256((password + salt).encode()).hexdigest()

    def _verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """
        Verifies a provided password against the stored hash.
        """
        return self._hash_password(plain_password) == hashed_password

    def _create_token(self, user_id: str, expires_in: int = 3600) -> str:
        """
        Generates a signed session token.
        Payload includes 'sub' (subject) and 'exp' (expiration).
        """
        payload = {
            "sub": user_id,
            "exp": int(time.time()) + expires_in,
            "iat": int(time.time()),
            "scope": "admin"
        }
        
        # Create the token parts
        json_payload = json.dumps(payload).encode()
        token_part = base64.b64encode(json_payload).decode()
        
        # Sign it
        signature = hashlib.sha256((token_part + self.secret_key).encode()).hexdigest()
        
        return f"{token_part}.{signature}"

    def _decode_token(self, token: str) -> Optional[Dict]:
        """
        Parses and validates the incoming token.
        Returns the payload if valid, None otherwise.
        """
        try:
            if not token or "." not in token:
                return None

            token_part, signature = token.split('.')
            
            # Re-calculate signature to verify integrity
            recalc_signature = hashlib.sha256((token_part + self.secret_key).encode()).hexdigest()
            
            if signature != recalc_signature:
                return None # Invalid Signature
            
            # Decode payload
            payload_json = base64.b64decode(token_part).decode()
            payload = json.loads(payload_json)
            
            # Check expiration
            if payload['exp'] < time.time():
                return None # Expired
                
            return payload
        except Exception:
            return None

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Auth Service Test ---")
    auth = AuthMS()
    
    # 1. Test Login Success
    print("Attempting login (admin / admin123)...")
    token = auth.login("admin", "admin123")
    
    if token:
        print(f"Login Success! Token: {token[:20]}...")
        
        # 2. Test Validation
        is_valid = auth.validate_session(token)
        print(f"Token Valid? {is_valid}")
    else:
        print("Login Failed.")

    # 3. Test Login Fail
    print("\nAttempting login (admin / wrongpass)...")
    bad_token = auth.login("admin", "wrongpass")
    if not bad_token:
        print("Login Failed (Expected).")
--------------------------------------------------------------------------------

-------------------- FILE: _CodeChunkerMS\code_chunker.py ------------------------------
import os
from typing import List, Dict, Any, Optional
from pathlib import Path

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Map extensions to tree-sitter language names
LANG_MAP = {
    ".py": "python",
    ".js": "javascript", ".jsx": "javascript",
    ".ts": "typescript", ".tsx": "typescript",
    ".go": "go",
    ".rs": "rust",
    ".java": "java",
    ".cpp": "cpp", ".cc": "cpp", ".c": "c", ".h": "c"
}

# The "Atomic Units" we want to keep whole
CHUNK_NODES = {
    "class_definition", "function_definition", "method_definition", # Python
    "class_declaration", "function_declaration", "method_definition", # JS/TS
    "func_literal", "function_declaration", # Go
    "function_item", "impl_item" # Rust
}

class CodeChunkerMS:
    """
    The Surgeon: Uses Tree-Sitter (CST) to structurally parse code.
    Splits files into 'Semantic Chunks' (Classes, Functions) rather than
    arbitrary text slices. Preserves comments and docstrings.
    """
    def __init__(self):
        self._available = False
        try:
            from tree_sitter import Parser
            from tree_sitter_languages import get_language
            self.Parser = Parser
            self.get_language = get_language
            self._available = True
        except ImportError:
            print("CRITICAL: tree-sitter not installed. Chunker will fail.")

    def chunk_file(self, file_path: str, max_chars: int = 1500) -> List[Dict[str, Any]]:
        """
        Reads a file and breaks it into semantic code blocks.
        """
        if not self._available: return []
        
        path = Path(file_path)
        ext = path.suffix.lower()
        if ext not in LANG_MAP:
            return [] # fallback to text chunker for unknown types?

        lang_id = LANG_MAP[ext]
        try:
            code = path.read_text(encoding="utf-8", errors="ignore")
            return self._parse(code, lang_id, max_chars)
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
            return []

    def _parse(self, code: str, lang_id: str, max_chars: int) -> List[Dict]:
        language = self.get_language(lang_id)
        parser = self.Parser()
        parser.set_language(language)
        
        tree = parser.parse(bytes(code, "utf8"))
        chunks = []
        
        # Cursor allows efficient traversal
        cursor = tree.walk()
        
        # Recursive walker to find significant nodes
        def walk(node):
            # If this node is a major block (Function/Class)
            if node.type in CHUNK_NODES:
                # 1. Capture the code including preceding comments
                start_byte = node.start_byte
                end_byte = node.end_byte
                
                # Look backwards for docstrings/comments attached to this node
                # (Simple heuristic: scan previous sibling)
                prev = node.prev_sibling
                if prev and prev.type == "comment":
                    start_byte = prev.start_byte

                chunk_text = code[start_byte:end_byte]
                
                # 2. Check size
                if len(chunk_text) > max_chars:
                    # Too big? Recurse into children (split the function apart)
                    for child in node.children:
                        walk(child)
                else:
                    # Good size? Keep it as a chunk
                    chunks.append({
                        "type": node.type,
                        "text": chunk_text,
                        "start_line": node.start_point[0],
                        "end_line": node.end_point[0]
                    })
                return # Don't recurse if we kept the parent

            # If not a major node, just keep walking down
            for child in node.children:
                walk(child)

        walk(tree.root_node)
        
        # Fallback: If no structural chunks found (e.g. flat script), return whole file
        if not chunks and code.strip():
            chunks.append({
                "type": "file", 
                "text": code, 
                "start_line": 0, 
                "end_line": len(code.splitlines())
            })
            
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
    # Test with a dummy Python file
    chunker = CodeChunkerMS()
    
    if chunker._available:
        py_code = """
        # This is a helper function
        def helper(x):
            return x * 2

        class Processor:
            '''
            Main processing class.
            '''
            def process(self, data):
                # Process the data
                if data:
                    return helper(data)
                return None
        """
        
        # Write temp file
        import tempfile
        with tempfile.NamedTemporaryFile(suffix=".py", mode="w+", delete=False) as tmp:
            tmp.write(py_code)
            tmp_path = tmp.name
            
        print(f"--- Chunking {tmp_path} ---")
        chunks = chunker.chunk_file(tmp_path)
        
        for i, c in enumerate(chunks):
            print(f"\n[Chunk {i}] Type: {c['type']} (Lines {c['start_line']}-{c['end_line']})")
            print(f"{'-'*20}\n{c['text'].strip()}\n{'-'*20}")
            
        os.remove(tmp_path)
    else:
        print("Skipping test: tree-sitter not installed.")
--------------------------------------------------------------------------------

-------------------- FILE: _CodeGrapherMS\code_grapher.py ------------------------------
import ast
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional

class CodeGrapherMS:
    """
    The Cartographer of Logic: Parses Python code to extract high-level 
    symbols (classes, functions) and maps their 'Call' relationships.
    
    Output: A graph structure (Nodes + Edges) suitable for visualization 
    or dependency analysis.
    """

    def __init__(self):
        self.nodes = [] # List of symbols (functions, classes)
        self.edges = [] # List of relationships (source -> target)

    def scan_directory(self, root_path: str) -> Dict[str, Any]:
        """
        Recursively scans a directory for .py files and builds the graph.
        """
        root = Path(root_path).resolve()
        self.nodes = []
        self.edges = []
        
        if not root.exists():
            return {"error": f"Path {root} does not exist"}

        # 1. Parsing Pass (Create Nodes)
        for path in root.rglob("*.py"):
            try:
                # Skip hidden/venv folders
                if any(p.startswith('.') for p in path.parts) or "venv" in path.parts:
                    continue
                    
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    source = f.read()
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_symbols = self._parse_source(source, rel_path)
                self.nodes.extend(file_symbols)
                
            except Exception as e:
                print(f"Failed to parse {path.name}: {e}")

        # 2. Linking Pass (Create Edges)
        self._build_edges()

        return {
            "root": str(root),
            "node_count": len(self.nodes),
            "edge_count": len(self.edges),
            "nodes": self.nodes,
            "edges": self.edges
        }

    def _parse_source(self, source: str, file_path: str) -> List[Dict]:
        """
        Uses Python's AST to extract surgical symbol info.
        """
        try:
            tree = ast.parse(source)
        except SyntaxError:
            return []

        visitor = SurgicalVisitor(file_path)
        visitor.visit(tree)
        return visitor.symbols

    def _build_edges(self):
        """
        Resolves 'calls' strings into explicit graph edges.
        """
        # Create a quick lookup map: "my_function" -> NodeID
        # Note: This is a naive lookup (name collision possible). 
        # A robust version would use "module.class.method" fully qualified names.
        name_map = {n['name']: n['id'] for n in self.nodes}

        for node in self.nodes:
            source_id = node['id']
            calls = node.get('calls', [])
            
            for target_name in calls:
                if target_name in name_map:
                    target_id = name_map[target_name]
                    
                    # Avoid self-loops for cleanliness
                    if source_id != target_id:
                        self.edges.append({
                            "source": source_id,
                            "target": target_id,
                            "type": "calls"
                        })

# --- Helper Class: The AST Walker ---

class SurgicalVisitor(ast.NodeVisitor):
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.symbols = []

    def visit_FunctionDef(self, node):
        self._handle_func(node, "function")

    def visit_AsyncFunctionDef(self, node):
        self._handle_func(node, "async_function")

    def visit_ClassDef(self, node):
        # Record the class
        class_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": class_id,
            "file": self.file_path,
            "name": node.name,
            "type": "class",
            "line": node.lineno,
            "calls": [] # Classes don't 'call' things directly usually, their methods do
        })
        # Visit children (methods)
        self.generic_visit(node)

    def _handle_func(self, node, type_name):
        # Extract outgoing calls from the function body
        calls = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.append(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.append(child.func.attr)
        
        unique_calls = list(set(calls))
        
        node_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": node_id,
            "file": self.file_path,
            "name": node.name,
            "type": type_name,
            "line": node.lineno,
            "calls": unique_calls
        })

# --- Independent Test Block ---
if __name__ == "__main__":
    import sys
    
    # Defaults to current directory
    target_dir = sys.argv[1] if len(sys.argv) > 1 else "."
    
    print(f"Mapping Logic in: {target_dir}")
    grapher = CodeGrapherMS()
    graph_data = grapher.scan_directory(target_dir)
    
    print(f"\n--- Scan Complete ---")
    print(f"Nodes Found: {graph_data['node_count']}")
    print(f"Edges Built: {graph_data['edge_count']}")
    
    # Save to JSON for inspection
    out_file = "code_graph_dump.json"
    with open(out_file, "w") as f:
        json.dump(graph_data, f, indent=2)
    print(f"Graph saved to {out_file}")
--------------------------------------------------------------------------------

-------------------- FILE: _CognitiveMemoryMS\cognitive_memory.py ----------------------
import uuid
import json
import logging
import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from pydantic import BaseModel, Field

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_MEMORY_FILE = Path("working_memory.jsonl")
FLUSH_THRESHOLD = 5  # Number of turns before summarizing to Long Term Memory
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("CognitiveMem")
# ==============================================================================

class MemoryEntry(BaseModel):
    """Atomic unit of memory."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)
    role: str # 'user', 'assistant', 'system', 'tool'
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

class CognitiveMemoryMS:
    """
    The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
    flushing to Long-Term Memory (Vector Store).
    """
    def __init__(
        self, 
        persistence_path: Path = DEFAULT_MEMORY_FILE,
        summarizer_func: Optional[Callable[[str], str]] = None,
        long_term_ingest_func: Optional[Callable[[str, Dict], None]] = None
    ):
        """
        :param persistence_path: Where to save the working memory (JSONL).
        :param summarizer_func: Function to compress text (e.g., using LLM or Heuristics).
        :param long_term_ingest_func: Function to save summaries to Vector DB.
        """
        self.file_path = persistence_path
        self.summarizer = summarizer_func
        self.ingestor = long_term_ingest_func
        
        self.working_memory: List[MemoryEntry] = []
        self._load_working_memory()

    # --- Working Memory Operations ---

    def add_entry(self, role: str, content: str, metadata: Dict = None) -> MemoryEntry:
        """Adds an item to working memory and persists it."""
        entry = MemoryEntry(role=role, content=content, metadata=metadata or {})
        self.working_memory.append(entry)
        self._append_to_file(entry)
        log.info(f"Added memory: [{role}] {content[:30]}...")
        return entry

    def get_context(self, limit: int = 10) -> str:
        """
        Returns the most recent conversation history formatted for an LLM.
        """
        recent = self.working_memory[-limit:]
        return "\n".join([f"{e.role.upper()}: {e.content}" for e in recent])

    def get_full_history(self) -> List[Dict]:
        """Returns the raw list of memory objects."""
        return [e.dict() for e in self.working_memory]

    # --- Consolidation (The "Sleep" Cycle) ---

    def commit_turn(self):
        """
        Signal that a "Turn" (User + AI response) is complete.
        Checks if memory is full and triggers a flush if needed.
        """
        if len(self.working_memory) >= FLUSH_THRESHOLD:
            self._flush_to_long_term()

    def _flush_to_long_term(self):
        """
        Compresses working memory into a summary and moves it to Long-Term storage.
        """
        if not self.summarizer or not self.ingestor:
            log.warning("Flush triggered but Summarizer/Ingestor not configured. Skipping.")
            return

        log.info("ðŸŒ€ Flushing Working Memory to Long-Term Storage...")
        
        # 1. Combine Text
        full_text = "\n".join([f"{e.role}: {e.content}" for e in self.working_memory])
        
        # 2. Summarize
        try:
            summary = self.summarizer(full_text)
            log.info(f"Summary generated: {summary[:50]}...")
        except Exception as e:
            log.error(f"Summarization failed: {e}")
            return

        # 3. Ingest into Vector DB
        try:
            meta = {
                "source": "cognitive_memory_flush", 
                "date": datetime.datetime.utcnow().isoformat(),
                "original_entry_count": len(self.working_memory)
            }
            self.ingestor(summary, meta)
            log.info("âœ… Saved to Long-Term Memory.")
        except Exception as e:
            log.error(f"Ingestion failed: {e}")
            return

        # 4. Clear Working Memory (but keep file history or archive it?)
        # For this pattern, we clear the 'Active' RAM, and maybe rotate the log file.
        self.working_memory.clear()
        self._rotate_log_file()

    # --- Persistence Helpers ---

    def _load_working_memory(self):
        """Rehydrates memory from the JSONL file."""
        if not self.file_path.exists():
            return
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.working_memory.append(MemoryEntry.parse_raw(line))
            log.info(f"Loaded {len(self.working_memory)} items from {self.file_path}")
        except Exception as e:
            log.error(f"Corrupt memory file: {e}")

    def _append_to_file(self, entry: MemoryEntry):
        """Appends a single entry to the JSONL log."""
        with open(self.file_path, 'a', encoding='utf-8') as f:
            f.write(entry.json() + "\n")

    def _rotate_log_file(self):
        """Renames the current log to an archive timestamp."""
        if self.file_path.exists():
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_name = self.file_path.with_name(f"memory_archive_{timestamp}.jsonl")
            self.file_path.rename(archive_name)
            log.info(f"Rotated memory log to {archive_name}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # 1. Setup Mock Dependencies
    def mock_summarizer(text):
        return f"SUMMARY OF {len(text)} CHARS: The user and AI discussed AI architecture."

    def mock_ingest(text, metadata):
        print(f"\n[VectorDB] Indexing: '{text}'\n[VectorDB] Meta: {metadata}")

    # 2. Initialize
    print("--- Initializing Cognitive Memory ---")
    mem = CognitiveMemoryMS(
        summarizer_func=mock_summarizer,
        long_term_ingest_func=mock_ingest
    )

    # 3. Simulate Conversation
    print("\n--- Simulating Conversation ---")
    mem.add_entry("user", "Hello, who are you?")
    mem.add_entry("assistant", "I am a Cognitive Agent.")
    mem.add_entry("user", "What is your memory capacity?")
    mem.add_entry("assistant", "I have a tiered memory system.")
    mem.add_entry("user", "That sounds complex.")
    
    print(f"\nCurrent Context:\n{mem.get_context()}")

    # 4. Trigger Flush (Threshold is 5)
    print("\n--- Triggering Memory Flush ---")
    mem.commit_turn() # Should trigger flush because count is 5
    
    print(f"\nWorking Memory after flush: {len(mem.working_memory)} items")
    
    # Cleanup
    if Path("working_memory.jsonl").exists():
        os.remove("working_memory.jsonl")
    # Clean up archives if any were made
    for p in Path(".").glob("memory_archive_*.jsonl"):
        os.remove(p)
--------------------------------------------------------------------------------

-------------------- FILE: _CognitiveMemoryMS\requirements.txt -------------------------
pip install pydantic
--------------------------------------------------------------------------------

-------------------- FILE: _ContextAggregatorMS\context_aggregator.py ------------------
import os
import fnmatch
import datetime
from pathlib import Path
from typing import Set, Optional

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
# Extensions known to be binary/non-text (Images, Archives, Executables)
DEFAULT_BINARY_EXTENSIONS = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".mp4", ".mkv", ".avi", ".mov", ".webm",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".exe", ".dll", ".so",
    ".db", ".sqlite", ".mdb", ".pyc", ".pyo", ".class", ".jar", ".wasm"
}

# Folders to ignore by default
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", ".env", 
    "dist", "build", "coverage", ".idea", ".vscode"
}
# ==============================================================================

class ContextAggregatorMS:
    """
    The Context Builder: Flattens a project folder into a single readable text file.
    """

    def __init__(self, max_file_size_mb: int = 1):
        self.max_file_size_bytes = max_file_size_mb * 1024 * 1024

    def aggregate(self, 
                  root_path: str, 
                  output_file: str, 
                  extra_exclusions: Optional[Set[str]] = None,
                  use_default_exclusions: bool = True) -> int:
        
        project_root = Path(root_path).resolve()
        out_path = Path(output_file).resolve()
        
        # Build Exclusions
        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_IGNORE_DIRS)
        if extra_exclusions:
            exclusions.update(extra_exclusions)

        # Build Binary List
        binary_exts = DEFAULT_BINARY_EXTENSIONS.copy() # Always keep binaries as base unless manually cleared
        
        file_count = 0
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        try:
            with open(out_path, "w", encoding="utf-8") as out_f:
                out_f.write(f"File Dump from Project: {project_root.name}\nGenerated: {timestamp}\n{'='*60}\n\n")

                for root, dirs, files in os.walk(project_root):
                    dirs[:] = [d for d in dirs if d not in exclusions]
                    
                    for filename in files:
                        if self._should_exclude(filename, exclusions): continue

                        file_path = Path(root) / filename
                        if file_path.resolve() == out_path: continue

                        if self._is_safe_to_dump(file_path, binary_exts):
                            self._write_file_content(out_f, file_path, project_root)
                            file_count += 1
                            
        except IOError as e: print(f"Error writing dump: {e}")
        return file_count

    def _should_exclude(self, filename: str, exclusions: Set[str]) -> bool:
        return any(fnmatch.fnmatch(filename, pattern) for pattern in exclusions)

    def _is_safe_to_dump(self, file_path: Path, binary_exts: Set[str]) -> bool:
        if "".join(file_path.suffixes).lower() in binary_exts: return False
        try:
            if file_path.stat().st_size > self.max_file_size_bytes: return False
            with open(file_path, 'rb') as f:
                if b'\0' in f.read(1024): return False
        except (IOError, OSError): return False
        return True

    def _write_file_content(self, out_f, file_path: Path, project_root: Path):
        relative_path = file_path.relative_to(project_root)
        header = f"\n{'-'*20} FILE: {relative_path} {'-'*20}\n"
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as in_f:
                out_f.write(header + in_f.read() + f"\n{'-'*60}\n")
        except Exception as e:
            out_f.write(f"\n[Error reading file: {e}]\n")

if __name__ == "__main__":
    print("ContextAggregator initialized. Check top of file to tweak defaults.")
--------------------------------------------------------------------------------

-------------------- FILE: _DiffEngineMS\diff_engine.py --------------------------------
import sqlite3
import difflib
import datetime
import uuid
import logging
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Any

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("diff_engine.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("DiffEngine")
# ==============================================================================

class DiffEngineMS:
    """
    The Timekeeper: Implements a 'Hybrid' versioning architecture.
    1. HEAD: Stores full current content for fast read access (UI/RAG).
    2. HISTORY: Stores diff deltas using difflib for audit trails.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. The Head (Fast Access Cache)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id TEXT PRIMARY KEY,
                    path TEXT UNIQUE NOT NULL,
                    content TEXT,
                    last_updated TIMESTAMP
                )
            """)
            
            # 2. The Rising Edge (Diff History)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS diff_log (
                    id TEXT PRIMARY KEY,
                    file_id TEXT NOT NULL,
                    timestamp TIMESTAMP,
                    change_type TEXT,  -- 'CREATE', 'EDIT', 'DELETE'
                    diff_blob TEXT,    -- The text output of difflib
                    author TEXT,
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)

    # --- Core Workflow ---

    def update_file(self, path: str, new_content: str, author: str = "agent") -> Dict[str, Any]:
        """
        The Atomic Update Operation:
        1. Checks current state.
        2. Calculates Diff.
        3. Writes Diff to History.
        4. Updates Head to New Content.
        
        Returns: Dict with status (changed/unchanged), file_id, and diff_summary.
        """
        path = str(Path(path).as_posix()) # Normalize path
        now = datetime.datetime.utcnow()
        
        with self._get_conn() as conn:
            # 1. Fetch Head
            row = conn.execute("SELECT id, content FROM files WHERE path = ?", (path,)).fetchone()
            
            if not row:
                # --- CASE: NEW FILE ---
                file_id = str(uuid.uuid4())
                conn.execute(
                    "INSERT INTO files (id, path, content, last_updated) VALUES (?, ?, ?, ?)",
                    (file_id, path, new_content, now)
                )
                self._log_diff(conn, file_id, "CREATE", "[New File Created]", author, now)
                log.info(f"Created new file: {path}")
                return {"status": "created", "file_id": file_id}

            # --- CASE: EXISTING FILE ---
            file_id = row['id']
            old_content = row['content'] or ""
            
            # 2. Calculate Diff
            # difflib needs lists of lines
            old_lines = old_content.splitlines(keepends=True)
            new_lines = new_content.splitlines(keepends=True)
            
            # Standard unified diff
            diff_gen = difflib.unified_diff(
                old_lines, new_lines, 
                fromfile=f"a/{path}", tofile=f"b/{path}",
                lineterm=''
            )
            diff_text = "".join(diff_gen)

            if not diff_text:
                return {"status": "unchanged", "file_id": file_id}

            # 3. Write History
            self._log_diff(conn, file_id, "EDIT", diff_text, author, now)

            # 4. Update Head
            conn.execute(
                "UPDATE files SET content = ?, last_updated = ? WHERE id = ?",
                (new_content, now, file_id)
            )
            log.info(f"Updated file: {path}")
            return {"status": "updated", "file_id": file_id, "diff_size": len(diff_text)}

    def _log_diff(self, conn, file_id, change_type, diff_text, author, timestamp):
        diff_id = str(uuid.uuid4())
        conn.execute(
            "INSERT INTO diff_log (id, file_id, timestamp, change_type, diff_blob, author) VALUES (?, ?, ?, ?, ?, ?)",
            (diff_id, file_id, timestamp, change_type, diff_text, author)
        )

    # --- Retrieval ---

    def get_head(self, path: str) -> Optional[str]:
        """Fast retrieval of current content."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT content FROM files WHERE path = ?", (path,)).fetchone()
            return row['content'] if row else None

    def get_history(self, path: str) -> List[Dict]:
        """Retrieves the full evolution history of a file."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT id FROM files WHERE path = ?", (path,)).fetchone()
            if not row: return []
            
            rows = conn.execute(
                "SELECT timestamp, change_type, diff_blob, author FROM diff_log WHERE file_id = ? ORDER BY timestamp DESC",
                (row['id'],)
            ).fetchall()
            
            return [dict(r) for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    engine = DiffEngineMS()
    
    print("--- 1. Creating File ---")
    engine.update_file("notes.txt", "Todo List:\n1. Buy Milk\n")
    
    print("\n--- 2. Updating File (The Rising Edge) ---")
    # Change: Add 'Buy Eggs', Remove 'Buy Milk' (Simulating a replacement)
    new_text = "Todo List:\n1. Buy Eggs\n2. Code Python\n"
    res = engine.update_file("notes.txt", new_text, author="Jacob")
    
    print(f"Update Result: {res['status']}")
    
    print("\n--- 3. Inspecting History ---")
    history = engine.get_history("notes.txt")
    for event in history:
        print(f"\n[{event['timestamp']}] {event['change_type']} by {event['author']}")
        print(f"Diff Preview:\n{event['diff_blob'].strip()}")

    print("\n--- 4. Inspecting Head (Cache) ---")
    print(engine.get_head("notes.txt"))
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)
--------------------------------------------------------------------------------

-------------------- FILE: _ExplorerWidgetMS\explorer_widget.py ------------------------
import tkinter as tk
from tkinter import ttk
import os
import threading
import queue
import fnmatch
from pathlib import Path
from typing import Dict, Set

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
DEFAULT_EXCLUDED_FOLDERS = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "target", "out",
    "bin", "obj", "Debug", "Release", "logs"
}
# ==============================================================================

class ExplorerWidget(ttk.Frame):
    """
    A standalone file system tree viewer.
    """
    
    GLYPH_CHECKED = "[X]"
    GLYPH_UNCHECKED = "[ ]"

    def __init__(self, parent, root_path: str = ".", use_default_exclusions: bool = True, *args, **kwargs):
        super().__init__(parent, *args, **kwargs)
        
        self.root_path = Path(root_path).resolve()
        self.use_defaults = use_default_exclusions
        self.gui_queue = queue.Queue()
        self.folder_item_states: Dict[str, str] = {} 
        self.state_lock = threading.RLock()
        
        self._setup_styles()
        self._build_ui()
        self.process_gui_queue()
        self.refresh_tree()

    def _setup_styles(self):
        style = ttk.Style()
        if "clam" in style.theme_names(): style.theme_use("clam")
        style.configure("Explorer.Treeview", background="#252526", foreground="lightgray", fieldbackground="#252526", borderwidth=0, font=("Consolas", 10))
        style.map("Explorer.Treeview", background=[('selected', '#007ACC')], foreground=[('selected', 'white')])

    def _build_ui(self):
        self.columnconfigure(0, weight=1); self.rowconfigure(0, weight=1)
        self.tree = ttk.Treeview(self, show="tree", columns=("size",), selectmode="none", style="Explorer.Treeview")
        self.tree.column("size", width=80, anchor="e")
        ysb = ttk.Scrollbar(self, orient="vertical", command=self.tree.yview)
        xsb = ttk.Scrollbar(self, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=ysb.set, xscrollcommand=xsb.set)
        self.tree.grid(row=0, column=0, sticky="nsew")
        ysb.grid(row=0, column=1, sticky="ns"); xsb.grid(row=1, column=0, sticky="ew")
        self.tree.bind("<ButtonRelease-1>", self._on_click)

    def refresh_tree(self):
        for i in self.tree.get_children(): self.tree.delete(i)
        with self.state_lock:
            self.folder_item_states.clear()
            self.folder_item_states[str(self.root_path)] = "checked"
        
        tree_data = [{'parent': '', 'iid': str(self.root_path), 'text': f" {self.root_path.name} (Root)", 'open': True}]
        self._scan_recursive(self.root_path, str(self.root_path), tree_data)
        
        for item in tree_data:
            self.tree.insert(item['parent'], "end", iid=item['iid'], text=item['text'], open=item.get('open', False))
            self.tree.set(item['iid'], "size", "...")

        self._refresh_visuals(str(self.root_path))
        threading.Thread(target=self._calc_sizes_thread, args=(str(self.root_path),), daemon=True).start()

    def _scan_recursive(self, current_path: Path, parent_id: str, data_list: list):
        try:
            items = sorted(current_path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))
            for item in items:
                if not item.is_dir(): continue
                path_str = str(item.resolve())
                
                state = "checked"
                if self.use_defaults and item.name in DEFAULT_EXCLUDED_FOLDERS:
                    state = "unchecked"
                
                with self.state_lock: self.folder_item_states[path_str] = state
                data_list.append({'parent': parent_id, 'iid': path_str, 'text': f" {item.name}"})
                self._scan_recursive(item, path_str, data_list)
        except (PermissionError, OSError): pass

    def _on_click(self, event):
        item_id = self.tree.identify_row(event.y)
        if not item_id: return
        with self.state_lock:
            curr = self.folder_item_states.get(item_id, "unchecked")
            self.folder_item_states[item_id] = "checked" if curr == "unchecked" else "unchecked"
        self._refresh_visuals(str(self.root_path))

    def _refresh_visuals(self, start_node):
        def _update(node_id):
            if not self.tree.exists(node_id): return
            with self.state_lock: state = self.folder_item_states.get(node_id, "unchecked")
            glyph = self.GLYPH_CHECKED if state == "checked" else self.GLYPH_UNCHECKED
            name = Path(node_id).name
            if node_id == str(self.root_path): name += " (Root)"
            self.tree.item(node_id, text=f"{glyph} {name}")
            for child in self.tree.get_children(node_id): _update(child)
        _update(start_node)

    def _calc_sizes_thread(self, root_id):
        # (Simplified for brevity, same logic as before but uses queue)
        pass 
        # Note: In production you'd include the full calc logic here as in the previous version

    def get_selected_paths(self) -> list[str]:
        selected = []
        with self.state_lock:
            for path, state in self.folder_item_states.items():
                if state == "checked": selected.append(path)
        return selected

    def process_gui_queue(self):
        while not self.gui_queue.empty():
            try: self.gui_queue.get_nowait()()
            except queue.Empty: pass
        self.after(100, self.process_gui_queue)

if __name__ == "__main__":
    print("ExplorerWidget initialized. Check top of file to tweak defaults.")
--------------------------------------------------------------------------------

-------------------- FILE: _FingerprintScannerMS\fingerprint_scanner.py ----------------
import hashlib
import os
import logging
from pathlib import Path
from typing import Dict, Set, Optional, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Folders to ignore during the scan (Standard developer noise)
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", 
    "dist", "build", "coverage", "target", "out", "bin", "obj",
    "_project_library", "_sandbox", "_logs"
}

# Files to ignore
DEFAULT_IGNORE_FILES = {
    ".DS_Store", "Thumbs.db", "*.log", "*.tmp", "*.lock"
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("Fingerprint")
# ==============================================================================

class FingerprintScannerMS:
    """
    The Detective: Scans a directory tree and generates a deterministic
    'Fingerprint' (SHA-256 Merkle Root) representing its exact state.
    """
    
    def __init__(self):
        pass

    def scan_project(self, root_path: str) -> Dict[str, Any]:
        """
        Scans the project and returns a comprehensive state object.
        output = {
            "root": str,
            "project_fingerprint": str (The global hash),
            "file_hashes": {rel_path: sha256},
            "file_count": int
        }
        """
        root = Path(root_path).resolve()
        if not root.exists():
            raise FileNotFoundError(f"Path not found: {root}")

        file_map = {}
        
        # 1. Walk and Hash
        # Use sorted() to ensure iteration order doesn't affect the final hash
        for path in sorted(root.rglob("*")):
            if path.is_file():
                if self._should_ignore(path, root):
                    continue
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_hash = self._hash_file(path)
                
                if file_hash:
                    file_map[rel_path] = file_hash

        # 2. Calculate Merkle Root (Global Fingerprint)
        # We sort by relative path to ensure deterministic ordering
        sorted_hashes = [file_map[p] for p in sorted(file_map.keys())]
        combined_data = "".join(sorted_hashes).encode('utf-8')
        project_fingerprint = hashlib.sha256(combined_data).hexdigest()

        log.info(f"Scanned {len(file_map)} files. Fingerprint: {project_fingerprint[:8]}...")

        return {
            "root": str(root),
            "project_fingerprint": project_fingerprint,
            "file_hashes": file_map,
            "file_count": len(file_map)
        }

    def _should_ignore(self, path: Path, root: Path) -> bool:
        """Checks path against exclusion lists."""
        try:
            rel_parts = path.relative_to(root).parts
            
            # Check directories
            # If any parent directory is in the ignore list, skip
            for part in rel_parts[:-1]: 
                if part in DEFAULT_IGNORE_DIRS:
                    return True
            
            # Check filename
            import fnmatch
            name = path.name
            if name in DEFAULT_IGNORE_FILES:
                return True
            if any(fnmatch.fnmatch(name, pat) for pat in DEFAULT_IGNORE_FILES):
                return True
                
            return False
        except ValueError:
            return True

    def _hash_file(self, path: Path) -> Optional[str]:
        """Reads file bytes and returns SHA256 hash."""
        try:
            # Read binary to avoid encoding issues and to hash exact content
            content = path.read_bytes()
            return hashlib.sha256(content).hexdigest()
        except (PermissionError, OSError):
            log.warning(f"Could not read/hash: {path}")
            return None

# --- Independent Test Block ---
if __name__ == "__main__":
    import time
    
    # 1. Create a dummy project
    test_dir = Path("test_fingerprint_proj")
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
    test_dir.mkdir()
    
    (test_dir / "main.py").write_text("print('hello')")
    (test_dir / "utils.py").write_text("def add(a,b): return a+b")
    
    scanner = FingerprintScannerMS()
    
    # 2. Initial Scan
    print("--- Scan 1 (Initial) ---")
    state_1 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 1: {state_1['project_fingerprint']}")
    
    # 3. Modify a file
    print("\n--- Modifying 'main.py' ---")
    time.sleep(0.1) # Ensure filesystem timestamp tick (though we hash content)
    (test_dir / "main.py").write_text("print('hello world')")
    
    # 4. Scan again
    print("--- Scan 2 (After Modification) ---")
    state_2 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 2: {state_2['project_fingerprint']}")
    
    # 5. Compare
    if state_1['project_fingerprint'] != state_2['project_fingerprint']:
        print("\nâœ… SUCCESS: Fingerprint changed as expected.")
        # Find the diff
        for path, h in state_2['file_hashes'].items():
            if state_1['file_hashes'].get(path) != h:
                print(f"   Changed File: {path}")
    else:
        print("\nâŒ FAILURE: Fingerprint did not change.")

    # Cleanup
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
--------------------------------------------------------------------------------

-------------------- FILE: _GitPilotMS\git_pilot.py ------------------------------------
import os
import subprocess
import threading
import queue
import time
import tkinter as tk
from tkinter import ttk, messagebox, simpledialog
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple, Any, Callable

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Detect if GitHub CLI is available
def which(cmd: str) -> Optional[str]:
    for p in os.environ.get("PATH", "").split(os.pathsep):
        f = Path(p) / cmd
        if os.name == 'nt':
            for ext in (".exe", ".cmd", ".bat"): 
                if (f.with_suffix(ext)).exists(): return str(f.with_suffix(ext))
        if f.exists() and os.access(f, os.X_OK): return str(f)
    return None

USE_GH = which("gh") is not None
# ==============================================================================

@dataclass
class GitStatusEntry:
    path: str
    index: str
    workdir: str

@dataclass
class GitStatus:
    repo_path: str
    branch: Optional[str]
    ahead: int
    behind: int
    entries: List[GitStatusEntry]

# --- Backend: The Git Wrapper ---
class GitCLI:
    """
    A robust wrapper around the git command line executable.
    """
    def __init__(self, repo_path: Path):
        self.root = self._resolve_repo_root(repo_path)

    def _run(self, args: List[str], *, cwd: Optional[Path] = None) -> Tuple[str, str]:
        cmd = ["git", *args]
        # Prevent console window popping up on Windows
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

        proc = subprocess.run(
            cmd,
            cwd=str(cwd or self.root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
            startupinfo=startupinfo
        )
        if proc.returncode != 0:
            raise RuntimeError(proc.stderr.strip() or f"git {' '.join(args)} failed")
        return proc.stdout, proc.stderr

    @staticmethod
    def _resolve_repo_root(path: Path) -> Path:
        path = path.resolve()
        if (path / ".git").exists(): return path
        p = path
        while True:
            if (p / ".git").exists(): return p
            if p.parent == p: break
            p = p.parent
        return path

    def init(self) -> None:
        self._run(["init"])

    def status(self) -> GitStatus:
        try:
            out, _ = self._run(["rev-parse", "--abbrev-ref", "HEAD"])
            branch = out.strip()
        except Exception: branch = None
        
        ahead = behind = 0
        try:
            out, _ = self._run(["rev-list", "--left-right", "--count", "@{upstream}...HEAD"])
            left, right = out.strip().split()
            behind, ahead = int(left), int(right)
        except Exception: pass
        
        out, _ = self._run(["status", "--porcelain=v1"])
        entries = []
        for line in out.splitlines():
            if not line.strip(): continue
            xy = line[:2]
            path = line[3:]
            index, work = xy[0], xy[1]
            entries.append(GitStatusEntry(path=path, index=index, workdir=work))
        return GitStatus(str(self.root), branch, ahead, behind, entries)

    def stage(self, paths: List[str]) -> None:
        if paths: self._run(["add", "--"] + paths)

    def unstage(self, paths: List[str]) -> None:
        if paths: self._run(["reset", "HEAD", "--"] + paths)

    def diff(self, file: Optional[str] = None) -> str:
        args = ["diff"]
        if file: args += ["--", file]
        out, _ = self._run(args)
        return out

    def commit(self, message: str, author_name: str, author_email: str) -> str:
        env = os.environ.copy()
        if author_name: 
            env["GIT_AUTHOR_NAME"] = author_name
            env["GIT_COMMITTER_NAME"] = author_name
        if author_email:
            env["GIT_AUTHOR_EMAIL"] = author_email
            env["GIT_COMMITTER_EMAIL"] = author_email
            
        proc = subprocess.run(
            ["git", "commit", "-m", message], 
            cwd=str(self.root), 
            capture_output=True, 
            text=True, 
            env=env
        )
        if proc.returncode != 0: raise RuntimeError(proc.stderr.strip() or proc.stdout.strip())
        out, _ = self._run(["rev-parse", "HEAD"])
        return out.strip()

    def log(self, limit: int = 100) -> List[Tuple[str, str, str, int]]:
        fmt = "%H%x1f%s%x1f%an%x1f%at"
        try:
            out, _ = self._run(["log", f"-n{limit}", f"--pretty=format:{fmt}"])
            items = []
            for line in out.splitlines():
                commit, summary, author, at = line.split("\x1f")
                items.append((commit, summary, author, int(at)))
            return items
        except Exception: return []

    def branches(self) -> List[Tuple[str, bool]]:
        try:
            out, _ = self._run(["branch"])
            res = []
            for line in out.splitlines():
                is_head = line.strip().startswith("*")
                name = line.replace("*", "", 1).strip()
                res.append((name, is_head))
            return res
        except Exception: return []

    def checkout(self, name: str, create: bool = False) -> None:
        if create: self._run(["checkout", "-B", name])
        else: self._run(["checkout", name])

    def push(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        args = ["push", remote]
        if branch: args.append(branch)
        out, _ = self._run(args)
        return out

    def pull(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        if branch: out, _ = self._run(["pull", remote, branch])
        else: out, _ = self._run(["pull", remote])
        return out

# --- Threading Helper ---
class Worker:
    def __init__(self, ui_callback):
        self.q = queue.Queue()
        self.ui_callback = ui_callback
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()

    def submit(self, op: str, func, *args, **kwargs):
        self.q.put((op, func, args, kwargs))

    def _loop(self):
        while True:
            op, func, args, kwargs = self.q.get()
            try:
                result = op, True, func(*args, **kwargs)
            except Exception as e:
                result = op, False, e
            finally:
                self.ui_callback(result)

# --- Frontend: The GUI Panel ---
class GitPilotPanel(ttk.Frame):
    def __init__(self, master, initial_path: Optional[Path] = None, **kwargs):
        super().__init__(master, **kwargs)
        self.repo_path = None
        self.git = None
        self.worker = Worker(self._on_worker_done)

        self._build_ui()
        if initial_path:
            self.set_repo(initial_path)

    def set_repo(self, path: Path):
        try:
            self.git = GitCLI(path)
            self.repo_path = self.git.root
            self.path_var.set(f"Repo: {self.repo_path}")
            self._refresh()
        except Exception as e:
            self.path_var.set(f"Error: {e}")

    def _build_ui(self):
        self.columnconfigure(0, weight=1)
        self.rowconfigure(1, weight=1)

        # Status Bar
        bar = ttk.Frame(self)
        bar.grid(row=0, column=0, sticky="ew")
        self.path_var = tk.StringVar(value="No Repo Selected")
        self.busy_var = tk.StringVar()
        ttk.Label(bar, textvariable=self.path_var).pack(side="left", padx=5)
        ttk.Label(bar, textvariable=self.busy_var, foreground="blue").pack(side="right", padx=5)

        # Tabs
        self.nb = ttk.Notebook(self)
        self.nb.grid(row=1, column=0, sticky="nsew")
        
        self.tab_changes = self._build_changes_tab(self.nb)
        self.tab_log = self._build_log_tab(self.nb)
        
        self.nb.add(self.tab_changes, text="Changes")
        self.nb.add(self.tab_log, text="History")

    def _build_changes_tab(self, parent):
        frame = ttk.Frame(parent)
        paned = ttk.PanedWindow(frame, orient=tk.VERTICAL)
        paned.pack(fill="both", expand=True)

        # File List
        top = ttk.Frame(paned)
        top.rowconfigure(1, weight=1)
        top.columnconfigure(0, weight=1)
        
        # Toolbar
        tb = ttk.Frame(top)
        tb.grid(row=0, column=0, sticky="ew")
        ttk.Button(tb, text="Refresh", command=self._refresh).pack(side="left")
        ttk.Button(tb, text="Stage", command=self._stage).pack(side="left")
        ttk.Button(tb, text="Unstage", command=self._unstage).pack(side="left")
        ttk.Button(tb, text="Diff", command=self._show_diff).pack(side="left")
        ttk.Button(tb, text="Push", command=self._push).pack(side="left", padx=10)
        ttk.Button(tb, text="Pull", command=self._pull).pack(side="left")

        # Treeview
        self.tree = ttk.Treeview(top, columns=("path", "idx", "wd"), show="headings", selectmode="extended")
        self.tree.heading("path", text="Path")
        self.tree.heading("idx", text="Index")
        self.tree.heading("wd", text="Workdir")
        self.tree.column("path", width=400)
        self.tree.column("idx", width=50, anchor="center")
        self.tree.column("wd", width=50, anchor="center")
        self.tree.grid(row=1, column=0, sticky="nsew")
        
        paned.add(top, weight=3)

        # Commit Area
        bot = ttk.Frame(paned)
        bot.columnconfigure(1, weight=1)
        ttk.Label(bot, text="Message:").grid(row=0, column=0, sticky="nw")
        self.msg_text = tk.Text(bot, height=4)
        self.msg_text.grid(row=0, column=1, sticky="nsew")
        ttk.Button(bot, text="Commit", command=self._commit).grid(row=1, column=1, sticky="e", pady=5)
        
        paned.add(bot, weight=1)
        return frame

    def _build_log_tab(self, parent):
        frame = ttk.Frame(parent)
        self.log_tree = ttk.Treeview(frame, columns=("sha", "msg", "auth", "time"), show="headings")
        self.log_tree.heading("sha", text="SHA")
        self.log_tree.heading("msg", text="Message")
        self.log_tree.heading("auth", text="Author")
        self.log_tree.heading("time", text="Time")
        self.log_tree.column("sha", width=80)
        self.log_tree.column("msg", width=400)
        self.log_tree.pack(fill="both", expand=True)
        return frame

    # --- Actions ---

    def _submit(self, label, func, *args):
        self.busy_var.set(f"{label}...")
        self.worker.submit(label, func, *args)

    def _on_worker_done(self, result):
        self.after(0, self._handle_result, result)

    def _handle_result(self, result):
        label, ok, data = result
        self.busy_var.set("")
        if not ok:
            messagebox.showerror("Error", str(data))
            return
        
        if label == "refresh":
            status, logs = data
            self.tree.delete(*self.tree.get_children())
            for e in status.entries:
                self.tree.insert("", "end", values=(e.path, e.index, e.workdir))
            
            self.log_tree.delete(*self.log_tree.get_children())
            for sha, msg, auth, ts in logs:
                t_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(ts))
                self.log_tree.insert("", "end", values=(sha[:7], msg, auth, t_str))
        
        if label == "diff":
            top = tk.Toplevel(self)
            top.title("Diff")
            txt = tk.Text(top, font=("Consolas", 10))
            txt.pack(fill="both", expand=True)
            txt.insert("1.0", data)

        if label in ["stage", "unstage", "commit", "push", "pull"]:
            self._refresh()

    def _refresh(self):
        if not self.git: return
        self._submit("refresh", lambda: (self.git.status(), self.git.log()))

    def _get_selection(self):
        return [self.tree.item(i)['values'][0] for i in self.tree.selection()]

    def _stage(self):
        paths = self._get_selection()
        if paths: self._submit("stage", self.git.stage, paths)

    def _unstage(self):
        paths = self._get_selection()
        if paths: self._submit("unstage", self.git.unstage, paths)

    def _commit(self):
        msg = self.msg_text.get("1.0", "end").strip()
        if not msg: return
        self._submit("commit", self.git.commit, msg, "GitPilot", "pilot@local")
        self.msg_text.delete("1.0", "end")

    def _push(self):
        self._submit("push", self.git.push)

    def _pull(self):
        self._submit("pull", self.git.pull)

    def _show_diff(self):
        sel = self._get_selection()
        file = sel[0] if sel else None
        self._submit("diff", self.git.diff, file)

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Git Pilot Test")
    root.geometry("800x600")
    
    # Use current directory
    cwd = Path(os.getcwd())
    
    panel = GitPilotPanel(root, initial_path=cwd)
    panel.pack(fill="both", expand=True)
    
    root.mainloop()
--------------------------------------------------------------------------------

-------------------- FILE: _GraphEngineMS\graph_engine.py ------------------------------
import pygame
import math
import random

# Initialize font module globally once
pygame.font.init()

class GraphRenderer:
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        self.width = width
        self.height = height
        self.bg_color = bg_color
        
        # Surface for drawing
        self.surface = pygame.Surface((width, height))
        
        # Camera State
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction State
        self.dragged_node_idx = None
        self.hovered_node_idx = None

    def resize(self, width, height):
        """Re-initialize surface on window resize"""
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    def set_data(self, nodes, links):
        """
        Expects nodes to have: id, type ('file'|'concept'), label
        Expects links to be tuples of indices: (source_idx, target_idx)
        """
        self.nodes = nodes
        self.links = links
        
        # Initialize physics state for new nodes
        for n in self.nodes:
            if 'x' not in n:
                n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
                n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Cache visual properties based on type
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # #007ACC (Blue)
                n['_radius'] = 6
            else:
                n['_color'] = (160, 32, 240) # #A020F0 (Purple)
                n['_radius'] = 8

    # --- INPUT HANDLING (Coordinate Transforms) ---
    
    def screen_to_world(self, sx, sy):
        """Convert Tkinter screen coordinates to Physics world coordinates"""
        # (Screen - Center) / Zoom + Center - Camera
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        # Find clicked node
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2: # Generous hit box
                self.dragged_node_idx = i
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            # Move the node directly
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
        else:
            # Hover check
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            
            return prev_hover != self.hovered_node_idx # Return True if redraw needed

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        # Zoom towards mouse pointer logic could go here
        # For now, simple center zoom
        old_zoom = self.zoom
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    # --- PHYSICS ---

    def step_physics(self):
        if not self.nodes: return

        # Constants matching D3 feel
        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.9
        
        cx, cy = self.width / 2, self.height / 2

        # 1. Repulsion (Nodes push apart)
        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue # Don't move dragged node
            
            fx, fy = 0, 0
            
            # Center Gravity (Pull lightly to middle so they don't drift away)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # Node-Node Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Force = k / dist^2
                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 2. Attraction (Links pull together)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            
            # Spring force
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 3. Apply Velocity
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']

    # --- RENDERING ---

    def get_image_bytes(self):
        """ Renders the scene and returns raw RGB bytes + size """
        self.surface.fill(self.bg_color)
        
        # Pre-calculate center offset
        cx, cy = self.width / 2, self.height / 2
        
        # Helper for transforms
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # 1. Draw Links
        for u, v in self.links:
            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # 2. Draw Nodes
        for i, n in enumerate(self.nodes):
            sx, sy = to_screen(n['x'], n['y'])
            
            # Culling: Don't draw if off screen
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20:
                continue
                
            rad = int(n['_radius'] * self.zoom)
            col = n['_color']
            
            # Highlight hovered
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)
            
            pygame.draw.circle(self.surface, col, (sx, sy), rad)
            
            # Draw Labels (only if zoomed in enough or hovered)
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

        return pygame.image.tostring(self.surface, 'RGB')
--------------------------------------------------------------------------------

-------------------- FILE: _GraphEngineMS\graph_view.py --------------------------------
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import os

# Use relative import for sibling module
from .graph_engine import GraphRenderer

class GraphView(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        self.pack(fill="both", expand=True)
        
        # Widget logic
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)
        
        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None # Keep reference to avoid GC
        
        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)
        # Zoom bindings
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1)) # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9)) # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)
        
        # Logic State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False
        
        # Start Loop
        self.animate()

    def load_from_db(self, db_path):
        """
        Fetches Nodes and Edges from the SQLite DB and formats them 
        for the Pygame Physics Engine.
        """
        if not os.path.exists(db_path):
            print(f"GraphView Error: DB not found at {db_path}")
            return

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # 1. Fetch Nodes
        # Schema: id, type, label, data_json
        try:
            db_nodes = cursor.execute("SELECT id, type, label FROM graph_nodes").fetchall()
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
        except sqlite3.OperationalError:
            print("Graph tables missing. Run Ingest first.")
            conn.close()
            return

        conn.close()

        # 2. Map String IDs -> Integer Indices
        # The physics engine uses list indices (0, 1, 2) for speed.
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label = row
            id_to_index[node_id] = idx
            formatted_nodes.append({
                'id': node_id,
                'type': n_type,
                'label': label
            })

        # 3. Translate Edges
        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                u = id_to_index[src]
                v = id_to_index[tgt]
                formatted_links.append((u, v))

        # 4. Push to Engine
        print(f"Graph Loaded: {len(formatted_nodes)} Nodes, {len(formatted_links)} Edges")
        self.engine.set_data(formatted_nodes, formatted_links)

    # --- EVENT HANDLERS ---
    
    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        self.is_dragging_node = hit

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        else:
            # Pan Camera
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        # Just update hover state for aesthetics
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)

    def on_windows_scroll(self, event):
        # Windows typically gives 120 or -120
        if event.delta > 0:
            self.on_zoom(1.1)
        else:
            self.on_zoom(0.9)

    # --- RENDER LOOP ---

    def animate(self):
        # 1. Step Physics
        self.engine.step_
--------------------------------------------------------------------------------

-------------------- FILE: _HeruisticSumMS\heuristic_summarizer.py ---------------------
import re
import os
from typing import List, Optional

# ==============================================================================
# CONFIGURATION: REGEX PATTERNS
# ==============================================================================
# Captures: def my_func, class MyClass, function myFunc, interface MyInterface
SIG_RE = re.compile(r'^\s*(def|class|function|interface|struct|impl|func)\s+([A-Za-z_][A-Za-z0-9_]*)')

# Captures: # Heading, ## Subheading
MD_HDR_RE = re.compile(r'^\s{0,3}(#{1,3})\s+(.+)')

# Captures: """ Docstring """ or ''' Docstring ''' (Start of block)
DOC_RE = re.compile(r'^\s*("{3}|\'{3})(.*)', re.DOTALL)
# ==============================================================================

class HeuristicSumMS:
    """
    The Skimmer: Generates quick summaries of code/text files without AI.
    Scans for high-value lines (headers, signatures, docstrings) and concatenates them.
    """

    def summarize(self, text: str, filename: str = "", max_chars: int = 480) -> str:
        """
        Generates a summary string from the provided text.
        """
        lines = text.splitlines()
        picks = []

        # 1. Scan top 20 lines for Markdown Headers
        for ln in lines[:20]:
            m = MD_HDR_RE.match(ln)
            if m:
                picks.append(f"Heading: {m.group(2).strip()}")

        # 2. Scan top 40 lines for Code Signatures (Functions/Classes)
        for ln in lines[:40]:
            m = SIG_RE.match(ln)
            if m:
                picks.append(f"{m.group(1)} {m.group(2)}")

        # 3. Check for Docstrings / Preamble
        if lines:
            # Join first 80 lines to check for multi-line docstrings
            joined = "\n".join(lines[:80])
            m = DOC_RE.match(joined)
            if m:
                # Grab the first few lines of the docstring content
                after = joined.splitlines()[1:3]
                if after:
                    clean_doc = " ".join(s.strip() for s in after).strip()
                    picks.append(f"Doc: {clean_doc}")

        # 4. Fallback: First non-empty line if nothing else found
        if not picks:
            head = " ".join(l.strip() for l in lines[:2] if l.strip())
            if head:
                picks.append(head)

        # 5. Add Filename Context
        if filename:
            picks.append(f"[{os.path.basename(filename)}]")

        # 6. Deduplicate and Format
        seen = set()
        uniq = []
        for p in picks:
            if p and p not in seen:
                uniq.append(p)
                seen.add(p)

        summary = " | ".join(uniq)
        
        # 7. Truncate
        if len(summary) > max_chars:
            summary = summary[:max_chars-3] + "..."
            
        return summary.strip() if summary else "[No summary available]"

# --- Independent Test Block ---
if __name__ == "__main__":
    skimmer = HeuristicSumMS()
    
    # Test 1: Python Code
    py_code = """
    class DataProcessor:
        '''
        Handles the transformation of raw input data into structured formats.
        '''
        def process(self, data):
            pass
    """
    print(f"Python Summary: {skimmer.summarize(py_code, 'processor.py')}")

    # Test 2: Markdown
    md_text = """
    # Project Roadmap
    ## Phase 1
    We begin with ingestion.
    """
    print(f"Markdown Summary: {skimmer.summarize(md_text, 'README.md')}")
--------------------------------------------------------------------------------

-------------------- FILE: _IngestEngineMS\ingest_engine.py ----------------------------
import os
import time
import re
import sqlite3
import requests
import json
from typing import List, Generator, Dict, Any, Optional, Set
from dataclasses import dataclass

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

@dataclass
class IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

class SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    Used to generate the 'DEPENDS_ON' edges in the Knowledge Graph.
    """
    def __init__(self):
        # Python: "from x import y", "import x"
        self.py_pattern = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')
        # JS/TS: "import ... from 'x'", "require('x')"
        self.js_pattern = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        ext = os.path.splitext(file_path)[1].lower()
        
        lines = content.split('\n')
        for line in lines:
            match = None
            if ext == '.py':
                match = self.py_pattern.match(line)
            elif ext in ['.js', '.ts', '.tsx', '.jsx']:
                match = self.js_pattern.search(line)
            
            if match:
                # Clean up the module name (e.g., "backend.database" -> "database")
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        
        return dependencies

class IngestEngine:
    """
    The Heavy Lifter: Reads files, chunks text, fetches embeddings,
    populates the Graph Nodes, and weaves Graph Edges.
    """
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.stop_signal = False
        self.weaver = SynapseWeaver()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags")
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    def process_files(self, file_paths: List[str], model_name: str = "none") -> Generator[IngestStatus, None, None]:
        total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Optimization settings
        cursor.execute("PRAGMA synchronous = OFF")
        cursor.execute("PRAGMA journal_mode = MEMORY")

        # Memory for graph weaving (Node Name -> Node ID)
        node_registry = {}
        file_contents = {} # Cache content for the weaving pass

        # --- PHASE 1: INGESTION (Files, Chunks, Nodes) ---
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, "Ingestion Aborted.")
                break

            filename = os.path.basename(file_path)

            # 1. Read
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                file_contents[filename] = content # Cache for Phase 2
            except Exception as e:
                yield IngestStatus(file_path, (idx/total)*100, idx, total, f"Error: {e}")
                continue

            # 2. Track File
            try:
                cursor.execute("INSERT OR REPLACE INTO files (path, last_updated) VALUES (?, ?)", 
                              (file_path, time.time()))
                file_id = cursor.lastrowid
            except sqlite3.Error:
                continue

            # 3. Create Graph Node (for Visualization)
            # We use the filename as the unique ID for the graph to make linking easier
            cursor.execute("""
                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                VALUES (?, ?, ?, ?)
            """, (filename, 'file', filename, json.dumps({"path": file_path})))
            
            node_registry[filename] = filename

            # 4. Chunking & Embedding
            chunks = self._chunk_text(content)
            
            for i, chunk_text in enumerate(chunks):
                if self.stop_signal: break
                
                embedding = None
                if model_name != "none":
                    embedding = self._get_embedding(model_name, chunk_text)
                
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                
                cursor.execute("""
                    INSERT INTO chunks (file_id, chunk_index, content, embedding)
                    VALUES (?, ?, ?, ?)
                """, (file_id, i, chunk_text, emb_blob))

                # Visual Feedback
                thought_frame = {
                    "id": f"{file_id}_{i}",
                    "file": filename,
                    "chunk_index": i,
                    "content": chunk_text,
                    "vector_preview": embedding[:20] if embedding else [],
                    "concept_color": "#007ACC"
                }
                
                yield IngestStatus(
                    current_file=filename,
                    progress_percent=((idx + (i/len(chunks))) / total) * 100,
                    processed_files=idx,
                    total_files=total,
                    log_message=f"Processing {filename}...",
                    thought_frame=thought_frame
                )

            # Checkpoint per file
            conn.commit()

        # --- PHASE 2: WEAVING (Edges) ---
        yield IngestStatus("Graph", 100, total, total, "Weaving Knowledge Graph...")
        
        edge_count = 0
        for filename, content in file_contents.items():
            if self.stop_signal: break
            
            # Find imports
            deps = self.weaver.extract_dependencies(content, filename)
            
            for dep in deps:
                # Naive matching: if 'database' is imported, look for 'database.py' or 'database.ts'
                # in our registry.
                target_id = None
                for potential_match in node_registry.keys():
                    if potential_match.startswith(dep + '.') or potential_match == dep:
                        target_id = potential_match
                        break
                
                if target_id and target_id != filename:
                    try:
                        cursor.execute("""
                            INSERT OR IGNORE INTO graph_edges (source, target, weight)
                            VALUES (?, ?, 1.0)
                        """, (filename, target_id))
                        edge_count += 1
                    except:
                        pass

        conn.commit()
        conn.close()

        yield IngestStatus(
            current_file="Complete",
            progress_percent=100,
            processed_files=total,
            total_files=total,
            log_message=f"Ingestion Complete. Created {edge_count} dependency edges."
        )

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        if len(text) < chunk_size: return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += (chunk_size - overlap)
        return chunks

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": model, "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
        return None

# --- Independent Test Block ---
if __name__ == "__main__":
    TEST_DB = "test_ingest_v2.db"
    
    # Init DB Schema manually for test
    conn = sqlite3.connect(TEST_DB)
    conn.execute("CREATE TABLE IF NOT EXISTS files (id INTEGER PRIMARY KEY, path TEXT, last_updated REAL)")
    conn.execute("CREATE TABLE IF NOT EXISTS chunks (id INTEGER PRIMARY KEY, file_id INT, chunk_index INT, content TEXT, embedding BLOB)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, weight REAL)")
    conn.close()

    engine = IngestEngine(TEST_DB)
    # Self-ingest to test dependency parsing
    files = ["_IngestEngineMS.py"] 
    
    print("Running Ingest V2...")
    for status in engine.process_files(files, "none"):
        print(f"[{status.progress_percent:.0f}%] {status.log_message}")
    
    # Verify Edges
    conn = sqlite3.connect(TEST_DB)
    edges = conn.execute("SELECT * FROM graph_edges").fetchall()
    nodes = conn.execute("SELECT * FROM graph_nodes").fetchall()
    print(f"\nResult: {len(nodes)} Nodes, {len(edges)} Edges.")
    conn.close()
    
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)
--------------------------------------------------------------------------------

-------------------- FILE: _IsoProcessMS\iso_process.py --------------------------------
import multiprocessing as mp
import logging
import logging.handlers
import time
import queue
from typing import Any, Dict, Optional

# ==============================================================================
# WORKER LOGIC (Runs in Child Process)
# ==============================================================================
def _isolated_worker(result_queue: mp.Queue, log_queue: mp.Queue, payload: Any, config: Dict[str, Any]):
    """
    Entry point for the child process.
    Configures a logging handler to send records back to the parent.
    """
    # 1. Setup Logging Bridge
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    # Clear default handlers to avoid duplicate prints in child
    for h in root.handlers[:]:
        root.removeHandler(h)
    
    # Send all logs to the parent via the queue
    qh = logging.handlers.QueueHandler(log_queue)
    root.addHandler(qh)
    
    log = logging.getLogger("IsoWorker")

    try:
        log.info(f"Worker PID {mp.current_process().pid} started.")
        
        # --- 2. Heavy Imports (Simulated) ---
        log.info("Loading heavy libraries (Torch/Transformers)...")
        # from transformers import pipeline
        time.sleep(0.2) # Simulate import time

        # --- 3. The Logic ---
        model_name = config.get("model_name", "default-model")
        log.info(f"Initializing model '{model_name}'...")
        
        # Simulate processing steps with progress reporting
        for i in range(1, 4):
            time.sleep(0.3)
            log.info(f"Processing chunk {i}/3...")
        
        processed_data = f"Processed({payload}) via {model_name}"
        
        # --- 4. Return Result ---
        log.info("Work complete. Returning result.")
        result_queue.put({"success": True, "data": processed_data})

    except Exception as e:
        log.exception("Critical failure in worker process.")
        result_queue.put({"success": False, "error": str(e)})

# ==============================================================================
# PARENT CONTROLLER (Runs in Main Process)
# ==============================================================================
class IsoProcessMS:
    """
    The Safety Valve: Spawns isolated processes with real-time logging feedback.
    """
    def __init__(self, timeout_seconds: int = 60):
        self.timeout = timeout_seconds
        
        # Setup main logger
        self.log = logging.getLogger("IsoParent")
        if not self.log.handlers:
            logging.basicConfig(
                level=logging.INFO, 
                format='%(asctime)s [%(name)s] %(message)s',
                datefmt='%H:%M:%S'
            )

    def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
        config = config or {}
        
        # 1. Setup Queues
        ctx = mp.get_context("spawn")
        result_queue = ctx.Queue()
        log_queue = ctx.Queue()

        # 2. Setup Log Listener (The "Ear" of the parent)
        # This thread pulls logs from the queue and handles them in the main process
        listener = logging.handlers.QueueListener(log_queue, *logging.getLogger().handlers)
        listener.start()

        # 3. Launch Process
        process = ctx.Process(
            target=_isolated_worker,
            args=(result_queue, log_queue, payload, config)
        )
        
        self.log.info("ðŸš€ Spawning isolated process...")
        process.start()
        
        try:
            # 4. Wait for Result
            result_packet = result_queue.get(timeout=self.timeout)
            process.join()

            if result_packet["success"]:
                return result_packet["data"]
            else:
                raise RuntimeError(f"Worker Error: {result_packet['error']}")

        except queue.Empty:
            self.log.error("â³ Worker timed out! Terminating...")
            process.terminate()
            process.join()
            raise TimeoutError(f"Task exceeded {self.timeout}s limit.")
            
        finally:
            # Clean up the log listener so it doesn't hang
            listener.stop()

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing IsoProcessMS with Live Logging ---")
    iso = IsoProcessMS(timeout_seconds=5)
    
    try:
        result = iso.execute("Sensitive Data", {"model_name": "DeepSeek-V3"})
        print(f"\n[Parent] Final Result: {result}")
    except Exception as e:
        print(f"\n[Parent] Failed: {e}")
--------------------------------------------------------------------------------

-------------------- FILE: _LexicalSearchMS\lexical_search.py --------------------------
import sqlite3
import json
import os
from typing import List, Dict, Any, Optional

class LexicalSearchMS:
    """
    The Librarian's Index: A lightweight, AI-free search engine.
    
    Uses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).
    Ideal for environments where installing PyTorch/Transformers is impossible
    or overkill.
    """

    def __init__(self, db_path: str = "lexical_index.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """
        Sets up the schema. 
        Uses Triggers to automatically keep the FTS index in sync with the main table.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # 1. Main Content Table (Stores the actual data)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                metadata TEXT  -- JSON blob for extra info (path, author, etc)
            );
        """)
        
        # 2. Virtual FTS Table (The Search Index)
        # content='documents' means it references the table above (saves space)
        cur.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                content,
                content='documents',
                content_rowid='rowid'  -- Internal SQLite mapping
            );
        """)

        # 3. Triggers (The "Magic" - Auto-sync index on Insert/Delete/Update)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ai AFTER INSERT ON documents BEGIN
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ad AFTER DELETE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_au AFTER UPDATE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        
        conn.commit()
        conn.close()

    def add_document(self, doc_id: str, text: str, metadata: Dict[str, Any] = None):
        """
        Adds or updates a document in the index.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        meta_json = json.dumps(metadata or {})
        
        # Upsert logic (Replace if ID exists)
        cur.execute("""
            INSERT OR REPLACE INTO documents (id, content, metadata)
            VALUES (?, ?, ?)
        """, (doc_id, text, meta_json))
        
        conn.commit()
        conn.close()

    def search(self, query: str, top_k: int = 20) -> List[Dict]:
        """
        Performs a BM25 Ranked Search.
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row # Allows dict-like access
        cur = conn.cursor()
        
        try:
            # The SQL Magic: 'bm25(documents_fts)' calculates relevance score
            sql = """
                SELECT 
                    d.id, 
                    d.content, 
                    d.metadata,
                    snippet(documents_fts, 0, '<b>', '</b>', '...', 15) as preview,
                    bm25(documents_fts) as score
                FROM documents_fts 
                JOIN documents d ON d.rowid = documents_fts.rowid
                WHERE documents_fts MATCH ? 
                ORDER BY score ASC
                LIMIT ?
            """
            # FTS5 query syntax: quotes typically help with special chars
            safe_query = f'"{query}"'
            rows = cur.execute(sql, (safe_query, top_k)).fetchall()
            
            results = []
            for r in rows:
                results.append({
                    "id": r['id'],
                    "score": round(r['score'], 4),
                    "preview": r['preview'], # FTS5 auto-generates snippets!
                    "metadata": json.loads(r['metadata']),
                    "full_content": r['content']
                })
            
            return results
            
        except sqlite3.OperationalError as e:
            # Usually happens if query syntax is bad (e.g. unmatched quotes)
            print(f"Search syntax error: {e}")
            return []
        finally:
            conn.close()

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    db_name = "test_lexical.db"
    
    # 1. Init
    engine = LexicalSearchMS(db_name)
    
    # 2. Ingest Data
    print("Ingesting test data...")
    engine.add_document("doc1", "Python is a great language for data science.", {"category": "coding"})
    engine.add_document("doc2", "The snake python is a reptile found in jungles.", {"category": "biology"})
    engine.add_document("doc3", "Data science involves python, pandas, and SQL.", {"category": "coding"})
    
    # 3. Search
    query = "python data"
    print(f"\nSearching for: '{query}'")
    hits = engine.search(query)
    
    for hit in hits:
        print(f"[{hit['score']:.4f}] {hit['id']} ({hit['metadata']['category']})")
        print(f"   Preview: {hit['preview']}")
        
    # Cleanup
    if os.path.exists(db_name):
        os.remove(db_name)
--------------------------------------------------------------------------------

-------------------- FILE: _LibrarianServiceMS\librarian_service.py --------------------
import os
import shutil
import sqlite3
import time
from pathlib import Path
from typing import List, Dict, Optional

class LibrarianMS:
    """
    The Librarian: Manages the physical creation, deletion, and listing
    of Knowledge Base (KB) files.
    """
    
    def __init__(self, storage_dir: str = "./cortex_dbs"):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)

    def list_kbs(self) -> List[str]:
        """
        Scans the storage directory for .db files.
        Equivalent to api.listKBs() in Sidebar.tsx.
        """
        if not self.storage_dir.exists():
            return []
        
        # Return simple filenames sorted by modification time (newest first)
        files = list(self.storage_dir.glob("*.db"))
        files.sort(key=os.path.getmtime, reverse=True)
        return [f.name for f in files]

    def create_kb(self, name: str) -> Dict[str, str]:
        """
        Creates a new SQLite database and initializes the Cortex Schema.
        """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            raise FileExistsError(f"Knowledge Base '{safe_name}' already exists.")

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # --- THE CORTEX SCHEMA ---
            # 1. System Config: Stores version and global metadata
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS config (
                    key TEXT PRIMARY KEY,
                    value TEXT
                )
            """)
            
            # 2. Files: Tracks scanned files to avoid re-ingesting unchanged ones
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    path TEXT UNIQUE NOT NULL,
                    checksum TEXT,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'indexed'
                )
            """)
            
            # 3. Chunks: The actual atomic units of knowledge
            # Note: 'embedding' is stored as a BLOB (bytes) for raw vector data
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS chunks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_id INTEGER,
                    chunk_index INTEGER,
                    content TEXT,
                    embedding BLOB, 
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)
            
            # 4. Graph Nodes: For the GraphView visualization
            # Distinguishes between 'file' nodes and 'concept' nodes
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_nodes (
                    id TEXT PRIMARY KEY,
                    type TEXT,  -- 'file' or 'concept'
                    label TEXT,
                    data_json TEXT -- Flexible JSON for positions/colors
                )
            """)
            
            # 5. Graph Edges: The connections
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_edges (
                    source TEXT,
                    target TEXT,
                    weight REAL DEFAULT 1.0,
                    FOREIGN KEY(source) REFERENCES graph_nodes(id),
                    FOREIGN KEY(target) REFERENCES graph_nodes(id)
                )
            """)
            
            # Timestamp creation
            cursor.execute("INSERT INTO config (key, value) VALUES (?, ?)", 
                           ("created_at", str(time.time())))
            
            conn.commit()
            conn.close()
            return {"status": "success", "path": str(db_path), "name": safe_name}
            
        except Exception as e:
            # Cleanup on failure
            if db_path.exists():
                os.remove(db_path)
            raise e

    def delete_kb(self, name: str) -> bool:
        """
        Physically removes the database file.
        """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            os.remove(db_path)
            return True
        return False

    def duplicate_kb(self, source_name: str) -> Dict[str, str]:
        """
        Creates a copy of an existing KB.
        """
        safe_source = self._sanitize_name(source_name)
        source_path = self.storage_dir / safe_source
        
        if not source_path.exists():
            raise FileNotFoundError(f"Source KB '{safe_source}' not found.")
            
        # Generate new name
        base = safe_source.replace('.db', '')
        new_name = f"{base}_copy.db"
        dest_path = self.storage_dir / new_name
        
        # Handle collision if copy already exists
        counter = 1
        while dest_path.exists():
            new_name = f"{base}_copy_{counter}.db"
            dest_path = self.storage_dir / new_name
            counter += 1
            
        shutil.copy2(source_path, dest_path)
        return {"status": "success", "name": new_name}

    def _sanitize_name(self, name: str) -> str:
        """Ensures the filename ends in .db and has no illegal chars."""
        clean = "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).strip()
        clean = clean.replace(' ', '_')
        if not clean.endswith('.db'):
            clean += '.db'
        return clean

# --- Independent Test Block ---
if __name__ == "__main__":
    print("Initializing Librarian Service...")
    lib = LibrarianMS("./test_brains")
    
    # 1. Create
    print("Creating 'Project_Alpha'...")
    try:
        lib.create_kb("Project Alpha")
    except FileExistsError:
        print("Project Alpha already exists.")
        
    # 2. List
    kbs = lib.list_kbs()
    print(f"Available Brains: {kbs}")
    
    # 3. Duplicate
    if "Project_Alpha.db" in kbs:
        print("Duplicating Alpha...")
        lib.duplicate_kb("Project_Alpha.db")
        
    # 4. Final List
    print(f"Final Brains: {lib.list_kbs()}")
--------------------------------------------------------------------------------

-------------------- FILE: _LogViewMS\log_view.py --------------------------------------
import tkinter as tk
from tkinter import scrolledtext, filedialog
import queue
import logging
import datetime

class QueueHandler(logging.Handler):
    """Sends log records to a Tkinter-safe queue."""
    def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.log_queue.put(record)

class LogViewMS(tk.Frame):
    """
    The Console: A professional log viewer widget.
    Features:
    - Thread-safe (consumes from a Queue).
    - Message Consolidation ("Error occurred (x5)").
    - Level Filtering (Toggle INFO/DEBUG/ERROR).
    """
    def __init__(self, parent, log_queue: queue.Queue, **kwargs):
        super().__init__(parent, **kwargs)
        self.log_queue = log_queue
        
        # State for consolidation
        self.last_msg = None
        self.last_count = 0
        self.last_line_index = None
        
        self._build_ui()
        self._poll_queue()

    def _build_ui(self):
        # Toolbar
        toolbar = tk.Frame(self, bg="#2d2d2d", height=30)
        toolbar.pack(fill="x", side="top")
        
        # Filters
        self.filters = {
            "INFO": tk.BooleanVar(value=True),
            "DEBUG": tk.BooleanVar(value=True),
            "WARNING": tk.BooleanVar(value=True),
            "ERROR": tk.BooleanVar(value=True)
        }
        
        for level, var in self.filters.items():
            cb = tk.Checkbutton(
                toolbar, text=level, variable=var, 
                bg="#2d2d2d", fg="white", selectcolor="#444",
                activebackground="#2d2d2d", activeforeground="white"
            )
            cb.pack(side="left", padx=5)

        tk.Button(toolbar, text="Clear", command=self.clear, bg="#444", fg="white", relief="flat").pack(side="right", padx=5)
        tk.Button(toolbar, text="Save", command=self.save, bg="#444", fg="white", relief="flat").pack(side="right")

        # Text Area
        self.text = scrolledtext.ScrolledText(
            self, state="disabled", bg="#1e1e1e", fg="#d4d4d4", 
            font=("Consolas", 10), insertbackground="white"
        )
        self.text.pack(fill="both", expand=True)
        
        # Color Tags
        self.text.tag_config("INFO", foreground="#d4d4d4")
        self.text.tag_config("DEBUG", foreground="#569cd6")
        self.text.tag_config("WARNING", foreground="#ce9178")
        self.text.tag_config("ERROR", foreground="#f44747")
        self.text.tag_config("timestamp", foreground="#608b4e")

    def _poll_queue(self):
        """Pulls logs from the queue and updates UI."""
        try:
            while True:
                record = self.log_queue.get_nowait()
                self._display(record)
        except queue.Empty:
            pass
        finally:
            self.after(100, self._poll_queue)

    def _display(self, record):
        level = record.levelname
        if not self.filters.get(level, tk.BooleanVar(value=True)).get():
            return

        msg = record.getMessage()
        ts = datetime.datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        
        self.text.config(state="normal")
        
        # Consolidation Logic
        if msg == self.last_msg:
            self.last_count += 1
            # Delete previous line content (keep timestamp)
            # This is complex in Tk text, simplified approach:
            # We just append (xN) if we can, otherwise standard print
            # For simplicity in this microservice, we will just append standard lines 
            # to ensure stability, or implement simple dedup:
            pass 
        else:
            self.last_msg = msg
            self.last_count = 1
        
        self.text.insert("end", f"[{ts}] ", "timestamp")
        self.text.insert("end", f"{msg}\n", level)
        self.text.see("end")
        self.text.config(state="disabled")

    def clear(self):
        self.text.config(state="normal")
        self.text.delete("1.0", "end")
        self.text.config(state="disabled")

    def save(self):
        path = filedialog.asksaveasfilename(defaultextension=".log", filetypes=[("Log Files", "*.log")])
        if path:
            try:
                with open(path, "w", encoding="utf-8") as f:
                    f.write(self.text.get("1.0", "end"))
            except Exception as e:
                print(f"Save failed: {e}")

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Log View Test")
    root.geometry("600x400")
    
    # 1. Setup Queue
    q = queue.Queue()
    
    # 2. Setup Logger
    logger = logging.getLogger("TestApp")
    logger.setLevel(logging.DEBUG)
    logger.addHandler(QueueHandler(q))
    
    # 3. Mount View
    log_view = LogViewMS(root, q)
    log_view.pack(fill="both", expand=True)
    
    # 4. Generate Logs
    def generate_noise():
        logger.info("System initializing...")
        logger.debug("Checking sensors...")
        logger.warning("Sensor 4 response slow.")
        logger.error("Connection failed!")
        root.after(2000, generate_noise)
        
    generate_noise()
    root.mainloop()
--------------------------------------------------------------------------------

-------------------- FILE: _MonacoHostMS\editor.html -----------------------------------
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Monaco Host</title>
    <style>
        html, body { margin: 0; padding: 0; width: 100%; height: 100%; overflow: hidden; background-color: #1e1e1e; font-family: sans-serif; }
        #container { display: flex; flex-direction: column; height: 100%; }
        #tabs { background: #252526; display: flex; overflow-x: auto; height: 35px; }
        .tab { 
            padding: 8px 15px; color: #969696; background: #2d2d2d; cursor: pointer; border-right: 1px solid #1e1e1e; font-size: 13px;
            display: flex; align-items: center;
        }
        .tab.active { background: #1e1e1e; color: #fff; }
        .tab:hover { background: #2d2d2d; color: #fff; }
        #editor { flex-grow: 1; }
    </style>
</head>
<body>
    <div id="container">
        <div id="tabs"></div>
        <div id="editor"></div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs/loader.js"></script>
    <script>
        require.config({ paths: { 'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs' }});

        let editor;
        let models = {}; // filepath -> model
        let currentPath = null;

        require(['vs/editor/editor.main'], function() {
            editor = monaco.editor.create(document.getElementById('editor'), {
                value: "# Ready.\n",
                language: 'python',
                theme: 'vs-dark',
                automaticLayout: true
            });

            // Signal Python that we are ready
            if (window.pywebview) window.pywebview.api.signal_editor_ready();

            // Keybindings
            editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.KeyS, function() {
                if (currentPath) {
                    const content = editor.getValue();
                    window.pywebview.api.save_file(currentPath, content);
                }
            });
        });

        // --- API exposed to Python ---
        window.pywebview = window.pywebview || {};
        window.pywebview.api = window.pywebview.api || {};

        window.pywebview.api.open_in_tab = function(filepath, content) {
            // Determine language
            let lang = 'plaintext';
            if (filepath.endsWith('.py')) lang = 'python';
            if (filepath.endsWith('.js')) lang = 'javascript';
            if (filepath.endsWith('.html')) lang = 'html';
            if (filepath.endsWith('.json')) lang = 'json';

            // Create or switch model
            if (!models[filepath]) {
                const uri = monaco.Uri.file(filepath);
                models[filepath] = monaco.editor.createModel(content, lang, uri);
                addTab(filepath);
            }
            
            switchTo(filepath);
        };

        window.pywebview.api.reveal_range = function(filepath, startLine, endLine) {
            if (filepath !== currentPath) switchTo(filepath);
            editor.revealLineInCenter(startLine);
            editor.setSelection({
                startLineNumber: startLine,
                startColumn: 1,
                endLineNumber: endLine,
                endColumn: 1000
            });
        };

        // --- Internal Helpers ---
        function addTab(filepath) {
            const tabs = document.getElementById('tabs');
            const tab = document.createElement('div');
            tab.className = 'tab';
            tab.innerText = filepath;
            tab.onclick = () => switchTo(filepath);
            tab.dataset.path = filepath;
            tabs.appendChild(tab);
        }

        function switchTo(filepath) {
            if (!models[filepath]) return;
            editor.setModel(models[filepath]);
            currentPath = filepath;

            // UI update
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            const activeTab = document.querySelector(`.tab[data-path="${filepath}"]`);
            if (activeTab) activeTab.classList.add('active');
        }
    </script>
</body>
</html>
--------------------------------------------------------------------------------

-------------------- FILE: _MonacoHostMS\monaco_host.py --------------------------------
import webview
import threading
import json
import os
from pathlib import Path
from typing import Optional, Callable

class MonacoBridge:
    """
    The Bridge: Handles bidirectional communication between Python and the Monaco Editor.
    """
    def __init__(self):
        self._window = None
        self._ready_event = threading.Event()
        self.on_save_callback: Optional[Callable[[str, str], None]] = None

    def set_window(self, window):
        self._window = window

    # --- JS -> Python (Called from Editor) ---
    
    def signal_editor_ready(self):
        """Called by JS when Monaco is fully loaded."""
        self._ready_event.set()
        print("Monaco Editor is ready.")

    def save_file(self, filepath: str, content: str):
        """Called by JS when Ctrl+S is pressed."""
        if self.on_save_callback:
            self.on_save_callback(filepath, content)
        else:
            print(f"Saved {filepath} (No callback registered)")

    def log(self, message: str):
        """Called by JS to print to Python console."""
        print(f"[Monaco JS]: {message}")

    # --- Python -> JS (Called from App) ---

    def open_file(self, filepath: str, content: str):
        """Opens a file in a new tab in the editor."""
        self._ready_event.wait(timeout=5)
        if not self._window: return
        
        safe_path = filepath.replace('\\', '\\\\').replace("'", "\\'")
        safe_content = json.dumps(content)
        
        js = f"window.pywebview.api.open_in_tab('{safe_path}', {safe_content})"
        self._window.evaluate_js(js)

    def highlight_range(self, filepath: str, start_line: int, end_line: int):
        """Scrolls to and highlights a specific line range."""
        self._ready_event.wait(timeout=5)
        if not self._window: return
        
        safe_path = filepath.replace('\\', '\\\\')
        js = f"window.pywebview.api.reveal_range('{safe_path}', {start_line}, {end_line})"
        self._window.evaluate_js(js)

class MonacoHostMS:
    """
    The Host: Manages the PyWebView window lifecycle.
    """
    def __init__(self, html_path: str = "editor.html"):
        self.api = MonacoBridge()
        self.html_path = Path(html_path).resolve()
        self.window = None

    def launch(self, title="Monaco Editor", width=800, height=600, func=None):
        """
        Starts the editor window.
        :param func: Optional function to run in a background thread after launch.
        """
        self.window = webview.create_window(
            title, 
            str(self.html_path), 
            js_api=self.api,
            width=width, 
            height=height
        )
        self.api.set_window(self.window)
        
        if func:
            webview.start(func, debug=True)
        else:
            webview.start(debug=True)

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Setup
    host = MonacoHostMS()
    
    # 2. Define a background task to simulate "Opening a file" after 2 seconds
    def background_actions():
        import time
        print("Waiting for editor...")
        host.api._ready_event.wait()
        
        time.sleep(1)
        print("Opening test file...")
        
        dummy_code = """def hello_world():
    print("Hello from Python!")
    return True
"""
        host.api.open_file("test_script.py", dummy_code)
        
        # Define what happens on save
        host.api.on_save_callback = lambda path, content: print(f"SAVED TO DISK: {path}\nContent Size: {len(content)}")

    # 3. Launch
    print("Launching Editor...")
    host.launch(func=background_actions)
--------------------------------------------------------------------------------

-------------------- FILE: _NetworkLayoutMS\network_layout.py --------------------------
import networkx as nx
import logging
from typing import List, Dict, Any, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("NetLayout")
# ==============================================================================

class NetworkLayoutMS:
    """
    The Topologist: Calculates visual coordinates for graph nodes using
    server-side algorithms (NetworkX). 
    Useful for generating static map snapshots or pre-calculating positions 
    to offload client-side rendering.
    """
    def __init__(self):
        pass

    def calculate_layout(self, nodes: List[str], edges: List[Tuple[str, str]], 
                         algorithm: str = "spring", **kwargs) -> Dict[str, Tuple[float, float]]:
        """
        Computes (x, y) coordinates for the given graph.
        
        :param nodes: List of node IDs.
        :param edges: List of (source, target) tuples.
        :param algorithm: 'spring' (Force-directed) or 'circular'.
        :return: Dictionary {node_id: (x, y)}
        """
        G = nx.DiGraph()
        G.add_nodes_from(nodes)
        G.add_edges_from(edges)
        
        log.info(f"Computing layout for {len(nodes)} nodes, {len(edges)} edges...")
        
        try:
            if algorithm == "circular":
                pos = nx.circular_layout(G)
            else:
                # Spring layout (Fruchterman-Reingold) is standard for knowledge graphs
                k_val = kwargs.get('k', 0.15) # Optimal distance between nodes
                iter_val = kwargs.get('iterations', 50)
                pos = nx.spring_layout(G, k=k_val, iterations=iter_val, seed=42)
                
            # Convert numpy arrays to simple lists/tuples for JSON serialization
            return {n: (float(p[0]), float(p[1])) for n, p in pos.items()}
            
        except Exception as e:
            log.error(f"Layout calculation failed: {e}")
            return {}

# --- Independent Test Block ---
if __name__ == "__main__":
    layout = NetworkLayoutMS()
    
    # 1. Define a simple graph
    test_nodes = ["Main", "Utils", "Config", "DB", "Auth"]
    test_edges = [
        ("Main", "Utils"),
        ("Main", "Config"),
        ("Main", "DB"),
        ("Main", "Auth"),
        ("DB", "Config"),
        ("Auth", "DB")
    ]
    
    # 2. Compute Layout
    positions = layout.calculate_layout(test_nodes, test_edges, k=0.5)
    
    print("--- Calculated Positions ---")
    for node, (x, y) in positions.items():
        print(f"{node:<10}: ({x: .4f}, {y: .4f})")
--------------------------------------------------------------------------------

-------------------- FILE: _NetworkLayoutMS\requirements.txt ---------------------------
pip install networkx
--------------------------------------------------------------------------------

-------------------- FILE: _PromptOptimizerMS\prompt_optimizer.py ----------------------
import json
import logging
from typing import List, Dict, Any, Callable, Optional

# ==============================================================================
# CONFIGURATION: META-PROMPTS
# ==============================================================================
# The system prompt used to turn the LLM into a Prompt Engineer
REFINE_SYSTEM_PROMPT = (
    "You are a world-class prompt engineer. "
    "Given an original prompt and specific feedback, "
    "provide an improved, refined version of the prompt that incorporates the feedback. "
    "Return ONLY the refined prompt text, no preamble."
)

# The system prompt used to generate A/B test variations
VARIATION_SYSTEM_PROMPT = (
    "You are a creative AI assistant. "
    "Generate {num} innovative and diverse variations of the following prompt. "
    "Return the result as a valid JSON array of strings. "
    "Example: [\"variation 1\", \"variation 2\"]"
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptOpt")
# ==============================================================================

class PromptOptimizerMS:
    """
    The Tuner: Uses an LLM to refine prompts or generate variations.
    """
    def __init__(self, inference_func: Callable[[str], str]):
        """
        :param inference_func: A function that takes a string (prompt) and returns a string (completion).
                               Compatible with your ModelController.infer or OpenAI calls.
        """
        self.infer = inference_func

    def refine_prompt(self, draft_prompt: str, feedback: str) -> str:
        """
        Rewrites a prompt based on feedback.
        """
        full_prompt = (
            f"{REFINE_SYSTEM_PROMPT}\n\n"
            f"[Original Prompt]:\n{draft_prompt}\n\n"
            f"[Feedback]:\n{feedback}\n\n"
            f"[Refined Prompt]:"
        )
        
        log.info("Refining prompt...")
        try:
            result = self.infer(full_prompt)
            return result.strip()
        except Exception as e:
            log.error(f"Refinement failed: {e}")
            return draft_prompt # Fallback to original

    def generate_variations(self, draft_prompt: str, num_variations: int = 3, context_data: Optional[Dict] = None) -> List[str]:
        """
        Generates multiple versions of a prompt for testing.
        """
        meta_prompt = VARIATION_SYSTEM_PROMPT.format(num=num_variations)
        
        prompt_content = draft_prompt
        if context_data:
            prompt_content += f"\n\n--- Context ---\n{json.dumps(context_data, indent=2)}"

        full_prompt = (
            f"{meta_prompt}\n\n"
            f"[Original Prompt]:\n{prompt_content}\n\n"
            f"[JSON Array of Variations]:"
        )

        log.info(f"Generating {num_variations} variations...")
        try:
            # We explicitly ask for JSON, but LLMs are chatty, so we might need cleaning logic here
            raw_response = self.infer(full_prompt)
            
            # Simple cleanup to find the JSON array if the LLM added text around it
            start = raw_response.find('[')
            end = raw_response.rfind(']') + 1
            if start == -1 or end == 0:
                raise ValueError("No JSON array found in response")
                
            clean_json = raw_response[start:end]
            variations = json.loads(clean_json)
            
            if isinstance(variations, list):
                return [str(v) for v in variations]
            return []
            
        except Exception as e:
            log.error(f"Variation generation failed: {e}")
            return []

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Mock Inference Engine (Simulating an LLM)
    def mock_llm(prompt: str) -> str:
        if "[Refined Prompt]" in prompt:
            return "You are a helpful assistant who speaks like a pirate. How may I help ye?"
        if "[JSON Array]" in prompt:
            return '["Variation A: Pirate Mode", "Variation B: Formal Mode", "Variation C: Concise Mode"]'
        return "Error"

    optimizer = PromptOptimizerMS(inference_func=mock_llm)

    # 2. Test Refine
    print("--- Test: Refine ---")
    draft = "Help me."
    feedback = "Make it sound like a pirate."
    refined = optimizer.refine_prompt(draft, feedback)
    print(f"Original: {draft}")
    print(f"Refined:  {refined}")

    # 3. Test Variations
    print("\n--- Test: Variations ---")
    vars = optimizer.generate_variations(draft, num_variations=3)
    for i, v in enumerate(vars):
        print(f" {i+1}. {v}")
--------------------------------------------------------------------------------

-------------------- FILE: _PromptVaultMS\prompt_vault.py ------------------------------
import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, Callable
from pydantic import BaseModel, Field, ValidationError
from jinja2 import Environment, BaseLoader

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("prompt_vault.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptVault")
# ==============================================================================

# --- Data Models ---

class PromptVersion(BaseModel):
    """A specific historical version of a prompt."""
    version_num: int
    content: str
    author: str
    timestamp: datetime.datetime
    embedding: Optional[List[float]] = None

class PromptTemplate(BaseModel):
    """The master record for a prompt."""
    id: str
    slug: str
    title: str
    description: Optional[str] = ""
    tags: List[str] = []
    latest_version_num: int
    versions: List[PromptVersion] = []
    
    @property
    def latest(self) -> PromptVersion:
        """Helper to get the most recent content."""
        if not self.versions:
            raise ValueError("No versions found.")
        # versions are stored sorted by DB insertion usually, but let's be safe
        return sorted(self.versions, key=lambda v: v.version_num)[-1]

# --- Database Management ---

class PromptVaultMS:
    """
    The Vault: A persistent SQLite store for managing, versioning, 
    and rendering AI prompts.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()
        self.jinja_env = Environment(loader=BaseLoader())

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        """Bootstraps the schema."""
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS templates (
                    id TEXT PRIMARY KEY,
                    slug TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    tags_json TEXT,
                    latest_version INTEGER DEFAULT 1,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS versions (
                    id TEXT PRIMARY KEY,
                    template_id TEXT,
                    version_num INTEGER,
                    content TEXT,
                    author TEXT,
                    timestamp TIMESTAMP,
                    embedding_json TEXT,
                    FOREIGN KEY(template_id) REFERENCES templates(id)
                )
            """)

    # --- CRUD Operations ---

    def create_template(self, slug: str, title: str, content: str, author: str = "system", tags: List[str] = None) -> PromptTemplate:
        """Creates a new prompt template with an initial version 1."""
        tags = tags or []
        now = datetime.datetime.utcnow()
        t_id = str(uuid.uuid4())
        v_id = str(uuid.uuid4())

        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO templates (id, slug, title, description, tags_json, latest_version, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                    (t_id, slug, title, "", json.dumps(tags), 1, now, now)
                )
                conn.execute(
                    "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                    (v_id, t_id, 1, content, author, now)
                )
            log.info(f"Created template: {slug}")
            return self.get_template(slug)
        except sqlite3.IntegrityError:
            raise ValueError(f"Template '{slug}' already exists.")

    def add_version(self, slug: str, content: str, author: str = "user") -> PromptTemplate:
        """Adds a new version to an existing template."""
        current = self.get_template(slug)
        if not current:
            raise ValueError(f"Template '{slug}' not found.")

        new_ver = current.latest_version_num + 1
        now = datetime.datetime.utcnow()
        v_id = str(uuid.uuid4())

        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                (v_id, current.id, new_ver, content, author, now)
            )
            conn.execute(
                "UPDATE templates SET latest_version = ?, updated_at = ? WHERE id = ?",
                (new_ver, now, current.id)
            )
        log.info(f"Updated {slug} to v{new_ver}")
        return self.get_template(slug)

    def get_template(self, slug: str) -> Optional[PromptTemplate]:
        """Retrieves a full template with all history."""
        with self._get_conn() as conn:
            # 1. Fetch Template
            row = conn.execute("SELECT * FROM templates WHERE slug = ?", (slug,)).fetchone()
            if not row: return None

            # 2. Fetch Versions
            v_rows = conn.execute("SELECT * FROM versions WHERE template_id = ? ORDER BY version_num ASC", (row['id'],)).fetchall()
            
            versions = []
            for v in v_rows:
                versions.append(PromptVersion(
                    version_num=v['version_num'],
                    content=v['content'],
                    author=v['author'],
                    timestamp=v['timestamp']
                    # embedding logic skipped for brevity
                ))

            return PromptTemplate(
                id=row['id'],
                slug=row['slug'],
                title=row['title'],
                description=row['description'],
                tags=json.loads(row['tags_json']),
                latest_version_num=row['latest_version'],
                versions=versions
            )

    def render(self, slug: str, context: Dict[str, Any] = None) -> str:
        """Fetches the latest version and renders it with Jinja2."""
        template = self.get_template(slug)
        if not template:
            raise ValueError(f"Template '{slug}' not found.")
        
        raw_text = template.latest.content
        jinja_template = self.jinja_env.from_string(raw_text)
        return jinja_template.render(**(context or {}))

    def list_slugs(self) -> List[str]:
        with self._get_conn() as conn:
            rows = conn.execute("SELECT slug FROM templates").fetchall()
            return [r[0] for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # 1. Setup
    if DB_PATH.exists(): os.remove(DB_PATH)
    vault = PromptVaultMS()
    
    # 2. Create
    print("--- Creating Prompt ---")
    vault.create_template(
        slug="greet_user",
        title="Greeting Protocol",
        content="Hello {{ name }}, welcome to the {{ system_name }}!",
        tags=["ui", "onboarding"]
    )
    
    # 3. Versioning
    print("--- Updating Prompt ---")
    vault.add_version("greet_user", "Greetings, {{ name }}. System {{ system_name }} is online.")
    
    # 4. Retrieval & Rendering
    print("--- Rendering ---")
    final_text = vault.render("greet_user", {"name": "Alice", "system_name": "Nexus"})
    print(f"Rendered Output: {final_text}")
    
    # 5. Inspection
    tpl = vault.get_template("greet_user")
    print(f"Current Version: v{tpl.latest_version_num}")
    print(f"History: {[v.content for v in tpl.versions]}")
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)
--------------------------------------------------------------------------------

-------------------- FILE: _RegexWeaverMS\regex_weaver.py ------------------------------
import re
import logging
from typing import List, Set

# ==============================================================================
# CONFIGURATION: PATTERNS
# ==============================================================================
# Python: "import x", "from x import y"
PY_IMPORT = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')

# JS/TS: "import ... from 'x'", "require('x')"
JS_IMPORT = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RegexWeaver")
# ==============================================================================

class RegexWeaverMS:
    """
    The Weaver: A fault-tolerant dependency extractor.
    Uses Regex to find imports, making it faster and more permissive
    than AST parsers (works on broken code).
    """
    def __init__(self):
        pass

    def extract_dependencies(self, content: str, language: str) -> List[str]:
        """
        Scans code content for import statements.
        :param language: 'python' or 'javascript' (includes ts/jsx).
        """
        dependencies: Set[str] = set()
        lines = content.splitlines()
        
        pattern = PY_IMPORT if language == 'python' else JS_IMPORT
        
        for line in lines:
            # Skip comments roughly
            if line.strip().startswith(('#', '//')):
                continue
                
            if language == 'python':
                match = pattern.match(line)
            else:
                match = pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                # Clean up: "backend.database" -> "database"
                # We usually want the leaf name for simple linking
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                dependencies.add(clean_dep)
                
        return sorted(list(dependencies))

# --- Independent Test Block ---
if __name__ == "__main__":
    weaver = RegexWeaverMS()
    
    # 1. Python Test
    py_code = """
    import os
    from backend.utils import helper
    # from commented.out import ignore_me
    import pandas as pd
    """
    print(f"Python Deps: {weaver.extract_dependencies(py_code, 'python')}")
    
    # 2. JS Test
    js_code = """
    import React from 'react';
    const utils = require('./lib/utils');
    // import hidden from 'hidden';
    """
    print(f"JS Deps:     {weaver.extract_dependencies(js_code, 'javascript')}")
--------------------------------------------------------------------------------

-------------------- FILE: _RoleManagerMS\role_manager.py ------------------------------
import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("roles.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RoleManager")
# ==============================================================================

class Role(BaseModel):
    id: str
    name: str
    description: Optional[str] = ""
    system_prompt: str
    knowledge_bases: List[str] = []
    memory_policy: str = "scratchpad" # or 'auto_commit'
    created_at: datetime.datetime

class RoleManagerMS:
    """
    The Casting Director: Manages Agent Personas (Roles).
    Persists configuration for System Prompts, Attached KBs, and Memory Settings.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS roles (
                    id TEXT PRIMARY KEY,
                    name TEXT UNIQUE NOT NULL,
                    description TEXT,
                    system_prompt TEXT NOT NULL,
                    knowledge_bases_json TEXT,
                    memory_policy TEXT,
                    created_at TIMESTAMP
                )
            """)

    def create_role(self, name: str, system_prompt: str, description: str = "", kbs: List[str] = None) -> Role:
        """Creates a new Agent Persona."""
        role_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        kbs_json = json.dumps(kbs or [])
        
        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO roles (id, name, description, system_prompt, knowledge_bases_json, memory_policy, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    (role_id, name, description, system_prompt, kbs_json, "scratchpad", now)
                )
            log.info(f"Created Role: {name}")
            return self.get_role(name)
        except sqlite3.IntegrityError:
            raise ValueError(f"Role '{name}' already exists.")

    def get_role(self, name_or_id: str) -> Optional[Role]:
        """Retrieves a role by Name or ID."""
        with self._get_conn() as conn:
            # Try ID first
            row = conn.execute("SELECT * FROM roles WHERE id = ?", (name_or_id,)).fetchone()
            if not row:
                # Try Name
                row = conn.execute("SELECT * FROM roles WHERE name = ?", (name_or_id,)).fetchone()
            
            if not row: return None

            return Role(
                id=row['id'],
                name=row['name'],
                description=row['description'],
                system_prompt=row['system_prompt'],
                knowledge_bases=json.loads(row['knowledge_bases_json']),
                memory_policy=row['memory_policy'],
                created_at=row['created_at'] # Adapter might need datetime.fromisoformat if stored as str
            )

    def list_roles(self) -> List[Dict]:
        with self._get_conn() as conn:
            rows = conn.execute("SELECT id, name, description FROM roles").fetchall()
            return [dict(r) for r in rows]

    def delete_role(self, name: str):
        with self._get_conn() as conn:
            conn.execute("DELETE FROM roles WHERE name = ?", (name,))
        log.info(f"Deleted Role: {name}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    mgr = RoleManagerMS()
    
    # 1. Create
    mgr.create_role(
        name="SeniorDev", 
        system_prompt="You are a senior Python developer. Prefer Clean Code principles.",
        description="Expert coding assistant",
        kbs=["python_docs", "project_repo"]
    )
    
    # 2. Retrieve
    role = mgr.get_role("SeniorDev")
    print(f"Role: {role.name}")
    print(f"Prompt: {role.system_prompt}")
    print(f"KBs: {role.knowledge_bases}")
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)
--------------------------------------------------------------------------------

-------------------- FILE: _SandboxManagerMS\sandbox_manager.py ------------------------
import shutil
import hashlib
import os
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Default folders to ignore when syncing or diffing
DEFAULT_EXCLUDES = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "_sandbox", "_project_library"
}
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("SandboxMgr")
# ==============================================================================

class SandboxManagerMS:
    """
    The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.
    Allows for safe experimentation, diffing, and atomic promotion of changes.
    """
    def __init__(self, live_path: str, sandbox_path: str):
        self.live_root = Path(live_path).resolve()
        self.sandbox_root = Path(sandbox_path).resolve()

    def init_sandbox(self, force: bool = False):
        """
        Creates or resets the sandbox by mirroring the live project.
        """
        if self.sandbox_root.exists():
            if not force:
                raise FileExistsError(f"Sandbox already exists at {self.sandbox_root}")
            log.info("Wiping existing sandbox...")
            shutil.rmtree(self.sandbox_root)
        
        log.info(f"Cloning {self.live_root} -> {self.sandbox_root}...")
        self._mirror_tree(self.live_root, self.sandbox_root)
        log.info("Sandbox initialized.")

    def reset_sandbox(self):
        """
        Discards all sandbox changes and re-syncs from live.
        """
        self.init_sandbox(force=True)

    def get_diff(self) -> Dict[str, List[str]]:
        """
        Compares Sandbox vs Live. Returns added, modified, and deleted files.
        """
        sandbox_files = self._scan_files(self.sandbox_root)
        live_files = self._scan_files(self.live_root)
        
        sandbox_paths = set(sandbox_files.keys())
        live_paths = set(live_files.keys())

        # 1. Added: In sandbox but not in live
        added = sorted(list(sandbox_paths - live_paths))
        
        # 2. Deleted: In live but not in sandbox
        deleted = sorted(list(live_paths - sandbox_paths))
        
        # 3. Modified: In both, but hashes differ
        common = sandbox_paths.intersection(live_paths)
        modified = []
        for rel_path in common:
            if sandbox_files[rel_path] != live_files[rel_path]:
                modified.append(rel_path)
        modified.sort()

        return {
            "added": added,
            "modified": modified,
            "deleted": deleted
        }

    def promote_changes(self) -> Tuple[int, int, int]:
        """
        Applies changes from Sandbox to Live.
        Returns (added_count, modified_count, deleted_count).
        """
        diff = self.get_diff()
        
        # 1. Additions & Modifications (Copy file -> file)
        for rel_path in diff['added'] + diff['modified']:
            src = self.sandbox_root / rel_path
            dst = self.live_root / rel_path
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)
            
        # 2. Deletions (Remove file)
        for rel_path in diff['deleted']:
            target = self.live_root / rel_path
            if target.exists():
                os.remove(target)
                
        log.info(f"Promoted: {len(diff['added'])} added, {len(diff['modified'])} modified, {len(diff['deleted'])} deleted.")
        return len(diff['added']), len(diff['modified']), len(diff['deleted'])

    # --- Internal Helpers ---

    def _mirror_tree(self, src_root: Path, dst_root: Path):
        """Recursive copy that respects the exclusion list."""
        if not dst_root.exists():
            dst_root.mkdir(parents=True, exist_ok=True)

        for item in src_root.iterdir():
            if item.name in DEFAULT_EXCLUDES:
                continue
                
            dst_path = dst_root / item.name
            
            if item.is_dir():
                self._mirror_tree(item, dst_path)
            else:
                shutil.copy2(item, dst_path)

    def _scan_files(self, root: Path) -> Dict[str, str]:
        """
        Scans directory and returns {relative_path: sha256_hash}.
        """
        file_map = {}
        if not root.exists():
            return {}
            
        for path in root.rglob("*"):
            if path.is_file() and not self._is_excluded(path, root):
                rel = str(path.relative_to(root)).replace("\\", "/")
                file_map[rel] = self._get_hash(path)
        return file_map

    def _is_excluded(self, path: Path, root: Path) -> bool:
        """Checks if any part of the path is in the exclusion list."""
        try:
            rel_parts = path.relative_to(root).parts
            return any(p in DEFAULT_EXCLUDES for p in rel_parts)
        except ValueError:
            return False

    def _get_hash(self, path: Path) -> str:
        """Fast SHA-256 for file content."""
        try:
            # Skip binary files if needed, or hash them too (hashing is safe)
            return hashlib.sha256(path.read_bytes()).hexdigest()
        except Exception:
            return "read_error"

# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup test environment
    base = Path("test_env")
    live = base / "live_project"
    box = base / "sandbox"
    
    if base.exists(): shutil.rmtree(base)
    live.mkdir(parents=True)
    
    # 1. Create Mock Live Project
    (live / "main.py").write_text("print('v1')")
    (live / "utils.py").write_text("def help(): pass")
    (live / "node_modules").mkdir() # Should be ignored
    (live / "node_modules" / "junk.js").write_text("junk")
    
    print("--- Initializing Sandbox ---")
    mgr = SandboxManagerMS(str(live), str(box))
    mgr.init_sandbox()
    
    # 2. Make Changes in Sandbox
    print("\n--- Modifying Sandbox ---")
    (box / "main.py").write_text("print('v2')") # Modify
    (box / "new_feature.py").write_text("print('new')") # Add
    os.remove(box / "utils.py") # Delete
    
    # 3. Check Diff
    diff = mgr.get_diff()
    print(f"Diff Analysis:\n Added: {diff['added']}\n Modified: {diff['modified']}\n Deleted: {diff['deleted']}")
    
    # 4. Promote
    print("\n--- Promoting Changes ---")
    mgr.promote_changes()
    
    # Verify Live
    print(f"Live 'main.py' content: {(live / 'main.py').read_text()}")
    print(f"Live 'utils.py' exists? {(live / 'utils.py').exists()}")
    
    # Cleanup
    if base.exists(): shutil.rmtree(base)
--------------------------------------------------------------------------------

-------------------- FILE: _ScannerMS\scanner.py ---------------------------------------
import os
import time
from typing import Dict, List, Any, Optional

class ScannerMS:
    """
    The Scanner: Walks the file system, filters junk, and detects binary files.
    Generates the tree structure used by the UI.
    """
    
    def __init__(self):
        # Folders to completely ignore (Standard developer noise)
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage'
        }
        
        # Extensions that are explicitly binary/junk
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', '.svg', 
            '.zip', '.tar', '.gz', '.pdf', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }

    def is_binary(self, file_path: str) -> bool:
        """
        Determines if a file is binary using two heuristics:
        1. Extension check (Fast)
        2. Content check for null bytes (Accurate)
        """
        # 1. Fast Fail on Extension
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS:
            return True
            
        # 2. Content Inspection (Read first 1KB)
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                # Text files shouldn't contain null bytes
                if b'\x00' in chunk:
                    return True
        except (IOError, OSError):
            # If we can't read it, treat as binary/unsafe
            return True
            
        return False

    def scan_directory(self, root_path: str) -> Optional[Dict[str, Any]]:
        """
        Recursively scans a directory and returns a JSON-compatible tree.
        Returns None if path is invalid.
        """
        target = os.path.abspath(root_path)
        
        if not os.path.exists(target):
            return None
            
        if not os.path.isdir(target):
            # Handle single file case
            return self._create_node(target, is_dir=False)

        return self._scan_recursive(target)

    def _scan_recursive(self, current_path: str) -> Dict[str, Any]:
        """
        Internal recursive worker.
        """
        node = self._create_node(current_path, is_dir=True)
        node['children'] = []
        
        try:
            # os.scandir is faster than os.listdir as it returns file attributes
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                
                for entry in entries:
                    # Skip ignored directories
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS:
                        continue
                        
                    # Skip hidden files (dotfiles)
                    if entry.name.startswith('.'):
                        continue

                    if entry.is_dir():
                        child_node = self._scan_recursive(entry.path)
                        if child_node: # Only add if valid
                            node['children'].append(child_node)
                    else:
                        child_node = self._create_node(entry.path, is_dir=False)
                        node['children'].append(child_node)
                        
        except PermissionError:
            node['error'] = "Access Denied"
            
        return node

    def _create_node(self, path: str, is_dir: bool) -> Dict[str, Any]:
        """
        Standardizes the node structure for the UI.
        """
        name = os.path.basename(path)
        node = {
            'text': name,
            'path': path,
            'type': 'folder' if is_dir else 'file',
            'checked': False, # UI State
        }
        
        if not is_dir:
            if self.is_binary(path):
                node['type'] = 'binary'
                
        return node

    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        """
        Helper to extract all valid file paths from a tree node 
        (e.g., when the user clicks 'Start Ingest').
        """
        files = []
        if tree_node['type'] == 'file':
            files.append(tree_node['path'])
        elif tree_node['type'] == 'folder' and 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files

# --- Independent Test Block ---
if __name__ == "__main__":
    scanner = ScannerMS()
    
    # Scan the current directory
    cwd = os.getcwd()
    print(f"Scanning: {cwd} ...")
    
    start_time = time.time()
    tree = scanner.scan_directory(cwd)
    duration = time.time() - start_time
    
    if tree:
        file_count = len(scanner.flatten_tree(tree))
        print(f"Scan complete in {duration:.4f}s")
        print(f"Found {file_count} files.")
        
        # Print top level children to verify
        print("Top Level Structure:")
        for child in tree.get('children', [])[:5]:
            print(f" - [{child['type'].upper()}] {child['text']}")
    else:
        print("Scan failed or path invalid.")
--------------------------------------------------------------------------------

-------------------- FILE: _SearchEngineMS\search_engine.py ----------------------------
import sqlite3
import json
import struct
import requests
import os
from typing import List, Dict, Any, Optional

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

class SearchEngineMS:
    """
    The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
    
    Architecture:
    1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.
    2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.
    3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).
    """

    def __init__(self, model_name: str = "phi3:mini-128k"):
        self.model = model_name

    def search(self, db_path: str, query: str, limit: int = 10) -> List[Dict]:
        """
        Main entry point. Returns a list of results sorted by relevance.
        """
        if not os.path.exists(db_path):
            return []

        conn = sqlite3.connect(db_path)
        # Enable sqlite-vec extension if needed, though standard connect might miss it 
        # depending on system install. For now, we assume the DB is pre-populated 
        # and standard SQL queries work if the extension is loaded globally or unnecessary 
        # for simple selects (standard SQLite can read vec0 tables usually, just not query them efficiently without ext).
        # Note: If sqlite-vec is not loaded, the vec0 MATCH queries below will fail.
        # We try to load it here just in case.
        conn.enable_load_extension(True)
        try:
            import sqlite_vec
            sqlite_vec.load(conn)
        except:
            print("Warning: sqlite_vec not loaded in Search Engine. Vector search may fail.")

        cursor = conn.cursor()

        # 1. Vectorize the User Query
        query_vec = self._get_query_embedding(query)
        if not query_vec:
            # Fallback to keyword only if embedding fails
            return self._keyword_search_only(cursor, query, limit)

        # Pack vector for sqlite-vec (Float32 Little Endian)
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)

        # 2. HYBRID QUERY (The "Magic" SQL)
        # We use CTEs to get top 50 from Vector and top 50 from Keyword, then merge.
        sql = """
        WITH 
        vec_matches AS (
            SELECT rowid, distance,
            row_number() OVER (ORDER BY distance) as rank
            FROM knowledge_vectors
            WHERE embedding MATCH ? 
            AND k = 50
        ),
        fts_matches AS (
            SELECT rowid, rank as fts_score,
            row_number() OVER (ORDER BY rank) as rank
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT 50
        )
        SELECT 
            kc.file_path,
            kc.content,
            (
                -- RRF Formula: 1 / (k + rank)
                COALESCE(1.0 / (60 + v.rank), 0.0) +
                COALESCE(1.0 / (60 + f.rank), 0.0)
            ) as rrf_score
        FROM knowledge_chunks kc
        LEFT JOIN vec_matches v ON kc.id = v.rowid
        LEFT JOIN fts_matches f ON kc.id = f.rowid
        WHERE v.rowid IS NOT NULL OR f.rowid IS NOT NULL
        ORDER BY rrf_score DESC
        LIMIT ?;
        """

        try:
            # Escape quotes for FTS
            fts_query = f'"{query}"' 
            rows = cursor.execute(sql, (vec_bytes, fts_query, limit)).fetchall()
        except sqlite3.OperationalError as e:
            print(f"Search Error (likely missing sqlite-vec): {e}")
            return []

        results = []
        for r in rows:
            path, content, score = r
            snippet = self._extract_snippet(content, query)
            results.append({
                "path": path,
                "score": round(score, 4),
                "snippet": snippet,
                "full_content": content # Keeping this for "Reconstruct" later
            })

        conn.close()
        return results

    def _keyword_search_only(self, cursor, query: str, limit: int) -> List[Dict]:
        """Fallback if embeddings are offline."""
        sql = """
            SELECT file_path, content
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        rows = cursor.execute(sql, (f'"{query}"', limit)).fetchall()
        return [{
            "path": r[0], 
            "score": 0.0, 
            "snippet": self._extract_snippet(r[1], query),
            "full_content": r[1]
        } for r in rows]

    def _get_query_embedding(self, text: str) -> Optional[List[float]]:
        """Call Ollama to get the vector for the search query."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.model, "prompt": text},
                timeout=5
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
        return None

    def _extract_snippet(self, content: str, query: str) -> str:
        """Finds the best window of text around the keyword."""
        lower_content = content.lower()
        lower_query = query.lower().split()[0] # Take first word for simple centering
        
        idx = lower_content.find(lower_query)
        if idx == -1:
            return content[:200].replace('\n', ' ') + "..."
            
        start = max(0, idx - 60)
        end = min(len(content), idx + 140)
        snippet = content[start:end].replace('\n', ' ')
        return f"...{snippet}..."

# --- Independent Test Block ---
if __name__ == "__main__":
    # Note: Requires a real DB path to work
    print("Initializing Search Engine...")
    engine = SearchEngineMS()
    # Test would go here
--------------------------------------------------------------------------------

-------------------- FILE: _ServiceRegistryMS\service_registry.py ----------------------
import ast
import json
import os
import uuid
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OUTPUT_FILE = "registry.json"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("ServiceRegistry")
# ==============================================================================

class ServiceRegistryMS:
    """
    The Tokenizer: Scans a library of Python microservices and generates
    standardized JSON 'Service Tokens' describing their capabilities.
    """
    def __init__(self, root_path: str = "."):
        self.root = Path(root_path).resolve()
        self.registry = []

    def scan(self, save_to: str = OUTPUT_FILE) -> List[Dict]:
        """
        Scans the root directory for microservices and saves the registry.
        """
        log.info(f"Scanning for microservices in: {self.root}")
        
        # 1. Walk directories looking for likely microservices
        # Convention: Folders starting with '_' and ending with 'MS'
        for item in self.root.iterdir():
            if item.is_dir() and item.name.startswith("_") and item.name.endswith("MS"):
                self._process_microservice_folder(item)
        
        # 2. Save Registry
        try:
            with open(save_to, "w", encoding="utf-8") as f:
                json.dump(self.registry, f, indent=2)
            log.info(f"âœ… Registry built. Found {len(self.registry)} services. Saved to {save_to}")
        except Exception as e:
            log.error(f"Failed to save registry: {e}")
            
        return self.registry

    def _process_microservice_folder(self, folder: Path):
        """Finds and parses the primary Python file in a microservice folder."""
        # Heuristic: Find .py files that aren't __init__ or setup
        candidates = list(folder.glob("*.py"))
        
        for file in candidates:
            if file.name.startswith("__"): continue
            
            try:
                token = self._tokenize_file(file)
                if token:
                    self.registry.append(token)
                    log.info(f"  + Tokenized: {token['name']} ({file.name})")
                    # Assume one main service class per folder for now
                    break 
            except Exception as e:
                log.warning(f"  - Failed to parse {file.name}: {e}")

    def _tokenize_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """Parses a Python file to extract class metadata."""
        with open(file_path, "r", encoding="utf-8") as f:
            source = f.read()
        
        try:
            tree = ast.parse(source)
        except SyntaxError:
            return None
        
        # 1. Find the Main Class (Ends in 'MS' or matches filename concept)
        target_class = None
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                if node.name.endswith("MS"):
                    target_class = node
                    break
        
        if not target_class:
            return None

        # 2. Extract Metadata
        class_name = target_class.name
        docstring = ast.get_docstring(target_class) or "No description provided."
        
        # Generate a deterministic ID based on the class name
        # We use UUID5 with a custom namespace to ensure the ID is always the same for the same class name
        namespace = uuid.uuid5(uuid.NAMESPACE_DNS, "microservice.library")
        token_id = f"MS_{uuid.uuid5(namespace, class_name).hex[:8].upper()}"
        
        # 3. Analyze Dependencies (Imports)
        dependencies = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names: dependencies.add(n.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module: dependencies.add(node.module.split('.')[0])
        
        # 4. Analyze Inputs/Outputs (Public Methods)
        methods = {}
        
        for node in target_class.body:
            if isinstance(node, ast.FunctionDef):
                # Skip private methods and __init__
                if node.name.startswith("_"): continue
                
                method_name = node.name
                method_doc = ast.get_docstring(node) or ""
                
                # Get Arguments
                args = []
                for arg in node.args.args:
                    if arg.arg == "self": continue
                    annotation = self._get_annotation(arg.annotation)
                    args.append(f"{arg.arg}: {annotation}")
                
                # Get Return Type
                ret_anno = self._get_annotation(node.returns)
                
                methods[method_name] = {
                    "args": args,
                    "returns": ret_anno,
                    "doc": method_doc.strip()
                }

        # 5. Construct Token
        rel_path = str(file_path.relative_to(self.root)).replace("\\", "/")
        
        return {
            "token_id": token_id,
            "name": class_name,
            "path": rel_path,
            "description": docstring.strip(),
            "methods": methods,
            "dependencies": sorted(list(dependencies))
        }

    def _get_annotation(self, node) -> str:
        """Recursively resolves AST type annotations to strings."""
        if node is None: return "Any"
        if isinstance(node, ast.Name): return node.id
        if isinstance(node, ast.Constant): return str(node.value)
        if isinstance(node, ast.Subscript):
            # Handle generics like List[str]
            val = self._get_annotation(node.value)
            slice_val = self._get_annotation(node.slice)
            return f"{val}[{slice_val}]"
        if isinstance(node, ast.Attribute):
            return f"{self._get_annotation(node.value)}.{node.attr}"
        return "Complex"

# --- Independent Test Block ---
if __name__ == "__main__":
    # Point this at the directory containing your microservices
    # For self-testing, we scan the current directory
    current_dir = "."
    print(f"--- Running Service Registry on '{current_dir}' ---")
    
    registry = ServiceRegistryMS(current_dir)
    tokens = registry.scan()
    
    if tokens:
        print(f"\n[Sample Token: {tokens[0]['name']}]")
        print(json.dumps(tokens[0], indent=2))
    else:
        print("\nNo microservices found (Did you place this script in the root of your collection?)")
--------------------------------------------------------------------------------

-------------------- FILE: _SmartChunkerMS\smart_chunker.py ----------------------------
import re
from typing import List, Optional

class SmartChunkerMS:
    """
    The Editor: A 'Recursive' text splitter. 
    It respects the natural structure of text (Paragraphs -> Sentences -> Words)
    rather than just hacking it apart by character count.
    """
    
    def __init__(self):
        # Separators in order of preference.
        # 1. Double newline (Paragraph break)
        # 2. Single newline (Line break / List item)
        # 3. Sentence endings (Period, Question, Exclamation + Space)
        # 4. Space (Word break)
        # 5. Empty string (Hard character cut)
        self.separators = ["\n\n", "\n", "(?<=[.?!])\s+", " ", ""]

    def chunk(self, text: str, max_size: int = 1000, overlap: int = 100) -> List[str]:
        """
        Recursively chunks text while trying to keep related content together.
        """
        return self._recursive_split(text, self.separators, max_size, overlap)

    def _recursive_split(self, text: str, separators: List[str], max_size: int, overlap: int) -> List[str]:
        final_chunks = []
        
        # 1. Base Case: If the text fits, return it
        if len(text) <= max_size:
            return [text]
        
        # 2. Edge Case: No more separators, forced hard split
        if not separators:
            return self._hard_split(text, max_size, overlap)

        # 3. Recursive Step: Try to split by the current separator
        current_sep = separators[0]
        next_separators = separators[1:]
        
        # Regex split to keep delimiters if possible (logic varies by regex complexity)
        # For simple string splits like \n\n, we just split.
        if len(current_sep) > 1 and "(" in current_sep: 
            # It's a regex lookbehind (sentence splitter), use re.split
            splits = re.split(current_sep, text)
        else:
            splits = text.split(current_sep)

        # Now we have a list of smaller pieces. We need to merge them back together
        # until they fill the 'max_size' bucket, then start a new bucket.
        current_doc = []
        current_length = 0
        
        for split in splits:
            if not split: continue
            
            # If a single split is STILL too big, recurse deeper on it
            if len(split) > max_size:
                # If we have stuff in the buffer, flush it first
                if current_doc:
                    final_chunks.append(current_sep.join(current_doc))
                    current_doc = []
                    current_length = 0
                
                # Recurse on the big chunk using the NEXT separator
                sub_chunks = self._recursive_split(split, next_separators, max_size, overlap)
                final_chunks.extend(sub_chunks)
                continue

            # Check if adding this split would overflow
            if current_length + len(split) + len(current_sep) > max_size:
                # Flush the current buffer
                doc_text = current_sep.join(current_doc)
                final_chunks.append(doc_text)
                
                # Start new buffer with overlap logic?
                # For simplicity in recursion, we often just start fresh or carry over 
                # a small tail if we implemented a rolling window here.
                # To keep this "Pure logic" simple, we start fresh with the current split.
                current_doc = [split]
                current_length = len(split)
            else:
                # Add to buffer
                current_doc.append(split)
                current_length += len(split) + len(current_sep)

        # Flush remaining
        if current_doc:
            final_chunks.append(current_sep.join(current_doc))

        return final_chunks

    def _hard_split(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Last resort: naive character sliding window."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
    chunker = SmartChunkerMS()
    
    # Example: A technical document with structure
    doc = """
    # Intro to AI
    Artificial Intelligence is great. It helps us code.
    
    ## How it works
    1. Ingestion: Reading data.
    2. Processing: Thinking about data.
    
    This is a very long paragraph that effectively serves as a stress test for the sentence splitter. It should hopefully not break in the middle of a thought! We want to keep sentences whole.
    """
    
    print("--- Testing Smart Chunking (Max 60 chars) ---")
    # We set max_size very small to force it to use the sentence/word splitters
    chunks = chunker.chunk(doc, max_size=60, overlap=0)
    
    for i, c in enumerate(chunks):
        print(f"[{i}] {repr(c)}")
--------------------------------------------------------------------------------

-------------------- FILE: _SysInspectorMS\sys_inspector.py ----------------------------
import platform
import subprocess
import sys
import datetime
from typing import Dict, Optional

class SysInspectorMS:
    """
    The Auditor: Gathers hardware and environment statistics.
    Supports: Windows (WMIC), Linux (lscpu/lspci), and macOS (sysctl/system_profiler).
    """

    def generate_report(self) -> str:
        """
        Runs the full audit and returns a formatted string report.
        """
        system_os = platform.system()
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = [
            f"System Audit Report",
            f"Generated: {timestamp}",
            f"OS: {system_os} {platform.release()} ({platform.machine()})",
            "-" * 40,
            ""
        ]

        # 1. Hardware Section
        report.append("--- Hardware Information ---")
        if system_os == "Windows":
            report.extend(self._audit_windows())
        elif system_os == "Linux":
            report.extend(self._audit_linux())
        elif system_os == "Darwin":
            report.extend(self._audit_mac())
        else:
            report.append("Unsupported Operating System for detailed hardware audit.")

        # 2. Software Section
        report.append("\n--- Software Environment ---")
        report.append(f"Python Version: {platform.python_version()}")
        report.append(f"Python Executable: {sys.executable}")
        
        return "\n".join(report)

    def _run_cmd(self, cmd: str) -> str:
        """Helper to run shell commands safely."""
        try:
            # shell=True is often required for piped commands, specifically on Windows/Linux
            result = subprocess.run(
                cmd, 
                text=True, 
                capture_output=True, 
                check=False, 
                shell=True, 
                timeout=5
            )
            if result.returncode == 0 and result.stdout:
                return result.stdout.strip()
            elif result.stderr:
                return f"[Cmd Error]: {result.stderr.strip()}"
            return "[No Output]"
        except Exception as e:
            return f"[Execution Error]: {e}"

    # --- OS Specific Implementations ---

    def _audit_windows(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("wmic cpu get name"))
        # GPU
        data.append("GPU: " + self._run_cmd("wmic path win32_videocontroller get name"))
        # RAM
        try:
            mem_str = self._run_cmd("wmic computersystem get totalphysicalmemory").splitlines()[-1]
            mem_bytes = int(mem_str)
            data.append(f"Memory: {mem_bytes / (1024**3):.2f} GB")
        except:
            data.append("Memory: Could not retrieve total physical memory.")
        # Disk
        data.append("\nDisks:")
        data.append(self._run_cmd("wmic diskdrive get model,size"))
        return data

    def _audit_linux(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("lscpu | grep 'Model name'"))
        # GPU (Requires lspci, usually in pciutils)
        data.append("GPU: " + self._run_cmd("lspci | grep -i vga"))
        # RAM
        data.append("Memory:\n" + self._run_cmd("free -h"))
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("lsblk -o NAME,SIZE,MODEL"))
        return data

    def _audit_mac(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("sysctl -n machdep.cpu.brand_string"))
        # GPU
        data.append("GPU:\n" + self._run_cmd("system_profiler SPDisplaysDataType | grep -E 'Chipset Model|VRAM'"))
        # RAM
        data.append("Memory Details:\n" + self._run_cmd("system_profiler SPMemoryDataType | grep -E 'Size|Type|Speed'"))
        # RAM Total
        try:
            mem_bytes = int(self._run_cmd('sysctl -n hw.memsize'))
            data.append(f"Total Memory: {mem_bytes / (1024**3):.2f} GB")
        except: 
            pass
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("diskutil list physical"))
        return data

# --- Independent Test Block ---
if __name__ == "__main__":
    inspector = SysInspectorMS()
    print("Running System Inspector...")
    print("\n" + inspector.generate_report())
--------------------------------------------------------------------------------

-------------------- FILE: _TasklistVaultMS\task_vault.py ------------------------------
import sqlite3
import uuid
import logging
import datetime
import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Literal

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("task_vault.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("TaskVault")

TaskStatus = Literal["Pending", "Running", "Complete", "Error", "Awaiting-Approval"]
# ==============================================================================

class TaskVaultMS:
    """
    The Taskmaster: A persistent SQLite engine for hierarchical task management.
    Supports infinite nesting of sub-tasks and status tracking.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. Task Lists (The containers)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_lists (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP
                )
            """)
            # 2. Tasks (The items, supporting hierarchy via parent_id)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tasks (
                    id TEXT PRIMARY KEY,
                    list_id TEXT NOT NULL,
                    parent_id TEXT,
                    content TEXT NOT NULL,
                    status TEXT DEFAULT 'Pending',
                    result TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    FOREIGN KEY(list_id) REFERENCES task_lists(id) ON DELETE CASCADE,
                    FOREIGN KEY(parent_id) REFERENCES tasks(id) ON DELETE CASCADE
                )
            """)

    # --- List Management ---

    def create_list(self, name: str) -> str:
        """Creates a new task list and returns its ID."""
        list_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO task_lists (id, name, created_at) VALUES (?, ?, ?)",
                (list_id, name, now)
            )
        log.info(f"Created Task List: '{name}' ({list_id})")
        return list_id

    def get_lists(self) -> List[Dict]:
        """Returns metadata for all task lists."""
        with self._get_conn() as conn:
            rows = conn.execute("SELECT * FROM task_lists ORDER BY created_at DESC").fetchall()
            return [dict(r) for r in rows]

    # --- Task Management ---

    def add_task(self, list_id: str, content: str, parent_id: Optional[str] = None) -> str:
        """Adds a task (or sub-task) to a list."""
        task_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                """INSERT INTO tasks (id, list_id, parent_id, content, status, created_at, updated_at) 
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (task_id, list_id, parent_id, content, "Pending", now, now)
            )
        return task_id

    def update_task(self, task_id: str, content: str = None, status: TaskStatus = None, result: str = None):
        """Updates a task's details."""
        updates = []
        params = []
        
        if content:
            updates.append("content = ?")
            params.append(content)
        if status:
            updates.append("status = ?")
            params.append(status)
        if result:
            updates.append("result = ?")
            params.append(result)
            
        if not updates: return

        updates.append("updated_at = ?")
        params.append(datetime.datetime.utcnow())
        params.append(task_id)

        sql = f"UPDATE tasks SET {', '.join(updates)} WHERE id = ?"
        
        with self._get_conn() as conn:
            conn.execute(sql, params)
        log.info(f"Updated task {task_id}")

    # --- Tree Reconstruction ---

    def get_full_tree(self, list_id: str) -> Dict[str, Any]:
        """
        Fetches a list and reconstructs the full hierarchy of tasks.
        """
        with self._get_conn() as conn:
            # 1. Get List Info
            list_row = conn.execute("SELECT * FROM task_lists WHERE id = ?", (list_id,)).fetchone()
            if not list_row: return None
            
            # 2. Get All Tasks
            task_rows = conn.execute("SELECT * FROM tasks WHERE list_id = ?", (list_id,)).fetchall()
            
        # 3. Build Adjacency Map
        tasks_by_id = {}
        for r in task_rows:
            t = dict(r)
            t['sub_tasks'] = [] # Prepare children container
            tasks_by_id[t['id']] = t

        # 4. Link Parents and Children
        root_tasks = []
        for t_id, task in tasks_by_id.items():
            parent_id = task['parent_id']
            if parent_id and parent_id in tasks_by_id:
                tasks_by_id[parent_id]['sub_tasks'].append(task)
            else:
                root_tasks.append(task)

        return {
            "id": list_row['id'],
            "name": list_row['name'],
            "tasks": root_tasks
        }

    def delete_list(self, list_id: str):
        with self._get_conn() as conn:
            conn.execute("DELETE FROM task_lists WHERE id = ?", (list_id,))
        log.info(f"Deleted list {list_id}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    vault = TaskVaultMS()
    
    # 1. Create a Plan
    plan_id = vault.create_list("System Upgrade Plan")
    
    # 2. Add Root Tasks
    t1 = vault.add_task(plan_id, "Backup Database")
    t2 = vault.add_task(plan_id, "Update Server")
    
    # 3. Add Sub-Tasks
    t2_1 = vault.add_task(plan_id, "Stop Services", parent_id=t2)
    t2_2 = vault.add_task(plan_id, "Run Installer", parent_id=t2)
    
    # 4. Update Status
    vault.update_task(t1, status="Complete", result="Backup saved to /tmp/bk.tar")
    vault.update_task(t2_1, status="Running")
    
    # 5. Render Tree
    tree = vault.get_full_tree(plan_id)
    print(f"\n--- {tree['name']} ---")
    
    def print_node(node, indent=0):
        status_icon = "âœ“" if node['status'] == 'Complete' else "â—‹"
        print(f"{'  '*indent}{status_icon} {node['content']} [{node['status']}]")
        for child in node['sub_tasks']:
            print_node(child, indent + 1)

    for task in tree['tasks']:
        print_node(task)
        
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)
--------------------------------------------------------------------------------

-------------------- FILE: _TextChunkerMS\text_chunker.py ------------------------------
from typing import List, Tuple, Dict, Any

class TextChunkerMS:
    """
    The Butcher: A unified service for splitting text into digestible chunks
    for RAG (Retrieval Augmented Generation).
    """
    
    @staticmethod
    def chunk_by_chars(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
        """
        Standard Sliding Window. Best for prose/documentation.
        Splits purely by character count.
        """
        if chunk_size <= 0: raise ValueError("chunk_size must be positive")
        
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            # Advance start, backing up by overlap
            start += chunk_size - chunk_overlap
            
        return chunks

    @staticmethod
    def chunk_by_lines(text: str, max_lines: int = 200, max_chars: int = 4000) -> List[Dict[str, Any]]:
        """
        Line-Preserving Chunker. Best for Code.
        Respects line boundaries and returns metadata about line numbers.
        """
        lines = text.splitlines()
        chunks = []
        start = 0
        
        while start < len(lines):
            end = min(start + max_lines, len(lines))
            chunk_str = "\n".join(lines[start:end])
            
            # If too big, shrink window (back off)
            while len(chunk_str) > max_chars and end > start + 1:
                end -= 1
                chunk_str = "\n".join(lines[start:end])
            
            chunks.append({
                "text": chunk_str,
                "start_line": start + 1,
                "end_line": end
            })
            start = end
            
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
    chunker = TextChunkerMS()
    
    # 1. Prose Test
    print("--- Prose Chunking ---")
    lorem = "A" * 100 # 100 chars
    result = chunker.chunk_by_chars(lorem, chunk_size=40, chunk_overlap=10)
    for i, c in enumerate(result):
        print(f"Chunk {i}: len={len(c)}")

    # 2. Code Test
    print("\n--- Code Chunking ---")
    code = "\n".join([f"print('Line {i}')" for i in range(1, 10)])
    # Force splits small for testing
    result_code = chunker.chunk_by_lines(code, max_lines=3, max_chars=100)
    for i, c in enumerate(result_code):
        print(f"Chunk {i}: Lines {c['start_line']}-{c['end_line']}")
--------------------------------------------------------------------------------

-------------------- FILE: _ThoughtStreamMS\thought_stream.py --------------------------
import tkinter as tk
from tkinter import ttk
import datetime

class ThoughtStream(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        
        # Header
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        # The Stream Area (Canvas allows for custom drawing like sparklines)
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340) # Fixed width like React
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    def add_thought_bubble(self, filename, chunk_id, content, vector_preview, color):
        """
        Mimics the 'InspectorFrame' from your React code.
        """
        # Bubble Container
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        # Header: File + Timestamp
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        header_lbl = tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", 
                              fg="#007ACC", bg="#1a1a25", font=("Consolas", 8))
        header_lbl.pack(anchor="w", padx=5, pady=2)
        
        # Content Snippet
        snippet = content[:400] + "..." if len(content) > 400 else content
        content_lbl = tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", 
                               font=("Consolas", 8), justify="left", wraplength=300)
        content_lbl.pack(fill="x", padx=5, pady=2)
        
        # Vector Sparkline (The Custom Draw)
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector, color):
        """
        Recreates the 'vector_preview' visual from React using a micro-canvas.
        """
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        
        bar_w = w / len(vector) if len(vector) > 0 else 0
        
        for i, val in enumerate(vector):
            # Normalize -1..1 to 0..1 for height
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            
            # Draw bar
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")

# --- Usage Example ---
if __name__ == "__main__":
    root = tk.Tk()
    root.geometry("400x600")
    
    stream = ThoughtStream(root)
    stream.pack(fill="both", expand=True)
    
    # Simulate an incoming "Microservice" event
    import random
    fake_vector = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("ExplorerView.tsx", 1, "import React from 'react'...", fake_vector, "#FF00FF")
    
    root.mainloop()
--------------------------------------------------------------------------------

-------------------- FILE: _TreeMapperMS\tree_mapper.py --------------------------------
import os
from pathlib import Path
from typing import List, Set, Optional
import datetime

# ==============================================================================
# USER CONFIGURATION: DEFAULT EXCLUSIONS
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    '.venv', 'env', 'venv', 'dist', 'build', '.DS_Store'
}
# ==============================================================================

class TreeMapperMS:
    """
    The Cartographer: Generates ASCII-art style directory maps.
    """
    
    def generate_tree(self, 
                      root_path: str, 
                      additional_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> str:
        
        start_path = Path(root_path).resolve()
        if not start_path.exists(): return f"Error: Path '{root_path}' does not exist."

        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_EXCLUDES)
        if additional_exclusions:
            exclusions.update(additional_exclusions)

        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lines = [
            f"Project Map: {start_path.name}",
            f"Generated: {timestamp}",
            "-" * 40,
            f"ðŸ“ {start_path.name}/"
        ]

        self._walk(start_path, "", lines, exclusions)
        return "\n".join(lines)

    def _walk(self, directory: Path, prefix: str, lines: List[str], exclusions: Set[str]):
        try:
            children = sorted(
                [p for p in directory.iterdir() if p.name not in exclusions],
                key=lambda x: (x.is_file(), x.name.lower())
            )
        except PermissionError:
            lines.append(f"{prefix}â””â”€â”€ ðŸš« [Permission Denied]")
            return

        count = len(children)
        for index, path in enumerate(children):
            is_last = (index == count - 1)
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            
            if path.is_dir():
                lines.append(f"{prefix}{connector}ðŸ“ {path.name}/")
                extension = "    " if is_last else "â”‚   "
                self._walk(path, prefix + extension, lines, exclusions)
            else:
                lines.append(f"{prefix}{connector}ðŸ“„ {path.name}")

if __name__ == "__main__":
    print("TreeMapper initialized. Check top of file to tweak defaults.")
--------------------------------------------------------------------------------

-------------------- FILE: _VectorFactoryMS\requirements.txt ---------------------------
pip install chromadb faiss-cpu numpy
--------------------------------------------------------------------------------

-------------------- FILE: _VectorFactoryMS\vector_factory.py --------------------------
import os
import uuid
import logging
import shutil
from typing import List, Dict, Any, Optional, Protocol, Union
from pathlib import Path

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("VectorFactory")
# ==============================================================================

# --- Interface Definition ---

class VectorStore(Protocol):
    """The contract that all vector backends must fulfill."""
    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]) -> None:
        ...
    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        ...
    def count(self) -> int:
        ...
    def clear(self) -> None:
        ...

# --- Implementation 1: FAISS (Local, Fast, RAM-heavy) ---

class FaissVectorStore:
    def __init__(self, index_path: str, dimension: int):
        import numpy as np
        import faiss # Lazy import
        self.np = np
        self.faiss = faiss
        
        self.index_path = index_path
        self.dim = dimension
        self.metadata_store = []
        
        # Load or Create
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
            # Load metadata (simple JSON sidecar for this implementation)
            meta_path = index_path + ".meta.json"
            if os.path.exists(meta_path):
                import json
                with open(meta_path, 'r') as f:
                    self.metadata_store = json.load(f)
        else:
            self.index = faiss.IndexFlatL2(dimension)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        
        vecs = self.np.array(embeddings).astype("float32")
        self.index.add(vecs)
        self.metadata_store.extend(metadatas)
        self._save()

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        if self.index.ntotal == 0: return []
        
        q_vec = self.np.array([query_vector]).astype("float32")
        distances, indices = self.index.search(q_vec, k)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx != -1 and idx < len(self.metadata_store):
                entry = self.metadata_store[idx].copy()
                entry['score'] = float(dist) # FAISS returns L2 distance (lower is better)
                results.append(entry)
        return results

    def count(self) -> int:
        return self.index.ntotal

    def clear(self):
        self.index.reset()
        self.metadata_store = []
        self._save()

    def _save(self):
        self.faiss.write_index(self.index, self.index_path)
        import json
        with open(self.index_path + ".meta.json", 'w') as f:
            json.dump(self.metadata_store, f)

# --- Implementation 2: ChromaDB (Persistent, Feature-rich) ---

class ChromaVectorStore:
    def __init__(self, persist_dir: str, collection_name: str):
        import chromadb # Lazy import
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(collection_name)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        # Chroma requires unique IDs
        ids = [str(uuid.uuid4()) for _ in embeddings]
        
        # Ensure metadata is flat (Chroma limitation on nested dicts)
        clean_metas = [{k: str(v) if isinstance(v, (list, dict)) else v for k, v in m.items()} for m in metadatas]
        
        # Chroma expects 'documents' usually, but we handle logic upstream. 
        # We pass empty strings for 'documents' if purely vector-based, 
        # or map content from metadata if available.
        docs = [m.get("content", "") for m in metadatas]

        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            metadatas=clean_metas,
            documents=docs
        )

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        results = self.collection.query(
            query_embeddings=[query_vector],
            n_results=k
        )
        
        output = []
        if not results['ids']: return []

        # Unpack Chroma's columnar response format
        for i in range(len(results['ids'][0])):
            entry = results['metadatas'][0][i].copy()
            entry['score'] = results['distances'][0][i]
            entry['id'] = results['ids'][0][i]
            output.append(entry)
        return output

    def count(self) -> int:
        return self.collection.count()

    def clear(self):
        # Chroma doesn't have a truncate command, so we delete the collection
        name = self.collection.name
        self.client.delete_collection(name)
        self.collection = self.client.get_or_create_collection(name)

# --- The Factory ---

class VectorFactoryMS:
    """
    The Switchboard: Returns the appropriate VectorStore implementation
    based on configuration.
    """
    
    @staticmethod
    def create(backend: str, config: Dict[str, Any]) -> VectorStore:
        """
        :param backend: 'faiss' or 'chroma'
        :param config: Dict containing 'path', 'dim' (for FAISS), or 'collection' (for Chroma)
        """
        log.info(f"Initializing Vector Store: {backend.upper()}")
        
        if backend == "faiss":
            path = config.get("path", "vector_index.bin")
            dim = config.get("dim", 384)
            return FaissVectorStore(path, dim)
            
        elif backend == "chroma":
            path = config.get("path", "./chroma_db")
            name = config.get("collection", "default_collection")
            return ChromaVectorStore(path, name)
            
        else:
            raise ValueError(f"Unknown backend: {backend}")

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing VectorFactoryMS ---")
    
    # 1. Mock Data (dim=4 for simplicity)
    mock_vec = [0.1, 0.2, 0.3, 0.4]
    mock_meta = {"text": "Hello World", "source": "test"}
    
    # 2. Test FAISS
    print("\n[Testing FAISS]")
    try:
        faiss_store = VectorFactoryMS.create("faiss", {"path": "test_faiss.index", "dim": 4})
        faiss_store.add([mock_vec], [mock_meta])
        print(f"Count: {faiss_store.count()}")
        res = faiss_store.search(mock_vec, 1)
        print(f"Search Result: {res[0]['text']}")
        # Cleanup
        if os.path.exists("test_faiss.index"): os.remove("test_faiss.index")
        if os.path.exists("test_faiss.index.meta.json"): os.remove("test_faiss.index.meta.json")
    except ImportError:
        print("Skipping FAISS test (library not installed)")

    # 3. Test Chroma
    print("\n[Testing Chroma]")
    try:
        chroma_store = VectorFactoryMS.create("chroma", {"path": "./test_chroma_db", "collection": "test_col"})
        chroma_store.add([mock_vec], [mock_meta])
        print(f"Count: {chroma_store.count()}")
        res = chroma_store.search(mock_vec, 1)
        print(f"Search Result: {res[0]['text']}")
        # Cleanup
        if os.path.exists("./test_chroma_db"): shutil.rmtree("./test_chroma_db")
    except ImportError:
        print("Skipping Chroma test (library not installed)")
    except Exception as e:
        print(f"Chroma Error: {e}")
--------------------------------------------------------------------------------

-------------------- FILE: _WebScraperMS\requirements.txt ------------------------------
pip install httpx readability-lxml
--------------------------------------------------------------------------------

-------------------- FILE: _WebScraperMS\web_scraper.py --------------------------------
import httpx
import logging
import asyncio
from typing import Optional, Dict, Any
from readability import Document

# ==============================================================================
# CONFIGURATION
# ==============================================================================
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
TIMEOUT_SECONDS = 15.0

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("WebScraper")
# ==============================================================================

class WebScraperMS:
    """
    The Reader: Fetches URLs and extracts the main content using Readability.
    Strips ads, navbars, and boilerplate to return clean text for LLMs.
    """
    def __init__(self):
        self.headers = {"User-Agent": USER_AGENT}

    def scrape(self, url: str) -> Dict[str, Any]:
        """
        Synchronous wrapper for fetching and cleaning a URL.
        Returns: {
            "url": str,
            "title": str,
            "content": str (The main body text),
            "html": str (The raw HTML of the main content area)
        }
        """
        return asyncio.run(self._scrape_async(url))

    async def _scrape_async(self, url: str) -> Dict[str, Any]:
        log.info(f"Fetching: {url}")
        
        async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=TIMEOUT_SECONDS) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                log.error(f"HTTP Error {e.response.status_code}: {e}")
                raise
            except httpx.RequestError as e:
                log.error(f"Request failed: {e}")
                raise

        # Parse with Readability
        try:
            doc = Document(response.text)
            title = doc.title()
            # Summary() returns the HTML of the main content area
            clean_html = doc.summary() 
            
            # Convert HTML content to plain text for the LLM
            # (Simple strip tags implementation, for better results use BeautifulSoup)
            clean_text = self._strip_tags(clean_html)
            
            log.info(f"Successfully scraped '{title}' ({len(clean_text)} chars)")
            
            return {
                "url": url,
                "title": title,
                "content": clean_text,
                "html": clean_html
            }
        except Exception as e:
            log.error(f"Parsing failed: {e}")
            raise

    def _strip_tags(self, html: str) -> str:
        """
        Removes HTML tags to leave only the readable text.
        Note: A robust production version should use BeautifulSoup4.
        """
        import re
        # Remove scripts and styles
        html = re.sub(r'<(script|style).*?>.*?</\1>', '', html, flags=re.DOTALL)
        # Remove tags
        text = re.sub(r'<[^>]+>', ' ', html)
        # Collapse whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# --- Independent Test Block ---
if __name__ == "__main__":
    scraper = WebScraperMS()
    
    # Test URL (Example: Python's PEP 8)
    target_url = "https://peps.python.org/pep-0008/"
    
    print(f"--- Scraping {target_url} ---")
    try:
        data = scraper.scrape(target_url)
        print(f"\nTitle: {data['title']}")
        print(f"Content Preview:\n{data['content'][:500]}...")
        print(f"\nTotal Length: {len(data['content'])} characters")
    except Exception as e:
        print(f"Scrape failed: {e}")
--------------------------------------------------------------------------------
